
@misc{abalone_sa_abalone_nodate,
  title = {Abalone Rulebook},
  shorttitle = {Abalone {{Rulebook}}},
  author = {Abalone S.A.},
  copyright = {Copyright Abalone S.A.},
  howpublished = {https://cdn.1j1ju.com/medias/c2/b0/3a-abalone-rulebook.pdf},
  file = {/home/ture/Zotero/storage/JHEB8Q64/3a-abalone-rulebook.pdf}
}

@article{agrawal_learning_nodate,
  title = {Learning to {{Poke}} by {{Poking}}: {{Experiential Learning}} of {{Intuitive Physics}}},
  author = {Agrawal, Pulkit and Nair, Ashvin V and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  pages = {9},
  abstract = {We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.},
  langid = {english},
  file = {/home/ture/Zotero/storage/6BAXF385/Agrawal et al. - Learning to Poke by Poking Experiential Learning .pdf}
}

@misc{aichholzer_abalone_2006,
  title = {Abalone Games},
  author = {Aichholzer, Oswin},
  year = {2006},
  howpublished = {http://www.ist.tugraz.at/staff/aichholzer/research/rp/abalone/games.php},
  file = {/home/ture/Zotero/storage/2IBKQU5M/games.html}
}

@article{aichholzer_algorithmic_2002,
  title = {Algorithmic Fun-Abalone},
  author = {Aichholzer, Oswin and Aurenhammer, Franz and Werner, Tino},
  year = {2002},
  journal = {Special Issue on Foundations of Information Processing of TELEMATIK},
  volume = {1},
  pages = {4--6},
  file = {/home/ture/Zotero/storage/8QRDUB38/Aichholzer et al. - 2002 - Algorithmic fun-abalone.pdf}
}

@misc{aichholzer_oswin_2006,
  title = {Oswin {{Aichholzer}}'s Homepage},
  author = {Aichholzer, Oswin},
  year = {2006},
  howpublished = {http://www.ist.tugraz.at/staff/aichholzer/research/rp/abalone/},
  file = {/home/ture/Zotero/storage/XREI4DKN/abalone.html}
}

@article{auer_finite-time_nodate,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo},
  pages = {22},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/home/ture/Zotero/storage/9A95VB4X/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf}
}

@article{bellman_markovian_1957,
  title = {A {{Markovian Decision Process}}},
  author = {Bellman, Richard},
  year = {1957},
  journal = {Journal of Mathematics and Mechanics},
  volume = {6},
  number = {5},
  pages = {679--684},
  publisher = {{Indiana University Mathematics Department}},
  issn = {0095-9057},
  file = {/home/ture/Zotero/storage/RX3G7GRM/56038.pdf}
}

@article{berner_dota_2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  pages = {66},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  langid = {english},
  file = {/home/ture/Zotero/storage/5GSE6DJG/Berner et al. - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf}
}

@inproceedings{bouzy_associating_2006,
  title = {Associating {{Shallow}} and {{Selective Global Tree Search}} with {{Monte Carlo}} for 9 \texttimes{} 9 {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Bouzy, Bruno},
  editor = {{van den Herik}, H. Jaap and Bj{\"o}rnsson, Yngvi and Netanyahu, Nathan S.},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {67--80},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11674399_5},
  abstract = {This paper explores the association of shallow and selective global tree search with Monte Carlo in 9 \texttimes{} 9 Go. This exploration is based on Olga and Indigo, two experimental Monte-Carlo programs. We provide a min-max algorithm that iteratively deepens the tree until one move at the root is proved to be superior to the other ones. At each iteration, random games are started at leaf nodes to compute mean values. The progressive pruning rule and the min-max rule are applied to non terminal nodes. We set up experiments demonstrating the relevance of this approach. Indigo used this algorithm at the 8 th Computer Olympiad held in Graz.},
  isbn = {978-3-540-32489-8},
  langid = {english},
  keywords = {Current Depth,Game Tree,Leaf Node,Parent Node,Tree Search}
}

@article{bruasdal_deep_2020,
  title = {Deep Reinforcement {{Learning Using Monte-Carlo Tree Search}} for {{Hex}} and {{Othello}}},
  author = {Bru{\aa}sdal, Henrik},
  year = {2020},
  publisher = {{NTNU}},
  abstract = {N\aa r Deepminds AlphaGo-program slo den menneskelige profesjonelle Go-spilleren Fan Hui i 2015 var dette et stort gjennombrudd for kunstig intelligens til spilling. Go hadde vist seg \aa{} motst\aa{} de teknikkene som lenge hadde sl\aa tt mennesker i spill som sjakk. Gjennom en nyskapende kombinasjon av dype nevrale nettverk, forsterkende l\ae ring og Monte Carlo-tres\o k ble Go endelig mestret. Kort tid etterp\aa{} kom AlphaGo Zero, som oppn\aa dde det samme ved \aa{} l\ae re utelukkende ved \aa{} spille mot seg selv, og AlphaZero, som generaliserte teknikken til andre spill. Dette arbeidet inneholder en n\o ye gjennomgang av disse systemene og arbeidet i feltet som ledet opp mot dem. Det g\aa r gjennom min egen implementasjon av denne teknikken og dens bruk i spillene Hex og Othello. Ved hjelp av denne implementasjonen har jeg unders\o kt rollen utrulling spiller i algoritmen. Dette var en sentral del av tidligere arbeid i feltet og fremdeles brukt i AlphaGo, men ikke i AlphaGo Zero og AlphaZero. Flere eksperimenter har blitt gjennomf\o rt for \aa{} f\aa{} empiriske data for om utrulling fremdeles kan v\ae re en gunstig del av denne nyskapende kombinasjonen av teknikker, og hvordan disse i s\aa{} fall b\o r gjennomf\o res. Selv om det var noen indikasjoner i dataene p\aa{} at utrulling har liten eller ingen positiv effect er disse resultatene stort sett ikke entydige. Noen svakheter i oppsettet har blitt identifisert og noen nye sp\o rsm\aa l har blitt oppdaget. Men arbeidet har resultert i et funksjonelt system som kan brukes til videre unders\o kelser av problemomr\aa det og enten gi mer entydige data eller innsikt i nye sp\o rsm\aa l.},
  langid = {english},
  annotation = {Accepted: 2021-09-15T16:00:35Z},
  file = {/home/ture/Zotero/storage/RXYGMGDB/Bru√•sdal - 2020 - Deep reinforcement Learning Using Monte-Carlo Tree.pdf;/home/ture/Zotero/storage/HSPTZDLR/2777474.html}
}

@misc{campfireman_campfiremanabalone-boai_2021,
  title = {Campfireman/{{Abalone-BoAI}}},
  author = {{campfireman}},
  year = {2021},
  month = jun,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT}
}

@article{campos_abalearn_2003,
  title = {Abalearn: {{Ecient Self-Play Learning}} of the Game {{Abalone}}},
  shorttitle = {Abalearn},
  author = {Campos, Pedro and Langlois, Thibault},
  year = {2003},
  abstract = {This paper presents Abalearn, a self-teaching Abalone pro- gram capable of automatically reaching an intermediate level of play without needing expert-labeled training examples or deep searches. Our approach is based on a reinforcement learning algorithm that is risk- seeking, since defensive players in Abalone tend to never end a game. We extend the risk-sensitive reinforcement learning framework in order to deal with large state spaces and we also propose a set of features that seem relevant for achieving a good level of play. We evaluate our approach using a fixed heuristic opponent as a bench- mark, pitting our agents against human players online and comparing samples of our agents at dierent times of training.},
  file = {/home/ture/Zotero/storage/BF4V5CJK/Campos and Langlois - 2009 - Abalearn Ecient Self-Play Learning of the game Ab.pdf}
}

@mastersthesis{chorus_implementing_2009,
  title = {Implementing a Computer Player for Abalone Using Alpha-Beta and Monte-Carlo Search},
  author = {Chorus, Pascal},
  year = {2009},
  school = {Citeseer},
  file = {/home/ture/Zotero/storage/YY9WTM47/Chorus - 2009 - Implementing a computer player for abalone using a.pdf}
}

@misc{claussen_abalone_2021,
  title = {Abalone},
  author = {Claussen, Ture},
  year = {2021},
  journal = {GitLab},
  abstract = {GitLab.com},
  howpublished = {https://gitlab.com/CampFireMan/abalone},
  langid = {english},
  file = {/home/ture/Zotero/storage/5BYMCU37/abalone.html}
}

@misc{colah_calculus_nodate,
  title = {Calculus on {{Computational Graphs}}: {{Backpropagation}} -- Colah's Blog},
  author = {Colah, Christopher},
  howpublished = {https://colah.github.io/posts/2015-08-Backprop/},
  file = {/home/ture/Zotero/storage/53JQLZ2F/2015-08-Backprop.html}
}

@inproceedings{coulom_efficient_2007,
  title = {Efficient {{Selectivity}} and {{Backup Operators}} in {{Monte-Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Coulom, R{\'e}mi},
  editor = {{van den Herik}, H. Jaap and Ciancarini, Paolo and Donkers, H. H. L. M. (Jeroen)},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {72--83},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_7},
  abstract = {A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9\texttimes 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  isbn = {978-3-540-75538-8},
  langid = {english},
  keywords = {Black String,Good Move,Random Game,Random Simulation,Tree Search},
  file = {/home/ture/Zotero/storage/DXVL9T4P/Coulom - 2007 - Efficient Selectivity and Backup Operators in Mont.pdf}
}

@inproceedings{coulom_whole-history_2008,
  title = {Whole-{{History Rating}}: {{A Bayesian Rating System}} for {{Players}} of {{Time-Varying Strength}}},
  shorttitle = {Whole-{{History Rating}}},
  booktitle = {Computers and {{Games}}},
  author = {Coulom, R{\'e}mi},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {113--124},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_11},
  abstract = {Whole-History Rating (WHR) is a new method to estimate the time-varying strengths of players involved in paired comparisons. Like many variations of the Elo rating system, the whole-history approach is based on the dynamic Bradley-Terry model. But, instead of using incremental approximations, WHR directly computes the exact maximum a posteriori over the whole rating history of all players. This additional accuracy comes at a higher computational cost than traditional methods, but computation is still fast enough to be easily applied in real time to large-scale game servers (a new game is added in less than 0.001 second). Experiments demonstrate that, in comparison to Elo, Glicko, TrueSkill, and decayed-history algorithms, WHR produces better predictions.},
  isbn = {978-3-540-87608-3},
  langid = {english},
  keywords = {Incremental Algorithm,Prediction Rate,Rating Algorithm,Rating Uncertainty,Wiener Process},
  file = {/home/ture/Zotero/storage/HMUCYEXM/Coulom - 2008 - Whole-History Rating A Bayesian Rating System for.pdf;/home/ture/Zotero/storage/LSD7UUJH/Coulom - 2008 - Whole-History Rating A Bayesian Rating System for.pdf}
}

@misc{deepmind_match_nodate,
  title = {Match 1 - {{Google DeepMind Challenge Match}}: {{Lee Sedol}} vs {{AlphaGo}}},
  shorttitle = {Match 1 - {{Google DeepMind Challenge Match}}},
  author = {{DeepMind}},
  abstract = {Watch DeepMind's program AlphaGo take on the legendary Lee Sedol (9-dan pro), the top Go player of the past decade, in a \$1M 5-game challenge match in Seoul. This is the livestream for Match 1 to be played on: 9th March 13:00 KST (local), 04:00 GMT; note for US viewers this is the day before on: 8th March 20:00 PT, 23:00 ET.  In October 2015, AlphaGo became the first computer program ever to beat a professional Go player by winning 5-0 against the reigning 3-times European Champion Fan Hui (2-dan pro). That work was featured in a front cover article in the science journal Nature in January 2016. Match commentary by Michael Redmond (9-dan pro) and Chris Garlock.},
  howpublished = {https://www.youtube.com/watch?v=vFr3K2DORc8\&t=7020s}
}

@misc{deletang_multi_2019,
  title = {Multi {{GPU}}, Multi Process with {{Tensorflow}}},
  author = {Del{\'e}tang, Gr{\'e}goire},
  year = {2019},
  month = jul,
  journal = {Medium},
  abstract = {Tensorflow is a tremendous tool to experiment deep learning algorithms. But to exploit the power of deep learning, you need to leverage it\ldots},
  howpublished = {https://towardsdatascience.com/multi-gpu-multi-process-with-tensorflow-ba4cc2fe3ab7},
  langid = {english},
  file = {/home/ture/Zotero/storage/SNKFP4PJ/multi-gpu-multi-process-with-tensorflow-ba4cc2fe3ab7.html}
}

@article{demichelis_simple_2004,
  title = {The Simple Geometry of Perfect Information Games},
  author = {Demichelis, Stefano and Ritzberger, Klaus and Swinkels, Jeroen M.},
  year = {2004},
  month = jun,
  journal = {International Journal of Game Theory},
  volume = {32},
  number = {3},
  pages = {315--338},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s001820400169},
  langid = {english},
  file = {/home/ture/Zotero/storage/LE4XDBX5/Demichelis et al. - 2004 - The simple geometry of perfect information games.pdf}
}

@misc{foster_how_2019,
  title = {How to Build Your Own {{AlphaZero AI}} Using {{Python}} and {{Keras}}},
  author = {Foster, David},
  year = {2019},
  month = dec,
  journal = {Applied Data Science},
  abstract = {Teach a machine to learn Connect4 strategy through self-play and deep learning.},
  langid = {english},
  file = {/home/ture/Zotero/storage/QIH4KCYY/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188.html}
}

@misc{fridman_206_nodate,
  title = {\#206 - {{Ishan Misra}}: {{Self-Supervised Deep Learning}} in {{Computer Vision}} | {{Lex Fridman Podcast}}},
  shorttitle = {\#206 - {{Ishan Misra}}},
  author = {Fridman, Lex},
  number = {206},
  abstract = {Ishan Misra is a research scientist at FAIR working on self-supervised visual learning.},
  langid = {american},
  file = {/home/ture/Zotero/storage/2YLN3AXC/ishan-misra.html}
}

@misc{gamble_safety-first_2018,
  title = {{Safety-first AI for autonomous data centre cooling and industrial control}},
  author = {Gamble, Chris and Gao, Jim},
  year = {2018},
  month = aug,
  journal = {Deepmind},
  abstract = {Many of society's most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach.In 2016, we jointly developed an AI-powered recommendation system to improve the energy efficiency of Google's already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change.Now we're taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres.},
  howpublished = {https://deepmind.com/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control},
  langid = {ALL},
  file = {/home/ture/Zotero/storage/QDS3HFCI/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control.html}
}

@article{gao_multithreaded_nodate,
  title = {Multithreaded {{Pruned Tree Search}} in {{Distributed Systems}}},
  author = {Gao, Yaoqing and Marsland, T A},
  pages = {11},
  abstract = {Although efficient support for data-parallel applications is relatively well established, it remains open how well to support irregular and dynamic problems where there are no regular data structures and communication patterns. Tree search is central to solving a variety of problems in artificial intelligence and an important subset of the irregular applications where tasks are frequently created and terminated. In this paper, we introduce the design of a multithreaded distributed runtime system. Efficiency and ease of parallel programming are the two primary goals. In our system, multithreading is used to specify the asynchronous behavior in parallel game tree search, and dynamic load balancing is employed for efficient performance.},
  langid = {english},
  file = {/home/ture/Zotero/storage/KYCV5AV9/Gao and Marsland - Multithreaded Pruned Tree Search in Distributed Sy.pdf}
}

@article{gelly_achieving_nodate,
  title = {Achieving {{Master Level Play}} in 9 \texttimes{} 9 {{Computer Go}}},
  author = {Gelly, Sylvain and Silver, David},
  pages = {4},
  abstract = {The UCT algorithm uses Monte-Carlo simulation to estimate the value of states in a search tree from the current state. However, the first time a state is encountered, UCT has no knowledge, and is unable to generalise from previous experience. We describe two extensions that address these weaknesses. Our first algorithm, heuristic UCT, incorporates prior knowledge in the form of a value function. The value function can be learned offline, using a linear combination of a million binary features, with weights trained by temporal-difference learning. Our second algorithm, UCT\textendash RAVE, forms a rapid online generalisation based on the value of moves. We applied our algorithms to the domain of 9 \texttimes{} 9 Computer Go, using the program MoGo. Using both heuristic UCT and RAVE, MoGo became the first program to achieve human master level in competitive play.},
  langid = {english},
  file = {/home/ture/Zotero/storage/BTDK6UD2/Gelly and Silver - Achieving Master Level Play in 9 √ó 9 Computer Go.pdf}
}

@article{gelly_monte-carlo_2011,
  title = {Monte-{{Carlo}} Tree Search and Rapid Action Value Estimation in Computer {{Go}}},
  author = {Gelly, Sylvain and Silver, David},
  year = {2011},
  month = jul,
  journal = {Artificial Intelligence},
  volume = {175},
  number = {11},
  pages = {1856--1875},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2011.03.007},
  abstract = {A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9\texttimes 9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.},
  langid = {english},
  keywords = {Computer Go,Monte-Carlo,Reinforcement learning,Search},
  file = {/home/ture/Zotero/storage/T2PQDD4B/Gelly and Silver - 2011 - Monte-Carlo tree search and rapid action value est.pdf}
}

@article{halevy_unreasonable_2009,
  title = {The {{Unreasonable Effectiveness}} of {{Data}}},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  year = {2009},
  month = mar,
  journal = {IEEE Intelligent Systems},
  volume = {24},
  number = {2},
  pages = {8--12},
  issn = {1541-1672},
  doi = {10.1109/MIS.2009.36},
  langid = {english},
  file = {/home/ture/Zotero/storage/T6IP2PRV/Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf}
}

@book{haugeland_artificial_1985,
  title = {Artificial Intelligence: The Very Idea},
  shorttitle = {Artificial Intelligence},
  author = {Haugeland, John},
  year = {1985},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-08153-5},
  langid = {english},
  lccn = {Q335 .H38 1985},
  keywords = {Artificial intelligence},
  file = {/home/ture/Zotero/storage/NX65VA94/Haugeland - 1985 - Artificial intelligence the very idea.pdf}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {/home/ture/Zotero/storage/3IVN96YN/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/home/ture/Zotero/storage/B4NWSVQW/7780459.html}
}

@misc{henribru_deep_2021,
  title = {Deep {{MCTS}}},
  author = {{henribru}},
  year = {2021},
  month = oct,
  abstract = {Code and raw data for my master's thesis Deep reinforcement learning using Monte-Carlo tree search for Hex and Othello.}
}

@misc{higgins_brief_2017,
  title = {A {{Brief History}} of {{Deep Blue}}, {{IBM}}'s {{Chess Computer}} | {{Mental Floss}}},
  author = {Higgins, Chris},
  year = {2017},
  month = jul,
  howpublished = {https://web.archive.org/web/20170803130439/https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer},
  file = {/home/ture/Zotero/storage/ZE4C9CJX/brief-history-deep-blue-ibms-chess-computer.html}
}

@article{holland_studying_2006,
  title = {Studying {{Complex Adaptive Systems}}},
  author = {Holland, John H.},
  year = {2006},
  month = mar,
  journal = {Journal of Systems Science and Complexity},
  volume = {19},
  number = {1},
  pages = {1--8},
  issn = {1009-6124, 1559-7067},
  doi = {10.1007/s11424-006-0001-z},
  abstract = {Complex adaptive systems (cas) \textendash{} systems that involve many components that adapt or learn as they interact \textendash{} are at the heart of important contemporary problems. The study of cas poses unique challenges: Some of our most powerful mathematical tools, particularly methods involving fixed points, attractors, and the like, are of limited help in understanding the development of cas. This paper suggests ways to modify research methods and tools, with an emphasis on the role of computer-based models, to increase our understanding of cas.},
  langid = {english},
  file = {/home/ture/Zotero/storage/PJZH78SX/Holland - 2006 - Studying Complex Adaptive Systems.pdf}
}

@misc{hui_alphago_2018,
  title = {{{AlphaGo}}: {{How}} It Works Technically?},
  shorttitle = {{{AlphaGo}}},
  author = {Hui, Jonathan},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it\ldots},
  langid = {english},
  file = {/home/ture/Zotero/storage/AJ7N92UX/alphago-how-it-works-technically-26ddcc085319.html}
}

@incollection{hutchison_bandit_2006,
  title = {Bandit {{Based Monte-Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  volume = {4212},
  pages = {282--293},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_29},
  abstract = {For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  isbn = {978-3-540-45375-8 978-3-540-46056-5},
  langid = {english},
  file = {/home/ture/Zotero/storage/B8WJKRZA/Kocsis and Szepesv√°ri - 2006 - Bandit Based Monte-Carlo Planning.pdf}
}

@incollection{hutchison_parallel_2010,
  title = {Parallel {{Minimax Tree Searching}} on {{GPU}}},
  booktitle = {Parallel {{Processing}} and {{Applied Mathematics}}},
  author = {Rocki, Kamil and Suda, Reiji},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Wasniewski, Jerzy},
  year = {2010},
  volume = {6067},
  pages = {449--456},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14390-8_47},
  abstract = {The paper describes results of minimax tree searching algorithm implemented within CUDA platform. The problem regards move choice strategy in the game of Reversi. The parallelization scheme and performance aspects are discussed, focusing mainly on warp divergence problem and data transfer size. Moreover, a method of minimizing warp divergence and performance degradation is described. The paper contains both the results of test performed on multiple CPUs and GPUs. Additionally, it discusses {$\alpha\beta$} parallel pruning implementation.},
  isbn = {978-3-642-14389-2 978-3-642-14390-8},
  langid = {english},
  file = {/home/ture/Zotero/storage/5WUKFQ42/Rocki and Suda - 2010 - Parallel Minimax Tree Searching on GPU.pdf}
}

@inproceedings{ilin_abstraction_2017,
  title = {Abstraction Hierarchy in Deep Learning Neural Networks},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Ilin, Roman and Watson, Thomas and Kozma, Robert},
  year = {2017},
  month = may,
  pages = {768--774},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/IJCNN.2017.7965929},
  isbn = {978-1-5090-6182-2},
  langid = {english},
  file = {/home/ture/Zotero/storage/2X8CI7QX/Ilin et al. - 2017 - Abstraction hierarchy in deep learning neural netw.pdf}
}

@misc{jebara_for_2020,
  title = {For {{Your Ears Only}}: {{Personalizing Spotify Home}} with {{Machine Learning}}},
  shorttitle = {For {{Your Ears Only}}},
  author = {Jebara, Tony},
  year = {2020},
  month = jan,
  journal = {Spotify Engineering},
  abstract = {This article is based on the keynote given by Tony Jebara at TensorFlow World in Santa Clara, California, October 2019. You can watch the presentation here. Machine learning is at the heart of everything we do at Spotify. Especially on Spotify Home, where it enables us to personalize the user exp},
  chapter = {Machine Learning},
  howpublished = {https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/},
  langid = {american},
  file = {/home/ture/Zotero/storage/HZ2CN32E/for-your-ears-only-personalizing-spotify-home-with-machine-learning.html}
}

@article{JMLR:v9:vandermaaten08a,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  file = {/home/ture/Zotero/storage/XIYLPMQH/vandermaaten08a.pdf}
}

@article{kahn_uncertainty-aware_2017,
  title = {Uncertainty-{{Aware Reinforcement Learning}} for {{Collision Avoidance}}},
  author = {Kahn, Gregory and Villaflor, Adam and Pong, Vitchyr and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.01182 [cs]},
  eprint = {1702.01182},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a realworld RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/ture/Zotero/storage/H4546GT5/Kahn et al. - 2017 - Uncertainty-Aware Reinforcement Learning for Colli.pdf}
}

@misc{karpathy_peek_2017,
  title = {A {{Peek}} at {{Trends}} in {{Machine Learning}}},
  author = {Karpathy, Andrej},
  year = {2017},
  month = apr,
  journal = {Medium},
  abstract = {Have you looked at Google Trends? It's pretty cool \textemdash{} you enter some keywords and see how Google Searches of that term vary through time. I\ldots},
  langid = {english},
  file = {/home/ture/Zotero/storage/7VTC8RYF/a-peek-at-trends-in-machine-learning-ab8a1085a106.html}
}

@incollection{knuth_early_1980,
  title = {The {{Early Development}} of {{Programming Languages}}},
  booktitle = {A {{History}} of {{Computing}} in the {{Twentieth Century}}},
  author = {Knuth, DONALD E. and Pardo, LUIS TRABB},
  editor = {Metropolis, N. and Howlett, J. and Rota, GIAN-CARLO},
  year = {1980},
  month = jan,
  pages = {197--273},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  doi = {10.1016/B978-0-12-491650-0.50019-8},
  abstract = {This paper surveys the evolution of ``high-lever'' programming languages during the first decade of computer programming activity. We discuss the contributions of Zuse in 1945 (the ``Plankalk\"ul''), Goldstine and von Neumann in 1946 (``Flow Diagrams''), Curry in 1948 (``Composition''), Mauchly et al. in 1949 (``Short Code''), Burks in 1950 (``Intermediate PL''), Rutishauser in 1951 (``Klammerausdr\"ucke''), B\"ohm in 1951 (``Formules''), Glennie in 1952 (``AUTOCODE''), Hopper et al. in 1953 (``A-2''), Laning and Zierler in 1953 (``Algebraic Interpreter''), Backus et al. in 1954\textendash 1957 (``FORTRAN''), Brooker in 1954 (``Mark I AUTOCODE''), Kamynin and L\^iubimskii in 1954 (``{$\Pi\Pi$}-2''), Ershov in 1955 (``{$\Pi\Pi$}''), Grems and Porter in 1955 (``BACAIC''), Elsworth et al. in 1955 (``Kompiler 2''), Blum in 1956 (``ADES''), Perlis et al. in 1956 (``IT''), Katz et al. in 1956\textendash 1958 (``MATH-MATIC''), Bauer and Samelson in 1956\textendash 1958 (U.S. Patent 3,047,228). The principal features of each contribution are illustrated and discussed. For purposes of comparison, a particular fixed algorithm has been encoded (as far as possible) in each of the languages. This research is based primarily on unpublished source materials, and the authors hope that they have been able to compile a fairly complete picture of the early developments in this area.},
  isbn = {978-0-12-491650-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/MP48ABS4/STAN-CS-76-562_EarlyDevelPgmgLang_Aug76.pdf;/home/ture/Zotero/storage/B9MRYESM/B9780124916500500198.html}
}

@article{kochenderfer_algorithms_nodate,
  title = {Algorithms for {{Decision Making}}},
  author = {Kochenderfer, Mykel J and Wheeler, Tim A and Wray, Kyle H},
  pages = {690},
  langid = {english},
  file = {/home/ture/Zotero/storage/8EJIZEXH/Kochenderfer et al. - Algorithms for Decision Making.pdf}
}

@inproceedings{kocsis_bandit_2006,
  title = {Bandit {{Based Monte-Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {282--293},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_29},
  abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  isbn = {978-3-540-46056-5},
  langid = {english},
  keywords = {Bandit Problem,Drift Condition,Failure Probability,Multiarmed Bandit,Multiarmed Bandit Problem},
  file = {/home/ture/Zotero/storage/9TILQRUN/Kocsis and Szepesv√°ri - 2006 - Bandit Based Monte-Carlo Planning.pdf}
}

@article{krizhevsky_imagenet_2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/ture/Zotero/storage/QSMPK8RF/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{krizhevsky_imagenet_2017-1,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/ture/Zotero/storage/YLRRPH4R/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Mathematics and computing Subject\_term\_id: computer-science;mathematics-and-computing},
  file = {/home/ture/Zotero/storage/C6RF72LV/LeCun et al. - 2015 - Deep learning.pdf;/home/ture/Zotero/storage/QSMZVATG/nature14539.html}
}

@techreport{lee_abalone_2005,
  title = {Abalone \textendash{{Final Project Report}}},
  author = {Lee, Benson and Noh, Hyun Joo},
  year = {2005},
  pages = {11},
  langid = {english},
  file = {/home/ture/Zotero/storage/D27L8CN9/Lee - Abalone ‚ÄìFinal Project Report.pdf}
}

@inproceedings{lemmens_constructing_2005,
  title = {Constructing an Abalone Game-Playing Agent},
  booktitle = {Bachelor {{Conference Knowledge Engineering}}, {{Universiteit Maastricht}}},
  author = {Lemmens, NPPM},
  year = {2005},
  publisher = {{Citeseer}},
  file = {/home/ture/Zotero/storage/9ME8UC3J/Lemmens - 2005 - Constructing an abalone game-playing agent.pdf}
}

@article{mirhoseini_graph_2021,
  title = {A Graph Placement Methodology for Fast Chip Design},
  author = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
  year = {2021},
  month = jun,
  journal = {Nature},
  volume = {594},
  number = {7862},
  pages = {207--212},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03544-w},
  abstract = {Chip floorplanning is the engineering task of designing the physical layout of a computer chip. Despite five decades of research1, chip floorplanning has defied automation, requiring months of intense effort by physical design engineers to produce manufacturable layouts. Here we present a deep~reinforcement~learning approach to chip floorplanning. In under six hours, our method automatically generates chip floorplans that are superior or comparable to those produced by humans in all key metrics, including power consumption, performance and chip area. To achieve this, we pose chip floorplanning as a reinforcement~learning problem, and develop an edge-based graph convolutional neural network architecture capable of learning rich and transferable representations of the chip. As a result, our method utilizes past experience to become better and faster at solving new instances of the problem, allowing chip design to be performed by artificial agents with more experience than any human designer. Our method was used to design the next generation of Google's artificial intelligence (AI) accelerators, and has the potential to save thousands of hours of human effort for each new generation. Finally, we believe that more powerful AI-designed hardware will fuel advances in AI, creating a symbiotic relationship between the two fields. Machine learning tools are used to greatly accelerate chip layout design, by posing chip floorplanning as a reinforcement~learning problem and using neural networks to generate high-performance chip layouts.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Electrical and electronic engineering},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Electrical and electronic engineering Subject\_term\_id: computational-science;electrical-and-electronic-engineering},
  file = {/home/ture/Zotero/storage/ZVGZGDQ7/s41586-021-03544-w.html}
}

@techreport{mizrachi_introduction_2017,
  title = {Introduction to Artificial Intelligence {{Final Project}}},
  author = {Mizrachi, Rubi and Golran, Guy and Jacobi, Omer and Zats, Rom},
  year = {2017},
  institution = {{The Hebrew University of Jerusalem}},
  file = {/home/ture/Zotero/storage/N7P92TVG/report.pdf}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  langid = {english},
  file = {/home/ture/Zotero/storage/3W78FHUN/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.5602 [cs]},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/HTGDW7YD/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf}
}

@article{moore_cramming_2006,
  title = {Cramming More Components onto Integrated Circuits, {{Reprinted}} from {{Electronics}}, Volume 38, Number 8, {{April}} 19, 1965, Pp.114 Ff.},
  author = {Moore, Gordon E.},
  year = {2006},
  month = sep,
  journal = {IEEE Solid-State Circuits Society Newsletter},
  volume = {11},
  number = {3},
  pages = {33--35},
  issn = {1098-4232},
  doi = {10.1109/N-SSC.2006.4785860},
  abstract = {Moore's theories about the future of transistor technology first appeared in Electronics magazine in April 1965. Termed a "law" years later by Caltech professor Carver Mead, Moore's Law went on to become a self-fulfilling prophecy.},
  keywords = {Computers,Data mining,Films,Heating,Integrated circuits,Microwave amplifiers,Silicon},
  file = {/home/ture/Zotero/storage/H5IGTLDG/4785860.html}
}

@book{moroney_ai_2020,
  title = {{{AI}} and {{Machine Learning}} for {{Coders}}: {{A Programmer}}'s {{Guide}} to {{Artificial Intelligence}}},
  shorttitle = {{{AI}} and {{Machine Learning}} for {{Coders}}},
  author = {Moroney, Laurence},
  year = {2020},
  publisher = {{O'Reilly}},
  abstract = {If you're looking to make a career move from programmer to AI specialist, this is the ideal place to start. Based on Laurence Moroney's extremely successful AI courses, this introductory book provides a hands-on, code-first approach to help you build confidence while you learn key topics. You'll understand how to implement the most common scenarios in machine learning, such as computer vision, natural language processing (NLP), and sequence modeling for web, mobile, cloud, and embedded runtimes. Most books on machine learning begin with a daunting amount of advanced math. This guide is built on practical lessons that let you work directly with the code. You'll learn:  How to build models with TensorFlow using skills that employers desire The basics of machine learning by working with code samples How to implement computer vision, including feature detection in images How to use NLP to tokenize and sequence words and sentences Methods for embedding models in Android and iOS How to serve models over the web and in the cloud with TensorFlow Serving},
  googlebooks = {462OzQEACAAJ},
  isbn = {978-1-4920-7819-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Information Theory,Computers / Machine Theory},
  file = {/home/ture/Zotero/storage/P6Q9E7U7/Laurence Moroney - AI and Machine Learning for Coders_ A Programmer's Guide to Artificial Intelligence-O'Reilly Media (2020).pdf}
}

@article{muller_computer_2002,
  title = {Computer {{Go}}},
  author = {M{\"u}ller, Martin},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {145--179},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00121-7},
  abstract = {Computer Go is one of the biggest challenges faced by game programmers. This survey describes the typical components of a Go program, and discusses knowledge representation, search methods and techniques for solving specific subproblems in this domain. Along with a summary of the development of computer Go in recent years, areas for future research are pointed out.},
  langid = {english},
  keywords = {Computer Go,Game tree search,Go programs,Knowledge representation}
}

@article{nijssen_writing_nodate,
  title = {Writing a {{Bachelor Thesis}} in {{Computer Science}}},
  author = {Nijssen, Siegfried},
  pages = {37},
  langid = {english},
  file = {/home/ture/Zotero/storage/TCQ856MM/Nijssen - Writing a Bachelor Thesis in Computer Science.pdf}
}

@book{nilsson_artificial_1998,
  title = {Artificial {{Intelligence}}: {{A New Synthesis}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Nilsson, Nils J.},
  year = {1998},
  month = apr,
  publisher = {{Elsevier}},
  abstract = {Intelligent agents are employed as the central characters in this new introductory text. Beginning with elementary reactive agents, Nilsson gradually increases their cognitive horsepower to illustrate the most important and lasting ideas in AI. Neural networks, genetic programming, computer vision, heuristic search, knowledge representation and reasoning, Bayes networks, planning, and language understanding are each revealed through the growing capabilities of these agents. The book provides a refreshing and motivating new synthesis of the field by one of AI's master expositors and leading researchers. Artificial Intelligence: A New Synthesis takes the reader on a complete tour of this intriguing new world of AI. An evolutionary approach provides a unifying theme  Thorough coverage of important AI ideas, old and new Frequent use of examples and illustrative diagrams Extensive coverage of machine learning methods throughout the text Citations to over 500 references Comprehensive index},
  googlebooks = {lr1Po3DJZNIC},
  isbn = {978-0-08-049945-1},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General},
  file = {/home/ture/Zotero/storage/EG5D3CPW/Nils J. Nilsson - Artificial Intelligence_ A New Synthesis -Morgan Kaufmann Publishers, Inc. (1998).pdf}
}

@misc{noauthor_abalone_2020,
  title = {Abalone (Board Game)},
  year = {2020},
  month = dec,
  journal = {Wikipedia},
  abstract = {Abalone is a two-player abstract strategy board game designed by Michel Lalet and Laurent L\'evi in 1987. Players are represented by opposing black and white marbles on a hexagonal board with the objective of pushing six of the opponent's marbles off the edge of the board. Abalone was published in 1990 and has sold more than 4.5 million units. The year it was published it received one of the first Mensa Select awards. It is currently sold in more than thirty countries.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Abalone\_(board\_game)\&oldid=994557581},
  langid = {english},
  annotation = {Page Version ID: 994557581},
  file = {/home/ture/Zotero/storage/5H8G9LDZ/index.html}
}

@misc{noauthor_cloud_nodate,
  title = {Cloud {{Tpu}} | {{Cloud TPU}}},
  journal = {Google Cloud},
  abstract = {Custom-built for machine learning workloads, Cloud TPUs accelerate training and inference at scale.},
  howpublished = {https://cloud.google.com/tpu},
  langid = {english},
  file = {/home/ture/Zotero/storage/5EW4VYZY/tpu.html}
}

@misc{noauthor_fig_nodate,
  title = {Fig. 1. {{Outline}} of a {{Monte-Carlo Tree Search}}.},
  journal = {ResearchGate},
  abstract = {Download scientific diagram | Outline of a Monte-Carlo Tree Search. ~ from publication: Progressive Strategies for Monte-Carlo Tree Search | Monte-Carlo Tree Search (MCTS) is a new best-first search guided by the results of Monte-Carlo simulations. In this article, we introduce two progressive strategies for MCTS, called progressive bias and progressive unpruning. They enable the use of relatively time-expensive... | Trees, MCTS and Computer | ResearchGate, the professional network for scientists.},
  howpublished = {https://www.researchgate.net/figure/Outline-of-a-Monte-Carlo-Tree-Search\_fig1\_23751563},
  langid = {english},
  file = {/home/ture/Zotero/storage/6UKJFTPM/Outline-of-a-Monte-Carlo-Tree-Search_fig1_23751563.html}
}

@misc{noauthor_free_nodate,
  title = {Free {{Trial}} and {{Free Tier}}},
  journal = {Google Cloud},
  abstract = {Start building on GCP with a Free Trial that includes \$300 in credits. Plus, enjoy access to 20+ select products, like Compute Engine, free of charge.},
  howpublished = {https://cloud.google.com/free},
  langid = {english},
  file = {/home/ture/Zotero/storage/CVDRRM54/free.html}
}

@misc{noauthor_globalinterpreterlock_nodate,
  title = {{{GlobalInterpreterLock}} - {{Python Wiki}}},
  howpublished = {https://wiki.python.org/moin/GlobalInterpreterLock},
  file = {/home/ture/Zotero/storage/7SQ4W2XZ/GlobalInterpreterLock.html}
}

@article{noauthor_go_2022,
  title = {Go (Game)},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  abstract = {Go or Weiqi, Weichi (simplified Chinese: Âõ¥Ê£ã; traditional Chinese: ÂúçÊ£ã; pinyin: w\'eiq\'i) is an abstract strategy board game for two players in which the aim is to surround more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day. A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go and over 20 million current players, the majority of whom live in East Asia.The playing pieces are called stones. One player uses the white stones and the other, black. The players take turns placing the stones on the vacant intersections (points) of a board. Once placed on the board, stones may not be moved, but stones are removed from the board if the stone (or group of stones) is surrounded by opposing stones on all orthogonally adjacent points, in which case the stone is captured. The game proceeds until neither player wishes to make another move. When a game concludes, the winner is determined by counting each player's surrounded territory along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second). Games may also be terminated by resignation. The standard Go board has a 19\texttimes 19 grid of lines, containing 361 points. Beginners often play on smaller 9\texttimes 9 and 13\texttimes 13 boards, and archaeological evidence shows that the game was played in earlier centuries on a board with a 17\texttimes 17 grid. However, boards with a 19\texttimes 19 grid had become standard by the time the game reached Korea in the 5th century CE and Japan in the 7th century CE.Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan (c. 4th century BCE).Despite its relatively simple rules, Go is extremely complex. Compared to chess, Go has both a larger board with more scope for play and longer games and, on average, many more alternatives to consider per move. The number of legal board positions in Go has been calculated to be approximately 2.1\texttimes 10170, which is vastly greater than the number of atoms in the observable universe, estimated to be of the order of 1080.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1065445067}
}

@misc{noauthor_grayskull_nodate,
  title = {Grayskull},
  journal = {Tenstorrent},
  abstract = {We'll be selling Tenstorrent AI PCIe cards, workstations, and servers soon. Stay tuned for availability.},
  howpublished = {https://tenstorrent.com/grayskull/},
  langid = {american},
  file = {/home/ture/Zotero/storage/A8RMSDJJ/grayskull.html}
}

@article{noauthor_huangs_2021,
  title = {Huang's Law},
  year = {2021},
  month = oct,
  journal = {Wikipedia},
  abstract = {Huang's law is an observation in computer science and engineering that advancements in graphics processing units (GPU) are growing at a rate much faster than with traditional central processing units (CPU). The observation is in contrast to Moore's law that predicted the number of transistors in a dense integrated circuit (IC) doubles about every two years. Huang's law states that the performance of GPUs will more than double every two years.  The hypothesis is subject to questions about its validity.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1052522933},
  file = {/home/ture/Zotero/storage/YBLSLKJR/index.html}
}

@misc{noauthor_lightelligence_nodate,
  title = {Lightelligence},
  journal = {Lightelligence},
  howpublished = {https://www.lightelligence.ai/},
  langid = {american},
  file = {/home/ture/Zotero/storage/SXDWDNP2/www.lightelligence.ai.html}
}

@misc{noauthor_lightmatter_nodate,
  title = {Lightmatter},
  journal = {Lightmatter},
  abstract = {The photonic (super)computer company.},
  howpublished = {https://lightmatter.co/},
  langid = {american},
  file = {/home/ture/Zotero/storage/C676JRSH/lightmatter.co.html}
}

@misc{noauthor_peer_nodate,
  title = {Peer {{Sommerlund}} / Haliotis},
  abstract = {Abalone board game library},
  file = {/home/ture/Zotero/storage/S3KRSZD7/haliotis.html}
}

@misc{noauthor_red_nodate,
  title = {Red {{Blob Games}}: {{Hexagonal Grids}}},
  shorttitle = {Red {{Blob Games}}},
  abstract = {Guide to math, algorithms, and code for hexagonal grids in games},
  howpublished = {https://www.redblobgames.com/grids/hexagons/},
  langid = {english},
  file = {/home/ture/Zotero/storage/6XJJDMJ9/hexagons.html}
}

@misc{noauthor_simple_nodate,
  title = {Simple {{Alpha Zero}}},
  howpublished = {https://web.stanford.edu/\textasciitilde surag/posts/alphazero.html},
  file = {/home/ture/Zotero/storage/S73RNETQ/alphazero.html}
}

@misc{noauthor_state_2019,
  title = {The {{State}} of {{Machine Learning Frameworks}} in 2019},
  year = {2019},
  month = oct,
  journal = {The Gradient},
  abstract = {Since deep learning regained prominence in 2012, many machine learning frameworks have clamored to become the new favorite among researchers and industry practitioners. From the early academic outputs Caffe and Theano to the massive industry-backed PyTorch and TensorFlow, this deluge of options makes it difficult to keep track of what},
  howpublished = {https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/},
  langid = {english},
  file = {/home/ture/Zotero/storage/NCIQRY5P/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry.html}
}

@article{noauthor_stockfish_2021,
  title = {Stockfish (Chess)},
  year = {2021},
  month = dec,
  journal = {Wikipedia},
  abstract = {Stockfish is a free and open-source chess engine, available for various desktop and mobile platforms. It is developed by Marco Costalba, Joona Kiiski, Gary Linscott, Tord Romstad, St\'ephane Nicolet, Stefan Geschwentner, and Joost VandeVondele, with many contributions from a community of open-source developers.Stockfish is consistently ranked first or near the top of most chess-engine rating lists and is the strongest CPU chess engine in the world. It won the unofficial world computer chess championships in seasons 6 (2014),  9 (2016),  11 (2018),  12 (2018),  13 (2018), 14 (2019), 16 (2019), 18 (2020), 19 (2020), 20 (2020-21) and 21 (2021). It finished runner-up in seasons 5 (2013), 7 (2014),  8 (2015), 15 (2019) and 17 (2020). Stockfish is derived from Glaurung, an open-source engine by Tord Romstad released in 2004.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1060102676},
  file = {/home/ture/Zotero/storage/2MTIP95L/index.html}
}

@misc{noauthor_stockfish_nodate,
  title = {Stockfish - {{Open Source Chess Engine}}},
  howpublished = {https://stockfishchess.org/},
  file = {/home/ture/Zotero/storage/KNMZJN5Q/stockfishchess.org.html}
}

@misc{noauthor_system_nodate,
  title = {System {{Architecture}} | {{Cloud TPU}}},
  journal = {Google Cloud},
  howpublished = {https://cloud.google.com/tpu/docs/system-architecture-tpu-vm},
  langid = {english},
  file = {/home/ture/Zotero/storage/THV6PYKD/system-architecture-tpu-vm.html}
}

@misc{noauthor_tfkerasmodel_nodate,
  title = {Tf.Keras.{{Model}} | {{TensorFlow Core}} v2.7.0},
  journal = {TensorFlow},
  abstract = {Model groups layers into an object with training and inference features.},
  howpublished = {https://www.tensorflow.org/api\_docs/python/tf/keras/Model\#predict},
  langid = {english},
  file = {/home/ture/Zotero/storage/S6HFUJLM/Model.html}
}

@misc{noauthor_tpu_nodate,
  title = {{{TPU Research Cloud}}},
  howpublished = {https://sites.research.google/trc/},
  file = {/home/ture/Zotero/storage/96VFE5C2/trc.html}
}

@misc{noauthor_wolframalpha_nodate,
  title = {Wolfram|{{Alpha Widgets}}: "{{Plot}} Two Functions" - {{Free Mathematics Widget}}},
  howpublished = {https://www.wolframalpha.com/widgets/view.jsp?id=59752a7f2c9aa5d375de1f1d13a3f5c4},
  file = {/home/ture/Zotero/storage/FTHVWHNC/view.html}
}

@misc{noauthor_zobrist_nodate,
  title = {Zobrist {{Hashing}} - {{Chessprogramming}} Wiki},
  howpublished = {https://www.chessprogramming.org/Zobrist\_Hashing},
  file = {/home/ture/Zotero/storage/E9U4YWBE/Zobrist_Hashing.html}
}

@article{ozcan_simple_2004-1,
  title = {A {{Simple Intelligent Agent}} for {{Playing Abalone Game}}: {{ABLA}}},
  author = {Ozcan, Ender and Hulagu, Berk},
  year = {2004},
  pages = {10},
  abstract = {Forming winning strategies for board games requires good heuristics and fast search algorithms on game trees. High branching factors and the need for looking deeper in game trees are overwhelming, even for today's high performance PC's. Therefore, better game-plays are only available with better algorithms and heuristics for an ordinary player, not with faster machines. Abalone is a recent two-person strategy game. Initial evaluations point out that the branching factor is larger than the chess. In this paper, a new heuristic used by a simple intelligent agent for playing Abalone game, named as ABLA is introduced. ABLA's performance is promising as compared to existing computerized Abalone players.},
  langid = {english},
  file = {/home/ture/Zotero/storage/Q5LJM9T4/Ozcan and Hulagu - A Simple Intelligent Agent for Playing Abalone Gam.pdf}
}

@article{papadopoulos_exploring_2012,
  title = {Exploring {{Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha-Beta Search}}},
  author = {Papadopoulos, Athanasios and Toumpas, Konstantinos and Chrysopoulos, Antonios and Mitkas, Pericles A},
  year = {2012},
  journal = {IEEE Conference on Computational Intelligence and Games},
  pages = {8},
  abstract = {This paper discusses the design and implementation of a highly efficient MiniMax algorithm for the game Abalone. For perfect information games with relatively low branching factor for their decision tree (such as Chess, Checkers etc.) and a highly accurate evaluation function, Alpha-Beta search proved to be far more efficient than Monte Carlo Tree Search. In recent years many new techniques have been developed to improve the efficiency of the Alpha-Beta tree, applied to a variety of scientific fields. This paper explores several techniques for increasing the efficiency of Alpha-Beta Search on the board game of Abalone while introducing some new innovative techniques that proved to be very effective. The main idea behind them is the incorporation of probabilistic features to the otherwise deterministic AlphaBeta search.},
  langid = {english},
  file = {/home/ture/Zotero/storage/6NQG9BJM/Papadopoulos et al. - 2012 - Exploring Optimization Strategies in Board Game Ab.pdf}
}

@article{petosa_multiplayer_2019,
  title = {Multiplayer {{AlphaZero}}},
  author = {Petosa, Nick and Balch, Tucker},
  year = {2019},
  month = dec,
  journal = {arXiv:1910.13012 [cs]},
  eprint = {1910.13012},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The AlphaZero algorithm has achieved superhuman performance in two-player, deterministic, zero-sum games where perfect information of the game state is available. This success has been demonstrated in Chess, Shogi, and Go where learning occurs solely through self-play. Many real-world applications (e.g., equity trading) require the consideration of a multiplayer environment. In this work, we suggest novel modifications of the AlphaZero algorithm to support multiplayer environments, and evaluate the approach in two simple 3-player games. Our experiments show that multiplayer AlphaZero learns successfully and consistently outperforms a competing approach: Monte Carlo tree search. These results suggest that our modified AlphaZero can learn effective strategies in multiplayer game scenarios. Our work supports the use of AlphaZero in multiplayer games and suggests future research for more complex environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/ture/Zotero/storage/LTAZRQ7M/Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf}
}

@inproceedings{pinto_supersizing_2016,
  title = {Supersizing Self-Supervision: {{Learning}} to Grasp from {{50K}} Tries and 700 Robot Hours},
  shorttitle = {Supersizing Self-Supervision},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Pinto, Lerrel and Gupta, Abhinav},
  year = {2016},
  month = may,
  pages = {3406--3413},
  publisher = {{IEEE}},
  address = {{Stockholm, Sweden}},
  doi = {10.1109/ICRA.2016.7487517},
  isbn = {978-1-4673-8026-3},
  file = {/home/ture/Zotero/storage/KC4RHQPR/Pinto and Gupta - 2016 - Supersizing self-supervision Learning to grasp fr.pdf}
}

@article{rosin_multi-armed_2011,
  title = {Multi-Armed Bandits with Episode Context},
  author = {Rosin, Christopher D.},
  year = {2011},
  month = mar,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {3},
  pages = {203--230},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9258-6},
  abstract = {A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0, 1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multiarmed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm.},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKNKB6JG/Rosin - 2011 - Multi-armed bandits with episode context.pdf}
}

@article{rosin_multi-armed_2011-1,
  title = {Multi-Armed Bandits with Episode Context},
  author = {Rosin, Christopher D.},
  year = {2011},
  month = mar,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {3},
  pages = {203--230},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9258-6},
  langid = {english},
  file = {/home/ture/Zotero/storage/P9NGITQI/Rosin - 2011 - Multi-armed bandits with episode context.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2021},
  edition = {Fourth},
  publisher = {{Pearson Education, Inc}},
  abstract = {Artificial Intelligence: A Modern Approach offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.},
  isbn = {978-1-5376-0031-4},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKE25V5T/Stuart Russell, Peter Norvig - Artificial Intelligence_ A Modern Approach (4th Edition) (Pearson Series in Artifical Intelligence)-Language_ English (2020).pdf}
}

@article{schmid_player_2021,
  title = {Player of {{Games}}},
  author = {Schmid, Martin and Moravcik, Matej and Burch, Neil and Kadlec, Rudolf and Davidson, Josh and Waugh, Kevin and Bard, Nolan and Timbers, Finbarr and Lanctot, Marc and Holland, Zach and Davoodi, Elnaz and Christianson, Alden and Bowling, Michael},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03178 [cs]},
  eprint = {2112.03178},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Games have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/PS5G8X9G/Schmid et al. - 2021 - Player of Games.pdf;/home/ture/Zotero/storage/AJDRA7N2/2112.html}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3\textemdash the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4\textemdash the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi\textemdash canonical environments for high-performance planning\textemdash the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game. A reinforcement-learning algorithm that combines a tree-based search with a learned model achieves superhuman performance in high-performance planning and visually complex domains, without any knowledge of their underlying dynamics.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science Subject\_term\_id: computational-science;computer-science},
  file = {/home/ture/Zotero/storage/GZDB2LDU/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf;/home/ture/Zotero/storage/DAI2XJ6I/s41586-020-03051-4.html}
}

@misc{scriptim_scriptimabalone-boai_2021,
  title = {Scriptim/{{Abalone-BoAI}}},
  author = {Scriptim},
  year = {2021},
  month = apr,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT License         ,                 MIT License},
  keywords = {abalone,ai,ai-battle-game,artificial-intelligence,game,machine-learning,python,python3}
}

@article{shannon_xxii_1950,
  title = {{{XXII}}. {{Programming}} a Computer for Playing Chess},
  author = {Shannon, Claude E.},
  year = {1950},
  month = mar,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {41},
  number = {314},
  pages = {256--275},
  issn = {1941-5982, 1941-5990},
  doi = {10.1080/14786445008521796},
  langid = {english},
  file = {/home/ture/Zotero/storage/XZSDKV92/Shannon - 1950 - XXII. Programming a computer for playing chess.pdf}
}

@misc{siddiqi_ml_2019,
  title = {{{ML Platform Meetup}}: {{Infra}} for {{Contextual Bandits}} and {{Reinforcement Learning}}},
  shorttitle = {{{ML Platform Meetup}}},
  author = {Siddiqi, Faisal},
  year = {2019},
  month = oct,
  journal = {Medium},
  howpublished = {https://netflixtechblog.com/ml-platform-meetup-infra-for-contextual-bandits-and-reinforcement-learning-4a90305948ef},
  langid = {english},
  file = {/home/ture/Zotero/storage/X7WWS68K/ml-platform-meetup-infra-for-contextual-bandits-and-reinforcement-learning-4a90305948ef.html}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  langid = {english},
  file = {/home/ture/Zotero/storage/MD3YXY65/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {/home/ture/Zotero/storage/5J8BTAU2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/ture/Zotero/storage/ZC9N7QNB/nature24270.html}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \^a¬Ä¬úthinned\^a¬Ä¬ù networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/home/ture/Zotero/storage/8FGMBZW2/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/ture/Zotero/storage/BBRQKJF4/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{tesauro_td-gammon_1994,
  title = {{{TD-Gammon}}, a {{Self-Teaching Backgammon Program}}, {{Achieves Master-Level Play}}},
  author = {Tesauro, Gerald},
  year = {1994},
  month = mar,
  journal = {Neural Computation},
  volume = {6},
  number = {2},
  pages = {215--219},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.2.215},
  abstract = {TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD({$\lambda$}) reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a ``raw'' description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.},
  file = {/home/ture/Zotero/storage/M23ZN2RW/TD-Gammon-a-Self-Teaching-Backgammon-Program.html}
}

@techreport{thakoor_learning_nodate,
  type = {Final Project Report},
  title = {Learning to {{Play Othello Without Human Knowledge}}},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  institution = {{Stanford University}},
  abstract = {Game playing is a popular area within the field of artificial intelligence. Most agents in literature have hand-crafted features and are often trained on datasets obtained from expert human play. We implement a self- play based algorithm using neural networks for policy estimation and Monte Carlo Tree Search for policy im- provement, with no input human knowledge that learns to play Othello. We evaluate our learning algorithm for 6x6 and 8x8 versions of the game of Othello. Our work is compared with random and greedy baselines, as well as a minimax agent that uses a hand-crafted scoring function, and achieves impressive results. Further, our agent for the 6x6 version of Othello easily outperforms humans when tested against it.},
  file = {/home/ture/Zotero/storage/THCSVXJR/writeup.pdf}
}

@misc{thakoor_suragnairalpha-zero-general_nodate,
  title = {Suragnair/Alpha-Zero-General: {{A}} Clean Implementation Based on {{AlphaZero}} for Any Game in Any Framework + Tutorial + {{Othello}}/{{Gobang}}/{{TicTacToe}}/{{Connect4}} and More},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  howpublished = {https://github.com/suragnair/alpha-zero-general},
  file = {/home/ture/Zotero/storage/M78KG5GT/alpha-zero-general.html}
}

@misc{towzeur_towzeurgym-abalone_2021,
  title = {Towzeur/Gym-Abalone},
  author = {{towzeur}},
  year = {2021},
  month = jan,
  abstract = {An environment of the board game Abalone using OpenAI's Gym API},
  keywords = {abalone,gym,open-ai,reinforcement-learning}
}

@article{turing_icomputing_1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  file = {/home/ture/Zotero/storage/RB33PS2W/TURING - 1950 - I.‚ÄîCOMPUTING MACHINERY AND INTELLIGENCE.pdf;/home/ture/Zotero/storage/6RJMJ3NW/986238.html}
}

@book{vasilev_python_2019,
  title = {Python Deep Learning: Exploring Deep Learning Techniques and Neural Network Architectures with {{PyTorch}}, {{Keras}}, and {{TensorFlow}}},
  shorttitle = {Python Deep Learning},
  author = {Vasilev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
  year = {2019},
  edition = {Second edition},
  publisher = {{Packt Publishing Limited}},
  address = {{Birmingham Mumbai}},
  isbn = {978-1-78934-846-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/TP5DPUZ2/Vasilev et al. - 2019 - Python deep learning exploring deep learning tech.pdf}
}

@misc{verloop_abaloneai_nodate,
  title = {{{AbaloneAI}}},
  author = {Verloop, Michiel},
  journal = {GitHub},
  abstract = {An AI for the game Abalone, based on the paper \&quot;Exploring optimization strategies in board game Abalone for Alpha-Beta search\&quot;. - AbaloneAI/abalone/src/controller at main {$\cdot$} MichielVerloop...},
  howpublished = {https://github.com/MichielVerloop/AbaloneAI},
  langid = {english},
  file = {/home/ture/Zotero/storage/YD5EPDNL/AbaloneAI.html}
}

@article{verloop_critical_nodate,
  title = {A {{Critical Review}}: {{Exploring Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha-Beta Search}}},
  author = {Verloop, Michiel},
  pages = {49},
  langid = {english},
  file = {/home/ture/Zotero/storage/EK73C5BP/Verloop - A Critical Review Exploring Optimization Strategi.pdf}
}

@article{vinyals_grandmaster_2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  langid = {english},
  file = {/home/ture/Zotero/storage/Y2K2TQCF/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf}
}

@article{wang_adaptive_2021,
  title = {Adaptive {{Warm-Start MCTS}} in {{AlphaZero-like Deep Reinforcement Learning}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2021},
  month = may,
  journal = {arXiv:2105.06136 [cs]},
  eprint = {2105.06136},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {AlphaZero has achieved impressive performance in deep reinforcement learning by utilizing an architecture that combines search and training of a neural network in self-play. Many researchers are looking for ways to reproduce and improve results for other games/tasks. However, the architecture is designed to learn from scratch, tabula rasa, accepting a cold-start problem in self-play. Recently, a warmstart enhancement method for Monte Carlo Tree Search was proposed to improve the self-play starting phase. It employs a fixed parameter I to control the warm-start length. Improved performance was reported in small board games. In this paper we present results with an adaptive switch method. Experiments show that our approach works better than the fixed I , especially for ''deep,'' tactical, games (Othello and Connect Four). We conjecture that the adaptive value for I is also influenced by the size of the game, and that on average I will increase with game size. We conclude that AlphaZero-like deep reinforcement learning benefits from adaptive rollout based warm-start, as Rapid Action Value Estimate did for rollout-based reinforcement learning 15 years ago.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/T6AJBPW2/Wang et al. - 2021 - Adaptive Warm-Start MCTS in AlphaZero-like Deep Re.pdf}
}

@article{wang_dueling_2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  year = {2016},
  pages = {9},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  langid = {english},
  file = {/home/ture/Zotero/storage/WB3A8TH2/Wang et al. - Dueling Network Architectures for Deep Reinforceme.pdf}
}

@article{wang_warm-start_2020,
  title = {Warm-{{Start AlphaZero Self-Play Search Enhancements}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2020},
  journal = {arXiv:2004.12357 [cs]},
  volume = {12270},
  eprint = {2004.12357},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {528--542},
  doi = {10.1007/978-3-030-58115-2_37},
  abstract = {Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE) and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different (small) board games, with especially RAVE based variants playing strongly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ture/Zotero/storage/IC9JLYFX/Wang et al. - 2020 - Warm-Start AlphaZero Self-Play Search Enhancements.pdf;/home/ture/Zotero/storage/Y68GT8W8/2004.html}
}

@misc{weisstein_convolution_nodate,
  type = {Text},
  title = {Convolution},
  author = {Weisstein, Eric W.},
  publisher = {{Wolfram Research, Inc.}},
  abstract = {A convolution is an integral that expresses the amount of overlap of one function g as it is shifted over another function f. It therefore "blends" one function with another. For example, in synthesis imaging, the measured dirty map is a convolution of the "true" CLEAN map with the dirty beam (the Fourier transform of the sampling distribution). The convolution is sometimes also known by its German name, faltung ("folding"). Convolution is implemented in the...},
  copyright = {Copyright 1999-2021 Wolfram Research, Inc.  See https://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
  howpublished = {https://mathworld.wolfram.com/Convolution.html},
  langid = {english},
  file = {/home/ture/Zotero/storage/DBQNKUVR/Convolution.html}
}

@article{williams_simple_nodate,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J},
  pages = {28},
  abstract = {This article presents a general class of associativereinforcementlearning algorithmsfor connectionist networkscontainingstochasticunits. These algorithms,calledREINFORCEalgorithms,are shownto makeweight adjustmentsin a direction that lies alongthe gradient of expectedreinforcementin both immediate-reinforcement tasks and certain limited forms of delayed-reinforcementtasks, and they do this without explicitly computing gradient estimatesor even storing informationfrom which such estimates couldbe computed. Specificexamples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novelbut potentiallyinterestingin their own right. Also givenare resultsthat showhow such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerationsthat might be used to help developsimilar but potentially more powerfulreinforcement learning algorithms.},
  langid = {english},
  file = {/home/ture/Zotero/storage/Z8FUVGPU/Williams - Simple statistical gradient-following algorithms f.pdf}
}

@misc{xkcd_standards_nodate,
  title = {Standards},
  author = {XKCD},
  journal = {xkcd},
  howpublished = {https://xkcd.com/927/},
  file = {/home/ture/Zotero/storage/HWKJGXPI/927.html}
}

@techreport{yang_markov_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Markov {{Chain}} and {{Its Applications}}},
  author = {Yang, Xinye},
  year = {2019},
  month = mar,
  number = {ID 3562746},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3562746},
  abstract = {This paper will explore concepts of the Markov Chain and demonstrate its applications in probability prediction area and financial trend analysis. The historical background and the properties of the Markov's chain are analyzed.},
  langid = {english},
  keywords = {Linear Algebra,Markov Chain,probability},
  file = {/home/ture/Zotero/storage/UBIFVXSD/papers.html}
}

@inproceedings{zeng_learning_2018,
  title = {Learning {{Synergies Between Pushing}} and {{Grasping}} with {{Self-Supervised Deep Reinforcement Learning}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  year = {2018},
  month = oct,
  pages = {4238--4245},
  publisher = {{IEEE}},
  address = {{Madrid}},
  doi = {10.1109/IROS.2018.8593986},
  isbn = {978-1-5386-8094-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/CV2X2ASJ/Zeng et al. - 2018 - Learning Synergies Between Pushing and Grasping wi.pdf}
}


