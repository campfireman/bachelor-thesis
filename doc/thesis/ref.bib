
@misc{abalone_sa_abalone_nodate,
  title = {Abalone Rulebook},
  shorttitle = {Abalone {{Rulebook}}},
  author = {Abalone S.A.},
  copyright = {Copyright Abalone S.A.},
  howpublished = {https://cdn.1j1ju.com/medias/c2/b0/3a-abalone-rulebook.pdf},
  file = {/home/ture/Zotero/storage/JHEB8Q64/3a-abalone-rulebook.pdf}
}

@article{aichholzer_algorithmic_2002,
  title = {Algorithmic Fun-Abalone},
  author = {Aichholzer, Oswin and Aurenhammer, Franz and Werner, Tino},
  year = {2002},
  journal = {Special Issue on Foundations of Information Processing of TELEMATIK},
  volume = {1},
  pages = {4--6},
  file = {/home/ture/Zotero/storage/8QRDUB38/Aichholzer et al. - 2002 - Algorithmic fun-abalone.pdf}
}

@article{bruasdal_deep_2020,
  title = {Deep Reinforcement {{Learning Using Monte-Carlo Tree Search}} for {{Hex}} and {{Othello}}},
  author = {Bru{\aa}sdal, Henrik},
  year = {2020},
  publisher = {{NTNU}},
  abstract = {N\aa r Deepminds AlphaGo-program slo den menneskelige profesjonelle Go-spilleren Fan Hui i 2015 var dette et stort gjennombrudd for kunstig intelligens til spilling. Go hadde vist seg \aa{} motst\aa{} de teknikkene som lenge hadde sl\aa tt mennesker i spill som sjakk. Gjennom en nyskapende kombinasjon av dype nevrale nettverk, forsterkende l\ae ring og Monte Carlo-tres\o k ble Go endelig mestret. Kort tid etterp\aa{} kom AlphaGo Zero, som oppn\aa dde det samme ved \aa{} l\ae re utelukkende ved \aa{} spille mot seg selv, og AlphaZero, som generaliserte teknikken til andre spill. Dette arbeidet inneholder en n\o ye gjennomgang av disse systemene og arbeidet i feltet som ledet opp mot dem. Det g\aa r gjennom min egen implementasjon av denne teknikken og dens bruk i spillene Hex og Othello. Ved hjelp av denne implementasjonen har jeg unders\o kt rollen utrulling spiller i algoritmen. Dette var en sentral del av tidligere arbeid i feltet og fremdeles brukt i AlphaGo, men ikke i AlphaGo Zero og AlphaZero. Flere eksperimenter har blitt gjennomf\o rt for \aa{} f\aa{} empiriske data for om utrulling fremdeles kan v\ae re en gunstig del av denne nyskapende kombinasjonen av teknikker, og hvordan disse i s\aa{} fall b\o r gjennomf\o res. Selv om det var noen indikasjoner i dataene p\aa{} at utrulling har liten eller ingen positiv effect er disse resultatene stort sett ikke entydige. Noen svakheter i oppsettet har blitt identifisert og noen nye sp\o rsm\aa l har blitt oppdaget. Men arbeidet har resultert i et funksjonelt system som kan brukes til videre unders\o kelser av problemomr\aa det og enten gi mer entydige data eller innsikt i nye sp\o rsm\aa l.},
  langid = {english},
  annotation = {Accepted: 2021-09-15T16:00:35Z},
  file = {/home/ture/Zotero/storage/RXYGMGDB/Bru√•sdal - 2020 - Deep reinforcement Learning Using Monte-Carlo Tree.pdf;/home/ture/Zotero/storage/HSPTZDLR/2777474.html}
}

@misc{campfireman_campfiremanabalone-boai_2021,
  title = {Campfireman/{{Abalone-BoAI}}},
  author = {{campfireman}},
  year = {2021},
  month = jun,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT}
}

@article{campos_abalearn_2003,
  title = {Abalearn: {{Ecient Self-Play Learning}} of the Game {{Abalone}}},
  shorttitle = {Abalearn},
  author = {Campos, Pedro and Langlois, Thibault},
  year = {2003},
  abstract = {This paper presents Abalearn, a self-teaching Abalone pro- gram capable of automatically reaching an intermediate level of play without needing expert-labeled training examples or deep searches. Our approach is based on a reinforcement learning algorithm that is risk- seeking, since defensive players in Abalone tend to never end a game. We extend the risk-sensitive reinforcement learning framework in order to deal with large state spaces and we also propose a set of features that seem relevant for achieving a good level of play. We evaluate our approach using a fixed heuristic opponent as a bench- mark, pitting our agents against human players online and comparing samples of our agents at dierent times of training.},
  file = {/home/ture/Zotero/storage/BF4V5CJK/Campos and Langlois - 2009 - Abalearn Ecient Self-Play Learning of the game Ab.pdf}
}

@mastersthesis{chorus_implementing_2009,
  title = {Implementing a Computer Player for Abalone Using Alpha-Beta and Monte-Carlo Search},
  author = {Chorus, Pascal},
  year = {2009},
  school = {Citeseer},
  file = {/home/ture/Zotero/storage/YY9WTM47/Chorus - 2009 - Implementing a computer player for abalone using a.pdf}
}

@inproceedings{coulom_whole-history_2008,
  title = {Whole-{{History Rating}}: {{A Bayesian Rating System}} for {{Players}} of {{Time-Varying Strength}}},
  shorttitle = {Whole-{{History Rating}}},
  booktitle = {Computers and {{Games}}},
  author = {Coulom, R{\'e}mi},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {113--124},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_11},
  abstract = {Whole-History Rating (WHR) is a new method to estimate the time-varying strengths of players involved in paired comparisons. Like many variations of the Elo rating system, the whole-history approach is based on the dynamic Bradley-Terry model. But, instead of using incremental approximations, WHR directly computes the exact maximum a posteriori over the whole rating history of all players. This additional accuracy comes at a higher computational cost than traditional methods, but computation is still fast enough to be easily applied in real time to large-scale game servers (a new game is added in less than 0.001 second). Experiments demonstrate that, in comparison to Elo, Glicko, TrueSkill, and decayed-history algorithms, WHR produces better predictions.},
  isbn = {978-3-540-87608-3},
  langid = {english},
  keywords = {Incremental Algorithm,Prediction Rate,Rating Algorithm,Rating Uncertainty,Wiener Process},
  file = {/home/ture/Zotero/storage/HMUCYEXM/Coulom - 2008 - Whole-History Rating A Bayesian Rating System for.pdf;/home/ture/Zotero/storage/LSD7UUJH/Coulom - 2008 - Whole-History Rating A Bayesian Rating System for.pdf}
}

@misc{deepmind_match_nodate,
  title = {Match 1 - {{Google DeepMind Challenge Match}}: {{Lee Sedol}} vs {{AlphaGo}}},
  shorttitle = {Match 1 - {{Google DeepMind Challenge Match}}},
  author = {{DeepMind}},
  abstract = {Watch DeepMind's program AlphaGo take on the legendary Lee Sedol (9-dan pro), the top Go player of the past decade, in a \$1M 5-game challenge match in Seoul. This is the livestream for Match 1 to be played on: 9th March 13:00 KST (local), 04:00 GMT; note for US viewers this is the day before on: 8th March 20:00 PT, 23:00 ET.  In October 2015, AlphaGo became the first computer program ever to beat a professional Go player by winning 5-0 against the reigning 3-times European Champion Fan Hui (2-dan pro). That work was featured in a front cover article in the science journal Nature in January 2016. Match commentary by Michael Redmond (9-dan pro) and Chris Garlock.},
  howpublished = {https://www.youtube.com/watch?v=vFr3K2DORc8\&t=7020s}
}

@misc{deletang_multi_2019,
  title = {Multi {{GPU}}, Multi Process with {{Tensorflow}}},
  author = {Del{\'e}tang, Gr{\'e}goire},
  year = {2019},
  month = jul,
  journal = {Medium},
  abstract = {Tensorflow is a tremendous tool to experiment deep learning algorithms. But to exploit the power of deep learning, you need to leverage it\ldots},
  howpublished = {https://towardsdatascience.com/multi-gpu-multi-process-with-tensorflow-ba4cc2fe3ab7},
  langid = {english},
  file = {/home/ture/Zotero/storage/SNKFP4PJ/multi-gpu-multi-process-with-tensorflow-ba4cc2fe3ab7.html}
}

@article{demichelis_simple_2004,
  title = {The Simple Geometry of Perfect Information Games},
  author = {Demichelis, Stefano and Ritzberger, Klaus and Swinkels, Jeroen M.},
  year = {2004},
  month = jun,
  journal = {International Journal of Game Theory},
  volume = {32},
  number = {3},
  pages = {315--338},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s001820400169},
  langid = {english},
  file = {/home/ture/Zotero/storage/LE4XDBX5/Demichelis et al. - 2004 - The simple geometry of perfect information games.pdf}
}

@misc{foster_how_2019,
  title = {How to Build Your Own {{AlphaZero AI}} Using {{Python}} and {{Keras}}},
  author = {Foster, David},
  year = {2019},
  month = dec,
  journal = {Applied Data Science},
  abstract = {Teach a machine to learn Connect4 strategy through self-play and deep learning.},
  langid = {english},
  file = {/home/ture/Zotero/storage/QIH4KCYY/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188.html}
}

@misc{fridman_206_nodate,
  title = {\#206 - {{Ishan Misra}}: {{Self-Supervised Deep Learning}} in {{Computer Vision}} | {{Lex Fridman Podcast}}},
  shorttitle = {\#206 - {{Ishan Misra}}},
  author = {Fridman, Lex},
  number = {206},
  abstract = {Ishan Misra is a research scientist at FAIR working on self-supervised visual learning.},
  langid = {american},
  file = {/home/ture/Zotero/storage/2YLN3AXC/ishan-misra.html}
}

@article{gao_multithreaded_nodate,
  title = {Multithreaded {{Pruned Tree Search}} in {{Distributed Systems}}},
  author = {Gao, Yaoqing and Marsland, T A},
  pages = {11},
  abstract = {Although efficient support for data-parallel applications is relatively well established, it remains open how well to support irregular and dynamic problems where there are no regular data structures and communication patterns. Tree search is central to solving a variety of problems in artificial intelligence and an important subset of the irregular applications where tasks are frequently created and terminated. In this paper, we introduce the design of a multithreaded distributed runtime system. Efficiency and ease of parallel programming are the two primary goals. In our system, multithreading is used to specify the asynchronous behavior in parallel game tree search, and dynamic load balancing is employed for efficient performance.},
  langid = {english},
  file = {/home/ture/Zotero/storage/KYCV5AV9/Gao and Marsland - Multithreaded Pruned Tree Search in Distributed Sy.pdf}
}

@book{haugeland_artificial_1985,
  title = {Artificial Intelligence: The Very Idea},
  shorttitle = {Artificial Intelligence},
  author = {Haugeland, John},
  year = {1985},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-08153-5},
  langid = {english},
  lccn = {Q335 .H38 1985},
  keywords = {Artificial intelligence},
  file = {/home/ture/Zotero/storage/NX65VA94/Haugeland - 1985 - Artificial intelligence the very idea.pdf}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {/home/ture/Zotero/storage/3IVN96YN/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/home/ture/Zotero/storage/B4NWSVQW/7780459.html}
}

@misc{higgins_brief_2017,
  title = {A {{Brief History}} of {{Deep Blue}}, {{IBM}}'s {{Chess Computer}} | {{Mental Floss}}},
  author = {Higgins, Chris},
  year = {2017},
  month = jul,
  howpublished = {https://web.archive.org/web/20170803130439/https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer},
  file = {/home/ture/Zotero/storage/ZE4C9CJX/brief-history-deep-blue-ibms-chess-computer.html}
}

@article{holland_studying_2006,
  title = {Studying {{Complex Adaptive Systems}}},
  author = {Holland, John H.},
  year = {2006},
  month = mar,
  journal = {Journal of Systems Science and Complexity},
  volume = {19},
  number = {1},
  pages = {1--8},
  issn = {1009-6124, 1559-7067},
  doi = {10.1007/s11424-006-0001-z},
  abstract = {Complex adaptive systems (cas) \textendash{} systems that involve many components that adapt or learn as they interact \textendash{} are at the heart of important contemporary problems. The study of cas poses unique challenges: Some of our most powerful mathematical tools, particularly methods involving fixed points, attractors, and the like, are of limited help in understanding the development of cas. This paper suggests ways to modify research methods and tools, with an emphasis on the role of computer-based models, to increase our understanding of cas.},
  langid = {english},
  file = {/home/ture/Zotero/storage/PJZH78SX/Holland - 2006 - Studying Complex Adaptive Systems.pdf}
}

@misc{hui_alphago_2018,
  title = {{{AlphaGo}}: {{How}} It Works Technically?},
  shorttitle = {{{AlphaGo}}},
  author = {Hui, Jonathan},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it\ldots},
  langid = {english},
  file = {/home/ture/Zotero/storage/AJ7N92UX/alphago-how-it-works-technically-26ddcc085319.html}
}

@incollection{hutchison_parallel_2010,
  title = {Parallel {{Minimax Tree Searching}} on {{GPU}}},
  booktitle = {Parallel {{Processing}} and {{Applied Mathematics}}},
  author = {Rocki, Kamil and Suda, Reiji},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Wasniewski, Jerzy},
  year = {2010},
  volume = {6067},
  pages = {449--456},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14390-8_47},
  abstract = {The paper describes results of minimax tree searching algorithm implemented within CUDA platform. The problem regards move choice strategy in the game of Reversi. The parallelization scheme and performance aspects are discussed, focusing mainly on warp divergence problem and data transfer size. Moreover, a method of minimizing warp divergence and performance degradation is described. The paper contains both the results of test performed on multiple CPUs and GPUs. Additionally, it discusses {$\alpha\beta$} parallel pruning implementation.},
  isbn = {978-3-642-14389-2 978-3-642-14390-8},
  langid = {english},
  file = {/home/ture/Zotero/storage/5WUKFQ42/Rocki and Suda - 2010 - Parallel Minimax Tree Searching on GPU.pdf}
}

@article{JMLR:v9:vandermaaten08a,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  file = {/home/ture/Zotero/storage/XIYLPMQH/vandermaaten08a.pdf}
}

@misc{karpathy_peek_2017,
  title = {A {{Peek}} at {{Trends}} in {{Machine Learning}}},
  author = {Karpathy, Andrej},
  year = {2017},
  month = apr,
  journal = {Medium},
  abstract = {Have you looked at Google Trends? It's pretty cool \textemdash{} you enter some keywords and see how Google Searches of that term vary through time. I\ldots},
  langid = {english},
  file = {/home/ture/Zotero/storage/7VTC8RYF/a-peek-at-trends-in-machine-learning-ab8a1085a106.html}
}

@incollection{knuth_early_1980,
  title = {The {{Early Development}} of {{Programming Languages}}},
  booktitle = {A {{History}} of {{Computing}} in the {{Twentieth Century}}},
  author = {Knuth, DONALD E. and Pardo, LUIS TRABB},
  editor = {Metropolis, N. and Howlett, J. and Rota, GIAN-CARLO},
  year = {1980},
  month = jan,
  pages = {197--273},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  doi = {10.1016/B978-0-12-491650-0.50019-8},
  abstract = {This paper surveys the evolution of ``high-lever'' programming languages during the first decade of computer programming activity. We discuss the contributions of Zuse in 1945 (the ``Plankalk\"ul''), Goldstine and von Neumann in 1946 (``Flow Diagrams''), Curry in 1948 (``Composition''), Mauchly et al. in 1949 (``Short Code''), Burks in 1950 (``Intermediate PL''), Rutishauser in 1951 (``Klammerausdr\"ucke''), B\"ohm in 1951 (``Formules''), Glennie in 1952 (``AUTOCODE''), Hopper et al. in 1953 (``A-2''), Laning and Zierler in 1953 (``Algebraic Interpreter''), Backus et al. in 1954\textendash 1957 (``FORTRAN''), Brooker in 1954 (``Mark I AUTOCODE''), Kamynin and L\^iubimskii in 1954 (``{$\Pi\Pi$}-2''), Ershov in 1955 (``{$\Pi\Pi$}''), Grems and Porter in 1955 (``BACAIC''), Elsworth et al. in 1955 (``Kompiler 2''), Blum in 1956 (``ADES''), Perlis et al. in 1956 (``IT''), Katz et al. in 1956\textendash 1958 (``MATH-MATIC''), Bauer and Samelson in 1956\textendash 1958 (U.S. Patent 3,047,228). The principal features of each contribution are illustrated and discussed. For purposes of comparison, a particular fixed algorithm has been encoded (as far as possible) in each of the languages. This research is based primarily on unpublished source materials, and the authors hope that they have been able to compile a fairly complete picture of the early developments in this area.},
  isbn = {978-0-12-491650-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/MP48ABS4/STAN-CS-76-562_EarlyDevelPgmgLang_Aug76.pdf;/home/ture/Zotero/storage/B9MRYESM/B9780124916500500198.html}
}

@article{kochenderfer_algorithms_nodate,
  title = {Algorithms for {{Decision Making}}},
  author = {Kochenderfer, Mykel J and Wheeler, Tim A and Wray, Kyle H},
  pages = {690},
  langid = {english},
  file = {/home/ture/Zotero/storage/8EJIZEXH/Kochenderfer et al. - Algorithms for Decision Making.pdf}
}

@article{krizhevsky_imagenet_2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/ture/Zotero/storage/QSMPK8RF/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Mathematics and computing Subject\_term\_id: computer-science;mathematics-and-computing},
  file = {/home/ture/Zotero/storage/C6RF72LV/LeCun et al. - 2015 - Deep learning.pdf;/home/ture/Zotero/storage/QSMZVATG/nature14539.html}
}

@techreport{lee_abalone_2005,
  title = {Abalone \textendash{{Final Project Report}}},
  author = {Lee, Benson and Noh, Hyun Joo},
  year = {2005},
  pages = {11},
  langid = {english},
  file = {/home/ture/Zotero/storage/D27L8CN9/Lee - Abalone ‚ÄìFinal Project Report.pdf}
}

@inproceedings{lemmens_constructing_2005,
  title = {Constructing an Abalone Game-Playing Agent},
  booktitle = {Bachelor {{Conference Knowledge Engineering}}, {{Universiteit Maastricht}}},
  author = {Lemmens, NPPM},
  year = {2005},
  publisher = {{Citeseer}},
  file = {/home/ture/Zotero/storage/9ME8UC3J/Lemmens - 2005 - Constructing an abalone game-playing agent.pdf}
}

@techreport{mizrachi_introduction_2017,
  title = {Introduction to Artificial Intelligence {{Final Project}}},
  author = {Mizrachi, Rubi and Golran, Guy and Jacobi, Omer and Zats, Rom},
  year = {2017},
  institution = {{The Hebrew University of Jerusalem}},
  file = {/home/ture/Zotero/storage/N7P92TVG/report.pdf}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  langid = {english},
  file = {/home/ture/Zotero/storage/3W78FHUN/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@book{moroney_ai_2020,
  title = {{{AI}} and {{Machine Learning}} for {{Coders}}: {{A Programmer}}'s {{Guide}} to {{Artificial Intelligence}}},
  shorttitle = {{{AI}} and {{Machine Learning}} for {{Coders}}},
  author = {Moroney, Laurence},
  year = {2020},
  publisher = {{O'Reilly}},
  abstract = {If you're looking to make a career move from programmer to AI specialist, this is the ideal place to start. Based on Laurence Moroney's extremely successful AI courses, this introductory book provides a hands-on, code-first approach to help you build confidence while you learn key topics. You'll understand how to implement the most common scenarios in machine learning, such as computer vision, natural language processing (NLP), and sequence modeling for web, mobile, cloud, and embedded runtimes. Most books on machine learning begin with a daunting amount of advanced math. This guide is built on practical lessons that let you work directly with the code. You'll learn:  How to build models with TensorFlow using skills that employers desire The basics of machine learning by working with code samples How to implement computer vision, including feature detection in images How to use NLP to tokenize and sequence words and sentences Methods for embedding models in Android and iOS How to serve models over the web and in the cloud with TensorFlow Serving},
  googlebooks = {462OzQEACAAJ},
  isbn = {978-1-4920-7819-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Information Theory,Computers / Machine Theory},
  file = {/home/ture/Zotero/storage/P6Q9E7U7/Laurence Moroney - AI and Machine Learning for Coders_ A Programmer's Guide to Artificial Intelligence-O'Reilly Media (2020).pdf}
}

@article{nijssen_writing_nodate,
  title = {Writing a {{Bachelor Thesis}} in {{Computer Science}}},
  author = {Nijssen, Siegfried},
  pages = {37},
  langid = {english},
  file = {/home/ture/Zotero/storage/TCQ856MM/Nijssen - Writing a Bachelor Thesis in Computer Science.pdf}
}

@book{nilsson_artificial_1998,
  title = {Artificial {{Intelligence}}: {{A New Synthesis}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Nilsson, Nils J.},
  year = {1998},
  month = apr,
  publisher = {{Elsevier}},
  abstract = {Intelligent agents are employed as the central characters in this new introductory text. Beginning with elementary reactive agents, Nilsson gradually increases their cognitive horsepower to illustrate the most important and lasting ideas in AI. Neural networks, genetic programming, computer vision, heuristic search, knowledge representation and reasoning, Bayes networks, planning, and language understanding are each revealed through the growing capabilities of these agents. The book provides a refreshing and motivating new synthesis of the field by one of AI's master expositors and leading researchers. Artificial Intelligence: A New Synthesis takes the reader on a complete tour of this intriguing new world of AI. An evolutionary approach provides a unifying theme  Thorough coverage of important AI ideas, old and new Frequent use of examples and illustrative diagrams Extensive coverage of machine learning methods throughout the text Citations to over 500 references Comprehensive index},
  googlebooks = {lr1Po3DJZNIC},
  isbn = {978-0-08-049945-1},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General},
  file = {/home/ture/Zotero/storage/EG5D3CPW/Nils J. Nilsson - Artificial Intelligence_ A New Synthesis -Morgan Kaufmann Publishers, Inc. (1998).pdf}
}

@misc{noauthor_abalone_2020,
  title = {Abalone (Board Game)},
  year = {2020},
  month = dec,
  journal = {Wikipedia},
  abstract = {Abalone is a two-player abstract strategy board game designed by Michel Lalet and Laurent L\'evi in 1987. Players are represented by opposing black and white marbles on a hexagonal board with the objective of pushing six of the opponent's marbles off the edge of the board. Abalone was published in 1990 and has sold more than 4.5 million units. The year it was published it received one of the first Mensa Select awards. It is currently sold in more than thirty countries.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Abalone\_(board\_game)\&oldid=994557581},
  langid = {english},
  annotation = {Page Version ID: 994557581},
  file = {/home/ture/Zotero/storage/5H8G9LDZ/index.html}
}

@misc{noauthor_free_nodate,
  title = {Free {{Trial}} and {{Free Tier}}},
  journal = {Google Cloud},
  abstract = {Start building on GCP with a Free Trial that includes \$300 in credits. Plus, enjoy access to 20+ select products, like Compute Engine, free of charge.},
  howpublished = {https://cloud.google.com/free},
  langid = {english},
  file = {/home/ture/Zotero/storage/CVDRRM54/free.html}
}

@misc{noauthor_globalinterpreterlock_nodate,
  title = {{{GlobalInterpreterLock}} - {{Python Wiki}}},
  howpublished = {https://wiki.python.org/moin/GlobalInterpreterLock},
  file = {/home/ture/Zotero/storage/7SQ4W2XZ/GlobalInterpreterLock.html}
}

@misc{noauthor_peer_nodate,
  title = {Peer {{Sommerlund}} / Haliotis},
  abstract = {Abalone board game library},
  file = {/home/ture/Zotero/storage/S3KRSZD7/haliotis.html}
}

@misc{noauthor_red_nodate,
  title = {Red {{Blob Games}}: {{Hexagonal Grids}}},
  shorttitle = {Red {{Blob Games}}},
  abstract = {Guide to math, algorithms, and code for hexagonal grids in games},
  howpublished = {https://www.redblobgames.com/grids/hexagons/},
  langid = {english},
  file = {/home/ture/Zotero/storage/6XJJDMJ9/hexagons.html}
}

@misc{noauthor_simple_nodate,
  title = {Simple {{Alpha Zero}}},
  howpublished = {https://web.stanford.edu/\textasciitilde surag/posts/alphazero.html},
  file = {/home/ture/Zotero/storage/S73RNETQ/alphazero.html}
}

@misc{noauthor_system_nodate,
  title = {System {{Architecture}} | {{Cloud TPU}}},
  journal = {Google Cloud},
  howpublished = {https://cloud.google.com/tpu/docs/system-architecture-tpu-vm},
  langid = {english},
  file = {/home/ture/Zotero/storage/THV6PYKD/system-architecture-tpu-vm.html}
}

@misc{noauthor_tpu_nodate,
  title = {{{TPU Research Cloud}}},
  howpublished = {https://sites.research.google/trc/},
  file = {/home/ture/Zotero/storage/96VFE5C2/trc.html}
}

@misc{noauthor_ture_nodate,
  title = {Ture / Abalone},
  journal = {GitLab},
  abstract = {GitLab.com},
  howpublished = {https://gitlab.com/CampFireMan/abalone},
  langid = {english},
  file = {/home/ture/Zotero/storage/5BYMCU37/abalone.html}
}

@misc{noauthor_zobrist_nodate,
  title = {Zobrist {{Hashing}} - {{Chessprogramming}} Wiki},
  howpublished = {https://www.chessprogramming.org/Zobrist\_Hashing},
  file = {/home/ture/Zotero/storage/E9U4YWBE/Zobrist_Hashing.html}
}

@article{papadopoulos_exploring_2012,
  title = {Exploring {{Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha-Beta Search}}},
  author = {Papadopoulos, Athanasios and Toumpas, Konstantinos and Chrysopoulos, Antonios and Mitkas, Pericles A},
  year = {2012},
  journal = {IEEE Conference on Computational Intelligence and Games},
  pages = {8},
  abstract = {This paper discusses the design and implementation of a highly efficient MiniMax algorithm for the game Abalone. For perfect information games with relatively low branching factor for their decision tree (such as Chess, Checkers etc.) and a highly accurate evaluation function, Alpha-Beta search proved to be far more efficient than Monte Carlo Tree Search. In recent years many new techniques have been developed to improve the efficiency of the Alpha-Beta tree, applied to a variety of scientific fields. This paper explores several techniques for increasing the efficiency of Alpha-Beta Search on the board game of Abalone while introducing some new innovative techniques that proved to be very effective. The main idea behind them is the incorporation of probabilistic features to the otherwise deterministic AlphaBeta search.},
  langid = {english},
  file = {/home/ture/Zotero/storage/6NQG9BJM/Papadopoulos et al. - 2012 - Exploring Optimization Strategies in Board Game Ab.pdf}
}

@article{petosa_multiplayer_2019,
  title = {Multiplayer {{AlphaZero}}},
  author = {Petosa, Nick and Balch, Tucker},
  year = {2019},
  month = dec,
  journal = {arXiv:1910.13012 [cs]},
  eprint = {1910.13012},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The AlphaZero algorithm has achieved superhuman performance in two-player, deterministic, zero-sum games where perfect information of the game state is available. This success has been demonstrated in Chess, Shogi, and Go where learning occurs solely through self-play. Many real-world applications (e.g., equity trading) require the consideration of a multiplayer environment. In this work, we suggest novel modifications of the AlphaZero algorithm to support multiplayer environments, and evaluate the approach in two simple 3-player games. Our experiments show that multiplayer AlphaZero learns successfully and consistently outperforms a competing approach: Monte Carlo tree search. These results suggest that our modified AlphaZero can learn effective strategies in multiplayer game scenarios. Our work supports the use of AlphaZero in multiplayer games and suggests future research for more complex environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/ture/Zotero/storage/LTAZRQ7M/Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf}
}

@article{rosin_multi-armed_2011,
  title = {Multi-Armed Bandits with Episode Context},
  author = {Rosin, Christopher D.},
  year = {2011},
  month = mar,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {3},
  pages = {203--230},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9258-6},
  abstract = {A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0, 1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multiarmed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm.},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKNKB6JG/Rosin - 2011 - Multi-armed bandits with episode context.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2021},
  edition = {Fourth},
  publisher = {{Pearson Education, Inc}},
  abstract = {Artificial Intelligence: A Modern Approach offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.},
  isbn = {978-1-5376-0031-4},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKE25V5T/Stuart Russell, Peter Norvig - Artificial Intelligence_ A Modern Approach (4th Edition) (Pearson Series in Artifical Intelligence)-Language_ English (2020).pdf}
}

@article{schmid_player_2021,
  title = {Player of {{Games}}},
  author = {Schmid, Martin and Moravcik, Matej and Burch, Neil and Kadlec, Rudolf and Davidson, Josh and Waugh, Kevin and Bard, Nolan and Timbers, Finbarr and Lanctot, Marc and Holland, Zach and Davoodi, Elnaz and Christianson, Alden and Bowling, Michael},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03178 [cs]},
  eprint = {2112.03178},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Games have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/PS5G8X9G/Schmid et al. - 2021 - Player of Games.pdf;/home/ture/Zotero/storage/AJDRA7N2/2112.html}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3\textemdash the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4\textemdash the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi\textemdash canonical environments for high-performance planning\textemdash the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game. A reinforcement-learning algorithm that combines a tree-based search with a learned model achieves superhuman performance in high-performance planning and visually complex domains, without any knowledge of their underlying dynamics.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science Subject\_term\_id: computational-science;computer-science},
  file = {/home/ture/Zotero/storage/GZDB2LDU/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf;/home/ture/Zotero/storage/DAI2XJ6I/s41586-020-03051-4.html}
}

@misc{scriptim_scriptimabalone-boai_2021,
  title = {Scriptim/{{Abalone-BoAI}}},
  author = {Scriptim},
  year = {2021},
  month = apr,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT License         ,                 MIT License},
  keywords = {abalone,ai,ai-battle-game,artificial-intelligence,game,machine-learning,python,python3}
}

@article{shannon_xxii_1950,
  title = {{{XXII}}. {{Programming}} a Computer for Playing Chess},
  author = {Shannon, Claude E.},
  year = {1950},
  month = mar,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {41},
  number = {314},
  pages = {256--275},
  issn = {1941-5982, 1941-5990},
  doi = {10.1080/14786445008521796},
  langid = {english},
  file = {/home/ture/Zotero/storage/XZSDKV92/Shannon - 1950 - XXII. Programming a computer for playing chess.pdf}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  langid = {english},
  file = {/home/ture/Zotero/storage/MD3YXY65/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {/home/ture/Zotero/storage/5J8BTAU2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/ture/Zotero/storage/ZC9N7QNB/nature24270.html}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \^a¬Ä¬úthinned\^a¬Ä¬ù networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/home/ture/Zotero/storage/8FGMBZW2/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/ture/Zotero/storage/BBRQKJF4/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@techreport{thakoor_learning_nodate,
  type = {Final Project Report},
  title = {Learning to {{Play Othello Without Human Knowledge}}},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  institution = {{Stanford University}},
  abstract = {Game playing is a popular area within the field of artificial intelligence. Most agents in literature have hand-crafted features and are often trained on datasets obtained from expert human play. We implement a self- play based algorithm using neural networks for policy estimation and Monte Carlo Tree Search for policy im- provement, with no input human knowledge that learns to play Othello. We evaluate our learning algorithm for 6x6 and 8x8 versions of the game of Othello. Our work is compared with random and greedy baselines, as well as a minimax agent that uses a hand-crafted scoring function, and achieves impressive results. Further, our agent for the 6x6 version of Othello easily outperforms humans when tested against it.},
  file = {/home/ture/Zotero/storage/THCSVXJR/writeup.pdf}
}

@misc{thakoor_suragnairalpha-zero-general_nodate,
  title = {Suragnair/Alpha-Zero-General: {{A}} Clean Implementation Based on {{AlphaZero}} for Any Game in Any Framework + Tutorial + {{Othello}}/{{Gobang}}/{{TicTacToe}}/{{Connect4}} and More},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  howpublished = {https://github.com/suragnair/alpha-zero-general},
  file = {/home/ture/Zotero/storage/M78KG5GT/alpha-zero-general.html}
}

@misc{towzeur_towzeurgym-abalone_2021,
  title = {Towzeur/Gym-Abalone},
  author = {{towzeur}},
  year = {2021},
  month = jan,
  abstract = {An environment of the board game Abalone using OpenAI's Gym API},
  keywords = {abalone,gym,open-ai,reinforcement-learning}
}

@article{turing_icomputing_1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  file = {/home/ture/Zotero/storage/RB33PS2W/TURING - 1950 - I.‚ÄîCOMPUTING MACHINERY AND INTELLIGENCE.pdf;/home/ture/Zotero/storage/6RJMJ3NW/986238.html}
}

@book{vasilev_python_2019,
  title = {Python Deep Learning: Exploring Deep Learning Techniques and Neural Network Architectures with {{PyTorch}}, {{Keras}}, and {{TensorFlow}}},
  shorttitle = {Python Deep Learning},
  author = {Vasilev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
  year = {2019},
  edition = {Second edition},
  publisher = {{Packt Publishing Limited}},
  address = {{Birmingham Mumbai}},
  isbn = {978-1-78934-846-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/TP5DPUZ2/Vasilev et al. - 2019 - Python deep learning exploring deep learning tech.pdf}
}

@article{verloop_critical_nodate,
  title = {A {{Critical Review}}: {{Exploring Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha-Beta Search}}},
  author = {Verloop, Michiel},
  pages = {49},
  langid = {english},
  file = {/home/ture/Zotero/storage/EK73C5BP/Verloop - A Critical Review Exploring Optimization Strategi.pdf}
}

@article{vinyals_grandmaster_2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  langid = {english},
  file = {/home/ture/Zotero/storage/Y2K2TQCF/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf}
}

@article{wang_adaptive_2021,
  title = {Adaptive {{Warm-Start MCTS}} in {{AlphaZero-like Deep Reinforcement Learning}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2021},
  month = may,
  journal = {arXiv:2105.06136 [cs]},
  eprint = {2105.06136},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {AlphaZero has achieved impressive performance in deep reinforcement learning by utilizing an architecture that combines search and training of a neural network in self-play. Many researchers are looking for ways to reproduce and improve results for other games/tasks. However, the architecture is designed to learn from scratch, tabula rasa, accepting a cold-start problem in self-play. Recently, a warmstart enhancement method for Monte Carlo Tree Search was proposed to improve the self-play starting phase. It employs a fixed parameter I to control the warm-start length. Improved performance was reported in small board games. In this paper we present results with an adaptive switch method. Experiments show that our approach works better than the fixed I , especially for ''deep,'' tactical, games (Othello and Connect Four). We conjecture that the adaptive value for I is also influenced by the size of the game, and that on average I will increase with game size. We conclude that AlphaZero-like deep reinforcement learning benefits from adaptive rollout based warm-start, as Rapid Action Value Estimate did for rollout-based reinforcement learning 15 years ago.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/T6AJBPW2/Wang et al. - 2021 - Adaptive Warm-Start MCTS in AlphaZero-like Deep Re.pdf}
}

@article{wang_warm-start_2020,
  title = {Warm-{{Start AlphaZero Self-Play Search Enhancements}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2020},
  journal = {arXiv:2004.12357 [cs]},
  volume = {12270},
  eprint = {2004.12357},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {528--542},
  doi = {10.1007/978-3-030-58115-2_37},
  abstract = {Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE) and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different (small) board games, with especially RAVE based variants playing strongly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ture/Zotero/storage/IC9JLYFX/Wang et al. - 2020 - Warm-Start AlphaZero Self-Play Search Enhancements.pdf;/home/ture/Zotero/storage/Y68GT8W8/2004.html}
}


