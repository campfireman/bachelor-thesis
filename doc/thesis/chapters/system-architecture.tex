\chapter{System Architecture}
\label{system-architecture}
Well equipped with all the basic theoretical tools that we need, we can move on to implementing them for the game of Abalone. A first logical step would be to look at the existing software landscape to decide if we can utilize existing tools to speed up development.

\section{Software}
\subsection{Machine Learning Library}
Machine learning projects share many components. Most commonly that is the declaration of the computational graph and the training of the graph. The libraries not only provide those components but also bring significant optimizations and specialized code for hardware acceleration. Therefore, it is imperative to decide on a suitable library to speed up development by several orders of magnitude.

\begin{figure}
    \centering
    \includegraphics[height=7cm, keepaspectratio]{ml_framework_popularity.png}
    \caption{The mentions of pytorch and tensorflow in research papers in different publications \cite{noauthor_state_2019}}
    \label{ml_framework_popularity}
\end{figure}

A first indicator to go by is the popularity of the frameworks. Currently, the most relevant frameworks are Facebook's pytorch and Google's tensorflow. The Keras API has been introduced to tensorflow, therefore it is not considered further. In research pytorch seems to have quickly taken the dominance as depicted in figure \ref{ml_framework_popularity}. Aside from all differences between both libraries, the choice was guided by two practical reasons. Initially, tensorflow was selected due to the included support for TPUs as this project was granted free access to Google's Research Cloud \cite{noauthor_tpu_nodate}. At a later stage it became clear that Google was unwilling to increase the CPU quota for the account, limiting the server to 8 cores which posed a significant problem for parallel execution.

In table \ref{pytorch_vs_tensorflow_performance} there is also a significant performance difference in the inference step of the MCTS. There are two ways in tensorflow to perform inference, either through the \texttt{predict} function or by calling the \texttt{\_\_call\_\_} method of the $model$ itself. As in the implementation the inference is not batched but done for individual board states, the usage of the latter option is faster \cite{noauthor_tfkerasmodel_nodate}. Nevertheless, pytorch is about five times faster. As discussed later, this is the reason to pivot to pytorch as a framework.

\begin{table*}
    \begin{center}
        \begin{tabular}{ c|c|c|c|c }
            HW  & Framework & Neural net size & $\texttt{predict}(s)$ & $\texttt{\_\_call\_\_}(s)$ \\
            \hline
            \hline
            CPU & tf        & small           & ~0.027s               & ~0.011s                    \\
            GPU & tf        & small           & ~0.024s               & ~0.005s                    \\
            CPU & tf        & large           & ~0.027s               & ~0.015s                    \\
            GPU & tf        & large           & ~0.025s               & ~0.011s                    \\
        \end{tabular}
    \end{center}
    \caption{The average time ($n = 3,000$) taken to perform the feed-forward through the network for state $s$ with either ($\texttt{predict}(s)$) or ($\texttt{\_\_call\_\_}(s)$) in tensorflow}\label{tensorflow_predict_vs_call}
\end{table*}

\begin{table*}
    \begin{center}
        \begin{tabular}{ c|c|c|c|c }
            HW  & Framework & Neural net size & $\texttt{predict}(s)$ & $\texttt{search}(s)$ \\
            \hline
            \hline
            CPU & tf        & small           & ~0.01s                & ~0.016s              \\
            GPU & tf        & small           & ~0.005s               & ~0.011s              \\
            CPU & tf        & large           & ~0.014s               & ~0.02s               \\
            GPU & tf        & large           & ~0.011s               & ~0.017s              \\
            CPU & pytorch   & small           & ~0.005s               & ~0.011s              \\
            GPU & pytorch   & small           & ~0.001s               & ~0.007s              \\
            CPU & pytorch   & large           & ~0.005s               & ~0.011s              \\
            GPU & pytorch   & large           & ~0.002s               & ~0.008s              \\
        \end{tabular}
    \end{center}
    \caption{The average time ($n = 3,000$) taken to perform the feed-forward through the network for state $s$ ($\texttt{predict}(s)$) and one iteration of MCTS ($\texttt{search}(s)$)}\label{pytorch_vs_tensorflow_performance}
\end{table*}

\subsection{Training Framework}
\label{training_framework}
As there are existing frameworks that have implemented the system described in the AlphaZero paper in a more general and adaptable fashion, it has to be considered building on their foundation:
\begin{itemize}
    \item AlphaZero General is a framework developed as a university project at Stanford originally for the game of Hex and Othello. \cite{thakoor_learning_nodate,thakoor_suragnairalpha-zero-general_nodate}.
    \item Deep MCTS is a framework developed in the context of a Master's thesis \cite{bruasdal_deep_2020,henribru_deep_2021}.
\end{itemize}

To compare both options a catalog of criteria is considered. Parallel training and parallel search measure whether the software implements the parallel training pipeline and MCTS-APV proposed by the AlphaZero paper. Moreover, it is checked whether the libraries are agnostic to the game and ML library that is being used. Lastly, it is relevant to investigate whether the performance could be verified by other users.

\begin{table*}
    \begin{center}
        \begin{tabular}{ c|c|c }
            Criterion             & AlphaZero General & Deep MCTS \\
            \hline
            \hline
            Parallel training     & 0                 & 1         \\
            Parallel search       & 0                 & 0         \\
            ML library agnostic   & 1                 & 0         \\
            Game library agnostic & 1                 & 1         \\
            Verified performance  & 1                 & 0         \\
            Simplicity            & 1                 & 0         \\
            \hline
            \hline
            Sum                   & 4                 & 2         \\
        \end{tabular}
    \end{center}
    \caption{A comparison of existing AlphaZero frameworks}\label{training_framework_comparison}
\end{table*}

As shown by table \ref{training_framework_comparison}, the winner by those criteria is AlphaZero General. Core advantages of the library are its simplicity and popularity. Deep MCTS makes use of a lot of abstractions and generics which makes the code more professional and reusable but also significant difficulties in understanding. The application of AlphaZero General to multiple other games like Gobang, Santorini, Connect4 etc. have shown its performance. The major drawback is the lack of a parallelized training pipeline.

\subsection{Game Library}
There are two relevant implementations of game libraries for Abalone. As the interfacing language for pytorch is Python it makes sense to restrict the libraries to Python.

\begin{itemize}
    \item Abalone-BoAI is a game library that is specifically designed for interfacing with computational agents. It is very simple and has been used for the precursor project of this thesis \cite{scriptim_scriptimabalone-boai_2021}.
    \item Gym-abalone is tightly integrated with OpenAI's gym allowing agents to potentially play other games as well. The API is more designed for the reinforcement learning setting \cite{towzeur_towzeurgym-abalone_2021}.
\end{itemize}

Two arguments lead to the decision of choosing Abalone-BOAI:
\begin{enumerate}
    \item It was already optimized in the previous project as the generation of legal moves is computationally expensive.
    \item The API of the engine needs to be adapted to fit the interface of the training framework, wherefore intricate knowledge of the engine is of advantage.
\end{enumerate}

As mentioned in the section \ref{existing_game_playing_agents}, Verloop's reimplementation \cite{verloop_abaloneai_nodate} offers the simplest solution for interfacing with (one of) the strongest Abalone algorithms. In a small tournament personal implementations of minimax in Python \cite{claussen_abalone_2021} were inferior. This creates the necessity of interfacing between Java and Python. As the Java implementation is tightly coupled with the internal game-engine it was decided to implement a proxy player for each game-engine. The proxy players communicate through a named pipe as depicted in figure \ref{python_java_ipc}. The downside of this solution is the need of synchronizing two separate game states.

\begin{figure}
    \centering
    \includegraphics[height=4cm, keepaspectratio]{game_engine_communication.png}
    \caption{The inter process communication between the python game engine and the Java implementation of ABA-PRO}
    \label{python_java_ipc}
\end{figure}

As both implementations have a differing move notation an

\section{Neural Network}
\subsection{Dimensions}
In order to apply the neural network architecture proposed by AlphaZero (cf. figure \ref{alpha_zero_neural_network}) two dimensions need to be changed:
\begin{enumerate}
    \item The input matrix of size $19 \cdot 19 \cdot 17$ needs to be adjusted to fit the dimensions of the hexagonal board. The hexagonal board is transformed into an orthogonal base as proposed in \cite{towzeur_towzeurgym-abalone_2021} such that it can represented as an $9 \cdot 9$ matrix as depicted in figure \ref{input_matrix}. The adjacency of the marbles is maintained.
          The third dimension of size $17$ can be reduced to 1 by removing the move history and only passing the current state. Moreover, the players' marbles are represented as $1$ for white and $-1$ for black (0 for an empty space) in a single plane instead of separating them. Lastly, the neural network is always fed with the canonical representation of the board, as proposed by \cite{thakoor_learning_nodate}. This means that the board states are always seen from white's perspective. If it's the white players turn the board remains unchanged, if it's black's turn the colors of the board are switched.

    \item AlphaZero's output vector $\pi$ of size 381 represents all possible positions a piece can be placed on the board. As Abalone has a much more complex move system, the size of this vector needs to match the number of all possible moves. The number of all possible moves was found by generating all moves programmatically. The generated moves were stored in a bijective map. Each move is assigned an index in $\pi$ and represented as a string in the proposed notation above. Therefore, using the bijective map a move index can be found by its string representation and vice versa.

          As depicted in figure \ref{possible_move_generation_inline} the inline moves are generated by iterating over all fields of the board. For each field all directions are tested for being a valid move. If that is the case the move is added to the bijective map. In case the move is already in the map, the move is skipped.

          \begin{figure}
              \centering
              \includegraphics[width=12cm, keepaspectratio]{possible_move_generation_inline.png}
              \caption{Generation of inline moves, starting at position A1 and direction NE}
              \label{possible_move_generation_inline}
          \end{figure}

          Figure \ref{possible_move_generation_broadside} shows the generation of the broadside moves. The algorithm iterates each field. For each field marble lines of length two and three in all (possible) directions are generated. For each line broadside in all directions, moves are checked for their validity. The move direction of the marble line direction is ignored.

          \begin{figure}
              \centering
              \includegraphics[width=12cm, keepaspectratio]{possible_move_generation_broadside.png}
              \caption{Generation of broadside moves starting at position A1, line direction NE, line length of two and at move direction NE}
              \label{possible_move_generation_broadside}
          \end{figure}

          The resulting vector $\pi$ has a length of $1452$.
\end{enumerate}

\subsection{Architecture}
% explain pi and v
\section{Training Pipeline}
\subsection{Components}
\label{components}

The training pipeline of AlphaZero General has 5 main components:
\begin{enumerate}
    \item \textbf{Coach}: The main module that orchestrates the training process.
    \item \textbf{Game}: Provides an abstract interface that generalizes to many types of board games. Functions like the generation of legal moves, creating a unique string representation of the board, etc. need to be implemented.
    \item \textbf{Neural Net}: Is a wrapper for any type of neural network. It needs to be implemented for the specific framework used.
    \item \textbf{MCTS}: Encapsules the logic to perform Monte Carlo Tree Search on the game tree with the help of the neural network.
    \item \textbf{Arena}: Has the task to perform the faceoff between different agents.
\end{enumerate}

Aside from the modifications made to parallelize the framework in section \ref{parallelization}, the Arena was modified to use the abalone-engine and game-playing agents implemented for that engine. This way not only the neural nets can be faced off, but also the proxy player for Verloop's minimax and other algorithms. Moreover, the MCTS implementation was not able to deal with games that have board states appear multiple times in the search tree. The implementation uses hash maps to retrieve the action-values and counts for a board state. If the board state exists multiple times in the search tree the values are skewed. To solve the problem each hash of a state is extended by a hash of the parent node's hash and the current depth of the node.

Additionally, a CLI entry point was added to pass arguments to the training process. All relevant hyperparameters of a training run are persisted in a JSON file. Performance data (time per iteration, etc.) and results of the Arena are logged as CSV tables.

\subsection{Algorithm}

\begin{figure}[H]
    \centering
    \subfloat[The main loop of the training algorithm]{
        \includegraphics[height=19cm, keepaspectratio]{training_main_loop.png}
    }
    % \hfill
    \subfloat[The episode loop]{
        \includegraphics[height=19cm, keepaspectratio]{training_episode.png}
    }
    \caption{The self-play training pipeline}
    \label{training_algorithm}
\end{figure}

\subsection{Parallelization}
\label{parallelization}
AlphaZero General's training pipeline does not offer parallel training. By comparing the time taken to execute one episode for the game of Othello and Abalone it showed that a game of Abalone takes on average $40$ times (n=100) longer to finish: 1s for Othello and 40s for Abalone. Investigating potential improvements, it became clear that there is two major factors which determine the runtime of an episode:

1. Length of the game. A game of Abalone takes significantly more turns, caused by rules not forbidding loops etc. such that games can be infinite. Moreover defensive play styles can draw out games significantly. The issue can be alleviated by limiting the length of a game to a certain amount of turns $m$ and evaluating the state of the game at that step. In this case the length was limited to $m=200$.

2. The MCTS being computationally expensive: A closer inspection of the execution time, holding all variables constant demonstrates the culprit. The simulation step of MCTS using the neural network. The time taken by this function is one order of magnitude larger than the second most expensive operation, which is the generation of legal moves. For example when running 15 MCTS iterations with the original tensorflow implementation, one turn takes

\begin{itemize}
    \item Total time: 0.44s
    \item One MCTS iteration: 0.03s
    \item Neural network : 0.025s
    \item Valid move generation: 0.05s
\end{itemize}

Looking at the main variables that control the length of the nested loops, it becomes clear that reducing the time on MCTS iteration takes, is essential. Assuming function $f$ is the operation to perform one Monte Carlo simulation:

$$
    k := \text{number of games}
$$
$$
    m := \text{number of turns per game}
$$
$$
    n := \text{number of MCTS simulations}
$$
$$
    f \in O(kmn)
$$

Abalone requires a larger number of MCTS simulations. The other projects \cite{bruasdal_deep_2020,thakoor_learning_nodate} used only 30 iterations for Othello and Hex, but the search space of Abalone is larger. AlphaZero performs $1,600$ iterations \cite[p. 11]{silver_mastering_2017} for each move. In order for the network to improve, the MCTS needs to recommend a better move than the pure network. It is the policy improvement operator. This was the reason to decide to use PyTorch, as it brings an $5$-times improvement for that step as shown in \ref{pytorch_vs_tensorflow_performance}. Nevertheless, this only brings down execution time for one episode by a factor of $5$, still being $8$-times slower than the reference implementation for Othello. AlphaZero uses two ways to alleviate this problem:

\begin{itemize}
    \item Parallelization of the self-play training
    \item Parallelization of the MCTS (APV-MCTS)
\end{itemize}

As Python's global interpreter lock \cite{noauthor_globalinterpreterlock_nodate} doesn't allow for true multithreading, parallelizing the MCTS poses significant complexity and difficulty. However, allowing for simultaneous self-play and training of the neural network is feasible. As already mentioned in section \ref{training_framework}, the implementation by Bruåsdal \cite{bruasdal_deep_2020} provides this feature. For that reason its architecture is used as a blueprint for migrating AlphaZero General into a parallel architecture. The figure \ref{parallel_training_pipeline} depicts how the components mentioned interact in a parallel fashion.

\begin{figure}[H]
    \centering
    \includegraphics[height=10cm, keepaspectratio]{parallel-training.png}
    \caption{The different processes during the parallel training \cite[cf. p. 45]{bruasdal_deep_2020}}
    \label{parallel_training_pipeline}
\end{figure}

There are multiple self-play workers that use the current best neural network $f_{\theta}$ to generate experience. The experience is put into a queue to allow for asynchronous communication between the worker processes and the Coach. For each training iteration of the neural network, the queue is emptied and loaded into the replay buffer. If the experience buffer exceeds the maximum size, the oldest experience is dropped. The number of training batches is defined by the batch size and buffer size (buffer size divided by batch size). The training batches are created by randomly sampling tuples ($s, \pi, z$) from the buffer. No tuple is used twice, so that the entire buffer is utilized.

After training the neural network with the batches, the newly created network is pitted against the old version in the Arena. As both variants are pitted against each other multiple times, the Arena is parallelized as well. If the new network is stronger, it is saved on disk and a version counter is incremented. The version counter is a shared variable between the workers and the Coach. After each completed self-play game the worker checks if the version counter is larger than its stored version. If that is the case, the new network is loaded from disk.

An important factor is to balance the amount of workers against the time it takes to train the neural network. If the buffer grows large, training takes long. In the meantime a lot of new experience is potentially generated. This then overrides all previous experience, such that all experience is only generated from one network. This might make the training process less robust.

\subsection{Symmetrical board generation}
Abalone's board has six rotational symmetries and six mirror axes, as mentioned in \ref{state_space_complexity}. That means $\pi$ and $v$ are equivalent for up to 12 symmetrical board states. By generating the symmetrical board states for each tuple ($s, \pi, z)$, the data can be augmented by a factor of twelve. There are some board configurations in which there are less symmetrical boards as some are identical, for instance the default starting position. In general, the factor of 12 does hold for most board states.

To create the additional experience the positions of the marbles are turned into cube coordinates. For cube coordinates there is also a simple way to mirror coordinates and also to rotate coordinates. A mapping function transforms marble coordinates from their matrix coordinates of the form $(x, y)$ to the form of $(q, r, s)$. The origin of the cube coordinate system is laid into the center of the Abalone board, such that e.g. $E5 = (0, 0, 0)$ or $A1 = (-4, 4, 0)$.

The transformed coordinates can be rotated around the origin by 60° clockwise by shifting the values to the values to the right and inverting the sign:

\begin{BVerbatim}
    [ q,  r,  s ]
    to  [-r, -s, -q ]
    to     [  s,  q,  r ]
\end{BVerbatim}

To mirror a cube coordinate along one of the three main coordinate axes $q, r$ or $s$ the two coordinates that don't belong to the axis are swapped:

\begin{BVerbatim}
    function reflect_q(h) { return Cube(h.q, h.s, h.r); }
    function reflect_r(h) { return Cube(h.s, h.r, h.q); }
    function reflect_s(h) { return Cube(h.r, h.q, h.s); }
\end{BVerbatim}

To mirror along the axis that is orthogonal to the main axis, the coordinates need to be negated first. After applying the transformation to all marbles, all moves in $\pi$, whose probability is not $0$ , have to be transformed as well. First, the move corresponding to the index in $\pi$ is looked up. As a move consists of one or two marble coordinates, those are transformed by the same scheme. The directions can be transformed into cube coordinates as well:

\begin{BVerbatim}
    (+1, -1, 0): NORTH_EAST
    (+1, 0, -1): EAST
    (0, +1, -1): SOUTH_EAST
    (-1, +1, 0): SOUTH_WEST
    (-1, 0, +1): WEST
    (0, -1, +1): NORTH_WEST
\end{BVerbatim}

This way the same functions can be applied to the directions. After all operations are applied the cube coordinates are converted back to marble coordinates and moves.

\subsection{Warm-Up}
