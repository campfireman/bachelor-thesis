\chapter{Background theory}
Before we move to the nuts and bolts of AlphaZero and the concrete implementation for Abalone, we should establish a general understanding of the problem. That includes building the necessary theoretical background in artificial intelligence in general, as well as insight into the specialized knowledge such as deep reinforcement learning in particular.

\section{Artificial intelligence}
The introduction has already foreshadowed how the field of artificial intelligence has undergone a shift in their methodology. In the 1950s and 1960s figures like Alan Turing and von Neumann laid the foundations for modern computers. This sparked the idea one could create programs for these new machines that have similar abilities like humans and other organisms. Researchers at that time assumed there is no "universal principle" behind intelligence and focused on reason and symbol manipulation. Therefore these methods were considered "strong techniques", methods that relied on general principles like learning were labeled "weak techniques". Nowadays, the consensus in the field has reversed. \cite[p. 8f.]{sutton_reinforcement_2018}

% definition of artificial intelligence

\subsection{Rational agent}
Stemming from the Latin word \textit{agere} meaning "to act", an agent is something that acts. As one expects an agent to take sensible or intelligent actions the definition has to be further qualified by calling the agent rational. This means that it acts so as "to achieve the best outcome or, when there is uncertainty, the best expected outcome". \cite[p. 36]{russell_artificial_2021}

The agent exists in an environment which it perceives through sensors and it takes actions through its actuators. The content of the sensor's output for one observation is referred to as \textit{percept}. A cat uses eyes, ears and other organs to perceive the world and its legs, claws and so on to interact with the world. An autonomous car might use radar and cameras for acquiring information and steering and motors for navigation.

Internally the agent might have some built-in knowledge about the world, such as rules on how the environment works. The \textit{agent function} takes the entire history of percepts observed and this built-in knowledge and maps it to an action. A concrete implementation of this abstract function is called \textit{agent program}. The agent program might just be a simple tabular mapping from percepts to actions or could use a complex algorithm with an additional model.

\subsection{(Task) environment}
\label{environment}

As we are trying to build an agent that tries to achieve some specified goal, we can consider its environment as a problem or \textit{task} the agent tries to solve. Putting together both agent and the environment we see a loop of observing,  deliberating and finally taking an action as depicted in figure \ref{agent_environment_loop}.

\begin{figure}
    \centering
    \includegraphics[height=7cm, keepaspectratio]{agent_environment_loop.png}
    \caption{The agent-environment interaction loop \cite[cf. p. 96]{russell_artificial_2021}}
    \label{agent_environment_loop}
\end{figure}

To specify the task environment there are four key components. Let us specify the task environment for a machine classifying defective parts in a production line:

\begin{enumerate}
    \item Performance measure: This might be the percentage of correctly broken parts (true positives) weighted against the number of incorrectly identified parts (false positives).
    \item Environment: The conveyor belt and the parts
    \item Actuators: An arm to push the parts to a different conveyor belt
    \item Sensors: Possibly a camera, infrared sensors, etc.
\end{enumerate}

The initial letters form the acronym PEAS (framework).

There are a few categorizations of the properties of task environments that are extremely helpful for narrowing down potential applicability of different classes of algorithms. A key property is the observability of the environment. If the environment is \textit{fully observable}, the sensors detect all the information that is in any way relevant for taking an action. Conversely if not all information can be observed we call it \textit{partial observability}. For example in poker the other players' cards and the upcoming cards cannot be seen but are highly relevant to the agent's actions. As the current board state of chess fully comprises all information necessary to make a move, we can classify it as fully observable.

The values of the state of the environment and time can be categorized into discrete and continuous. An autonomous vehicle for instance is dealing with continuous time and also continuous states. The speed of the car takes a smooth range of real values and time can be meaningfully split into increasingly small intervals. Board games are entirely discrete. The set of all states is a finite collection of all (legal) permutations of the board and the marbles. Time progresses on the basis of turns.

An agent's actions might also be \textit{non-deterministic}. When dealing with systems of high complexity the next state might not only depend on the previous state and the action taken. There might be other car drivers taking unexpected actions or a comet hitting our car.

Further expanding on the passage of time we have to take into account if actions have consequences for future states. If each combination of percept and action is independent of each other we call it \textit{episodic} and \textit{sequential} otherwise. If we had to classify a production line of circuit boards as either defective or functional, it would be an episodic environment. The classification of an individual board does not matter for the next one.

Another aspect of time is whether the environment changes while the agent takes time to deliberate on the next move. In a \textit{dynamic} environment like the autonomous vehicle operates in, the environment changes continuously. By the time the car decides whether to go right, to avoid collision with a wall, this decision might have already become obsolete. A  turn-based game like chess is static as the board only changes when a move is made.

Lastly, an additional dimension to consider is the number of agents involved. The classification of circuit boards only involves one agent whereas chess is a \textit{multi-agent} environment. We also have to distinguish whether those multiple agents compete for the performance measure. In most board games, one player's win is the other player's loss. In contrast, the other vehicles apart from our autonomous vehicle all profit when it avoids a collision and vice versa. Therefore, they cooperate.



\section{Classic agent}
Now that we have an intricate understanding of the environment of agents we have also narrowed down the choice of algorithms we can employ. In general, perfect information games can be solved by adversarial search algorithms. That means in theory one can find the optimal solution by traversing the entire game tree to terminal states (game ending state). The theory behind this type of agent was already laid out as early as 1945 by Konrad Zuse's program generating legal chess moves \cite{knuth_early_1980}, but was described most comprehensively by Claude Shannon in 1950 in "Programming a Computer for Playing Chess". \cite{shannon_xxii_1950}

\subsection{Minimax algorithm}
Minimax assumes two roles: The minimizer ($min$) and the maximizer ($max$). We start the search from the current board state as the role of the maximizer and then alternate between the two. The result of the minimax search gives us the maximum utility for the given state, assuming both players behave optimally. Let us define the functions \cite[p. 303f.]{russell_artificial_2021}
\begin{itemize}
    \item $\texttt{utility}(s, p)$ returns the utility or the payoff for the terminal state $s$ seen from the perspective of player $p$. In the case of chess this might be $-1, 0$ and $1$ for a loss, draw and a win.
    \item $\texttt{is-terminal}(s)$ returns whether the given state $s$ is a terminal state or not.
    \item $\texttt{to-move}(s)$ returns the current player for the state $s$, either $min$ or $max$.
    \item $\texttt{result}(s, a)$ returns the resulting state if in state $s$ and taking action $a$.
    \item $\texttt{actions}(s)$ returns all legal moves for the given state $s$.
\end{itemize}

\begin{equation}
    \texttt{minimax}(s) =
    \begin{cases}
        \texttt{utility}(s, max)                                                      & \texttt{is-terminal}(s)    \\
        \texttt{max}_{a\in\texttt{actions}(s)}\texttt{minimax}(\texttt{result}(s, a)) & \texttt{to-move}(s) == max \\
        \texttt{min}_{a\in\texttt{actions}(s)}\texttt{minimax}(\texttt{result}(s, a)) & \texttt{to-move}(s) == min \\
    \end{cases}
\end{equation}

Putting this together we can see in figure \ref{minimax} a graphical representation of the search tree for an abstract example. The algorithm traverses down to a leaf node, evaluates its utility and passes the value back up to the parent node. Depending on whether it is a minimizer or a maximizer it chooses the smallest or the largest value passed up by its children. This value again is passed up to the parent until we reach the parent node which is always a maximizer, yielding us the maximum utility we can achieve given our opponent plays optimally.

\begin{figure}
    \centering
    % \includesvg[height=7cm]{minimax.svg}
    \includegraphics[height=7cm, keepaspectratio]{minimax.png}
    \caption{Minimax for a small search tree, resulting in an utility value of 1}
    \label{minimax}
\end{figure}

\subsection{Heuristic functions}
As the number of nodes of the game tree gets very large, the search on the tree usually does not reach terminal leaves that indicate a clear loss or win. Our computational resources will get exhausted first. For example minimax has already visited $ 361 * 360 * 359 * 358 = 16,702,719,120 $ nodes at a depth of $ d = 4 $ in the case of an average Go game.

Therefore, one has to limit the search to a computationally feasible depth and evaluate the intermediary result of a given transposition based on a so-called \textit{heuristic function}. This function replaces our previous $ utility(s) $ for terminal states and is based on human knowledge. The function should give precise feedback on the quality of a state from the perspective of the given player. A sensible function for chess might be a linear combination in the form of:

\begin{equation}
    h(s) = \omega_0f_0(s) + ... + \omega_nf_n(s)
\end{equation}

With functions $ f_i $ calculating different values such as

\begin{itemize}
    \item Material: First, we assign an integer to each piece that represents the piece's relative value (pawn = 1, bishop/knight = 3, rook = 5, and queen = 9). Then sum the values of the pieces left on the board for each player.
    \item Space: Count the squares controlled by each player.
    \item King safety: Check weaknesses in king's position, count attacking pieces close to the king etc.
    \item Win and loss: As a more definitive measure we can indicate whether the current state is a terminal state and hence a winning or losing state.
    \item Etc.
\end{itemize}

By applying different weights $ \omega_i $ to the functions $ f_i $ we essentially give incentives to the agent to prioritize certain behavior. If the win or loss function returns a value of either $-1$ or $+1$, we might combine it with a weight of 10,000 to make sure we choose winning states and avoid losing states above all. Armed with this heuristic function we can find good moves with minimax search even in highly complex state spaces.

However, the problem with heuristic functions is we need expert knowledge and a lot of empirical testing to find a suitable heuristic. In some cases like Go, such a heuristic function might not be competitive with even moderate human players. In other cases such as chess this strategy is very powerful. As mentioned in the introduction IBM's Deep Blue could beat the world's best player Gary Kasparov with heuristic based adversarial search.

\subsection{Alpha-beta pruning}
We can further improve minimax search markedly by using Alpha-beta-pruning. This method tries to eliminate unnecessary traversals down the search tree. In the best case, this leads to a reduction of nodes from $ O(b^d) $ to $ O(\sqrt{b^d}) $.

\begin{figure}
    \centering
    % \includesvg[height=7cm]{alpha_beta_pruning.svg}
    \includegraphics[height=7cm, keepaspectratio]{alpha_beta_pruning.png}
    \caption{Our previous example but with alpha beta pruning applied. The grayed out nodes indicate, that these in fact could be pruned from the tree}
    \label{alpha_beta_pruning}
\end{figure}

The order in which we visit nodes in minimax is similar to a graph traversal with depth first search, meaning we descend down until we find a leaf node. This gives us information about the utility of that node and, consequently, part of the tree. Going up the tree we keep an alpha value for the minimum value the maximizer will receive and a beta value for the maximum value the minimizer will achieve. For instance this lets us know if the minimizer already can choose a move worse than what we can achieve with another move, we do not descend further $ (\alpha > \beta) $.

Looking at the example in figure \ref{alpha_beta_pruning} will help us illustrate this principle. Our search revealed that choosing move $ a_1 $ will yield us a utility of at least $0$. Traversing down move $ a_2 $ the first leaf has a utility of $-1$. Hence, the minimizer will choose a move that is at most $-1$ which is already worse than the utility of $0$. We need not look further at this part of the tree.

The example also shows us an important prerequisite for this method to work. The order in which we expand which node matters decides how many nodes we can prune. Had we visited move $ c_3 $ and $ c_2 $ first, pruning wouldn't have been possible. The best case of $ O(\sqrt{b^d}) $ is entirely dependent on this ordering. We could find different ways of ranking the moves:

\begin{itemize}
    \item \textit{Killer move heuristic} prioritizes moves that are usually undoubtedly good like taking a piece in chess.
    \item \textit{Iterative deepening} Performs a minimax search only to a depth of one and uses the resulting values to rank the moves. Then searching one level deeper we use this ranking for ordering the moves. Even though there is a lot of redundancy, we make up for this more than enough by pruning much more effectively.
\end{itemize}

Other improvements to the procedure are thinkable as well. Once we perform a search for a certain state, we can store the resulting utility. If we encounter this position again, because of a different permutation of the move sequence (transposition), we can just look up the state utility in the \textit{transposition table}.

Combined with alpha beta pruning, the minimax algorithm is a very efficient way of finding the optimal utility in an adversarial search situation. However, as mentioned before in most games we cannot use the utility of terminal states, because the search tree grows too quickly. By optimizing for a heuristic function the quality of play solely depends on this function. For games such as chess minimax has been very successful, because humans could devise meaningful heuristic functions. The chess engine stockfish has been the most successful computer player for a long time and is based on this algorithm (and many optimizations). \cite{noauthor_stockfish_2021, noauthor_stockfish_nodate}

\subsection{Monte Carlo Tree Search}
For games like Go it was deemed impossible to find powerful heuristic functions, which makes the previous approach of minimax not a viable option. In addition, the initial position of a $19\times19$ Go board has a branching factor of $361$ decreasing only by one for each stone placed. A method proposed in 2006 by Coulom \cite{coulom_efficient_2007} called Monte Carlo Tree search was more successful for Go. The main idea is to use simulations or \textit{rollouts (or playouts)} to gain information on the quality of a state. To manage the complexity of the search tree more effectively the algorithm is \textit{selective} in which parts of the tree are \textit{expanded}. This ensures that resources are not wasted on unpromising moves.

In its purest form the simulations are performed randomly, meaning we take a state or node to be investigated and let two random players take turns until a terminal state is reached. Kocsis and Szepesv√°ri \cite{kocsis_bandit_2006} showed that it does in fact converge to optimal play. For games with a high branching factor we need a large number of simulations to get any meaningful information from the simulations, so we might use a \textit{rollout policy} instead. This policy guides the moves taken in the simulation towards better moves. This might be as simple as favoring capturing moves or as we will see later neural networks.

The algorithm iteratively expands the search tree. For each iteration it runs through four steps:
\begin{itemize}
    \item \textbf{Selection} is the process of deciding which node to consider next. We start at the root node and select a node until we reach a leaf node. This is the selected Node. We could select the nodes according to some stationary distribution or we could use the information that we gain over time.

    \item \textbf{Expansion} is the step in which we expand the selected node by appending a fresh child node.

    \item \textbf{Simulation} is as described before the step in which we perform a simulation with our rollout policy starting from the state of the newly generated child node.

    \item \textbf{Back-propagation} is the last step. We take the result of the simulation (utility) and write it to the node and parent nodes above until we reach the root node. Each node updates its cumulative utility $U(n)$ and the number of times it was visited $N(n)$. It is important to note, that this to be differentiated from backpropagation in the context of neural networks (c.f. \ref{neural_networks}).
\end{itemize}

The more this cycle is repeated, the more certainty is gained about the best move to take.

\begin{figure}
    \centering
    % \includesvg[height=7cm]{minimax.svg}
    \includegraphics[height=9cm, keepaspectratio]{monte_carlo_tree_search.png}
    \caption{Monte Carlo tree search stages \cite{noauthor_fig_nodate}}
    \label{monte_carlo_tree_search}
\end{figure}

The development of MCTS led to significant improvements in performance of game-playing agents in the game of Go. The algorithm "Crazy Stone '' from Coulom won the 10th KGS computer-Go tournament against competitors such as Indigo \cite{bouzy_associating_2006}. In the selection phase Crazy Stone estimates the probability of that move being better than the current best move and selects them according to that probability. The probability distribution over the moves is similar to the Gaussian distribution and the Boltzman equations. \cite[p. 4]{coulom_efficient_2007}

Another idea for selection is the upper confidence bound formula (\textit{UCB1}) \cite{auer_finite-time_nodate}, that weights how often we have visited a node $n$ and how promising it is. Let us define \cite[cf. p. 328]{russell_artificial_2021}:

\begin{enumerate}
    \item $\texttt{Parent}(n)$ returns the parent node of node $n$.
    \item $N(n) $ returns the number of playouts performed on node $n$ and its children.
    \item $U(n) $ returns the cumulative utility of node $n$. For instance, this might be the number of wins for node $n$ and its children.
\end{enumerate}

\begin{equation}
    \textit{UCB1}(n) = \frac{U(n)}{N(n)} + C \times \sqrt{\frac{\log{N(\texttt{Parent}(n))}}{N(n)}}
\end{equation}

The cumulative utility $U(n)$ is normalized by the number of times the node was visited $N(u)$. This helps favor moves that are either relatively unexplored and promising or have proven to be good over a larger set of nodes. It is also called the exploitation term. The additional term is called the exploration term. The more often a node is visited, the smaller this term gets, converging to 0 for large $N(n)$. The constant factor $C$ is subject to some debate which value might be optimal, some choose $\sqrt{2}$. In general, this hints at another point of investigation: The problem of \textit{exploration vs. exploitation} \ref{exploration_vs_exploitation} that we will inspect more closely later.

The leading researcher behind AlphaGo and AlphaZero David Silver started his research on Go with MCTS. As early as 2006 he, and Sylvian Gelly, investigated optimizations to MCTS \cite{gelly_achieving_nodate} for the game of Go. In 2011 they published a comprehensive paper \cite{gelly_monte-carlo_2011} proposing the algorithm MoGo and evaluating different strategies to improve the effectiveness of MCTS in Go. Seeing that

\begin{quotation}
    [...] professional Go players often play moves according to intuitive feelings that are hard to express or quantify. Precisely encoding their knowledge into machine-understandable rules has proven to be a dead-end: a classic example of the knowledge acquisition bottleneck. \cite[p. 1873]{gelly_monte-carlo_2011}
\end{quotation}

One of the ideas introduced is Rapid Action Value Estimation (RAVE). It already mentioned how one could reuse information gathered for minimax through a transposition table. In a search tree one will encounter transpositions for that searches were already performed. RAVE allows to reuse experience gathered from simulations for related positions. A key property observed by Silver was that MoGo scales proportional to the amount of compute or rather number of simulations it can perform per turn as depicted in figure \ref{mogo_scaling}.

\begin{figure}
    \centering
    \includegraphics[height=7cm, keepaspectratio]{mogo_scaling.png}
    \caption{Elo rating of MoGo in relation to the computational resources granted to the algorithm \cite[p. 1872]{gelly_monte-carlo_2011}}
    \label{mogo_scaling}
\end{figure}

\section{Learning agent}
The methods described until this point can be described as "Good old fashioned AI". They rely on search and human knowledge to perform adequately. Now we shift our focus to methods that use learning mechanisms to improve their play. With MCTS we've actually seen a kind of intermediary form of algorithm as it is "simulating moves into the future, observing the outcome, and using the outcome to determine which moves are good ones is one kind of reinforcement learning." \cite[p. 331]{russell_artificial_2021}

\subsection{Reinforcement learning}
By devising a heuristic function, we basically told our agent what to do by indicating how a good position looks. The agent optimized its actions to be in a good position as described by the function. In reinforcement learning the agent learns what action to take through interaction. We don't predefine which actions are to be taken. The agent tries to discover which actions yield the best \textit{reward}. The numerical reward signal might come immediately, but e.g. in the case of chess the reward for actions taken comes much later by winning the game (or losing it). According to Sutton and Barto those are the key components of reinforcement learning: "trial-and-error search and delayed reward". \cite[p. 1]{sutton_reinforcement_2018}

Reinforcement learning is not a specific solution or method, all methods for "goal-directed agents interacting in an uncertain environment" \cite[p. 3]{sutton_reinforcement_2018} are types of reinforcement learning. It is a general formalism that will help us reframe the problem of playing a board game (well) in a new light.

\subsection{Markov decision processes}
Central to reinforcement learning is a formalism for the environment called Markov decision process (MDP). Originally derived as an extension of Markov Chains by Richard Bellman \cite{yang_markov_2019, bellman_markovian_1957}, a (finite) MDP describes sequential decision making where actions have long-term consequences, thus, affecting future rewards. The MDP is an abstraction of "goal directed learning from abstraction" that reduces the problem to three components: Actions, states and rewards that are being exchanged between the agent and the environment. We already introduced the agent-environment interaction loop in figure \ref{agent_environment_loop}. We can reframe this image with the new terminology of the MDP framework in figure \ref{mdp_agent_environment}. For a \textit{finite} MDP the sets of all actions $ \mathcal{A}$ (\textit{action space}), states $\mathcal{S}$ (\textit{state space}) and rewards $ \mathcal{R} $ have a finite amount of elements.

\begin{figure}
    \centering
    \includegraphics[height=4cm, keepaspectratio]{mdp_agent_environment.png}
    \caption{The markov decision process as agent-environment interaction loop}
    \label{mdp_agent_environment}
\end{figure}

We divide the passage of time into discrete time steps $ t $ at which the agent senses the state $ S_t \in \mathcal{S} $ and then selects some action $ A_t \in \mathcal{A}(s) $. Resulting in that action is some reward $ R_{t+1} \in \mathcal{R} \subset \mathbb{R} $ that is received in the next timestep.

The transition probabilities between the state $ s $ and the next state $ s' $ and its reward $r \in \mathcal{R}$ is given by the function $ p $ which essentially defines the decision process as a whole. A game of chess is deterministic, all transitions have a probability of $1$ and these probabilities are defined by the rules of the game. However, the MDP is formulated to incorporate stochastic environments as well:

\begin{equation}
    p(s', r | s, a) = P\{S_t=s', R_t = r | S_{t-1} = s, A_{t-1}=a\}
\end{equation}

In a MDP "the state must include information about all aspects of the past agent environment interaction that make a difference for the future. If it does, then the state is said to have the \textit{Markov property}". \cite[p. 48]{sutton_reinforcement_2018} This means the transition probability $p$ is independent of the history of preceding states.

From these properties of the MDP we can define the four central components of Reinforcement learning:

\begin{itemize}
    \item \textbf{The reward signal} is the description of the agent's goal. A controller of a cooling system for a server farm might have the goal to minimize the energy spent for cooling while keeping the servers below a certain threshold. The reward signal then encompasses both of these subgoals. The controller has to maximize this reward. The \textit{reward hypothesis} states that:
          \begin{quotation}
              That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). \cite[p. 52]{sutton_reinforcement_2018}
          \end{quotation}
          Therefore, we introduce the reward $R_t $ and the goal $ G_t $ which is in the simplest case the sum of future rewards until the final time step $T$.

          \begin{equation}
              G_t = R_{t+1} + R_{t+2} + ... + R_T
          \end{equation}

          We might also discount the future rewards by some factor $\gamma \in [0, 1] $ to account for the decrease in certainty we have about future rewards:

          \begin{equation}
              G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^kR_{t+k+1}
          \end{equation}

    \item \textbf{The policy} is a mapping between the perceived states to probabilities of selecting each possible action. For TicTacToe we might imagine a table that lists for all possible states to the agent's action. Due to the size of the state space \ref{complexity_table} in more complex games this would not be feasible, therefore we used search processes as a policy so far.

          We denote the policy as the function $\pi $. It defines a probability distribution over all actions $ a \in \mathcal{A}(s)$ for each $ s \in \mathcal{S} $. $ \pi(a|s)$ is probability at a given timestep $t$ for the action to be $ a = A_t $ under the condition that $ s = S_t $.

    \item \textbf{The value function} is an estimation of the reward for the agent to be in a given state $ s $. In other words, it is the expected value $\mathbb{E}$  for the goal $G_t$ given the state $s$.  As the reward for a state depends on what actions we take in the future, the value depends on the policy $ \pi $ defined above. Given that we are at timestep $ t $ and in state $ s = S_t $, the \textit{state-value function $v_{\pi}$}:

          \begin{equation}
              v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^kR_{t+k+1} \middle| S_t = s \right]
          \end{equation}

          The heuristic function encountered previously was essentially a value function being used to guide the game-tree search.

          Furthermore, we define the value of action $a$ while being in state $s$ under the policy $\abstractname{}pi$ as the \textit{action-value function} $q$:


          \begin{equation}
              q_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^kR_{t+k+1} \middle| S_t = s, A_t = a \right]
          \end{equation}

          \begin{figure}
              \centering
              \includegraphics[height=2cm, keepaspectratio]{policy_value_action_value.png}
              \caption{A visual explanation of policy, value and action-value \cite[p.62]{sutton_reinforcement_2018}}
              \label{policy_value_action_value}
          \end{figure}

          The figure \ref{policy_value_action_value} shows how the three elements of policy, state-value and action-value relate to each other.

    \item \textbf{The model} helps to make predictions about how the environment might behave. For a given state and action it might return to the next state which aides in planning ahead. In the case of Abalone the model is the rules of the game, used by the function that returns all legal moves or the resulting board from a given board and a move.
\end{itemize}

The goal of reinforcement learning is to find (or approximate) a policy that maximizes the future reward for each action and state. For MDPs we define the \textit{optimal policy} $\pi_*$ as having higher or equal return than all other policies. Thus, the optimal policy maximizes the value function, resulting in the \textit{optimal state-value function}:

\begin{equation}
    v_{*}(s) = \max_{\pi} v_{\pi}(s)
\end{equation}

for all $s \in mathcal{S}$. For instance, in chess the goal is to find a policy that approximates the utility returned by exhaustive search. Given such a policy one could evaluate the best possible outcome for any state $s$ with the optimal state-value function. For this optimal policy we also have an optimal action-value function $ q_{*} $ for all $ s \in \mathcal{S}$:

\begin{equation}
    q_{*}(s, a) = \max_{\pi}q_{\pi}(s, a)
\end{equation}

\subsection{Exploration vs. Exploitation}
\label{exploration_vs_exploitation}
As the agent builds its knowledge while it is engaged in the environment it has to weigh \textit{exploiting} the gathered knowledge for ensuring a safe short term reward or sacrificing it for \textit{exploring} other actions that might turn out to bring higher rewards in the future. To illustrate this fundamental tradeoff in reinforcement learning, let us imagine a gambling machine with 10 levers, a 10-armed bandit. At each timestep $ t $ we have to decide which lever to pull and then we receive some reward $ R_t $. Each lever has some unchanging (\textit{stationary}) distribution over the rewards that is hidden from us. The distribution looks like the one given in figure \ref{10_armed_bandit}.

\begin{figure}
    \centering
    \includegraphics[height=7cm, keepaspectratio]{10_armed_bandit.png}
    \caption{The reward distributions of a 10-armed bandit \cite[p.28]{sutton_reinforcement_2018}}
    \label{10_armed_bandit}
\end{figure}

To estimate the action-value of each lever, we sum the rewards received for that lever $ \texttt{total-reward}(a) $ and divide it by the number of times we've chosen the lever $ N_t(a) $ at the timestep $ t $.

$$
    Q_t(a) = \frac{\texttt{total-reward}(a)}{N_t(a)}
$$

This is called the \textit{sample-average} method. The simplest policy would be to just always choose the action with the largest sample-average $ Q_t $ a \textit{greedy} policy.

$$
    A_t = \operatorname*{argmax}_a Q_t(a)
$$

Initially, we might choose a random lever and that yields a reward of 0. As all $ Q_t $ are still 0 we choose another random lever giving us a reward of 1. Then ,according to the greedy policy, we just repeatedly pull this lever indefinitely. We just get stuck on exploiting the small knowledge we have gained. To allow for some exploration of other actions we introduce a small probability $\varepsilon $ of choosing a different (random) action:
$$
    A_t =
    \begin{cases}
        \operatorname*{argmax}_a Q_t(a) & \text{with } 1 - \varepsilon \\
        \text{random } a                & \text{with }  \varepsilon    \\
    \end{cases}
$$

If we continue for an infinite number of times $ Q_t $ is guaranteed to converge to $ q_{*} $ as we sample each action enough to estimate its stationary distribution. We might also let the $ \epsilon $ decay over time to ensure we exploit the optimal lever eventually. This shows how we have to carefully consider what knowledge we have and how we plan to expand it further.

\subsection{(Deep) Neural networks}
\label{neural_networks}
As reinforcement learning is a very general framework things like the value function $ v(s)$ are just left as an abstract function. As neural networks are a general function approximator, we could use them to approximate the optimal value function $ v_{*}(s) $ to maximize rewards. Hence, we should introduce the basic building blocks of this technique to gauge whether we might utilize them further.

Neural networks are one specific machine learning method that has had large success in the recent past. Machine learning has been a paradigm shift in the way we think about building programs. In the classical development one uses the data and predefined rules as input for the development. This means we have to have intricate knowledge about the problem domain to produce the answers we want. For tasks like image classification this is a difficult process as it is very hard to think of patterns and conditions an image has to have, to find cats in them.

\begin{figure}
    \centering
    \includegraphics[height=5cm, keepaspectratio]{programming_paradigm_shift.png}
    \caption{A fundamental shift in how we think about programming \cite[p. 5f.]{moroney_ai_2020}}
    \label{programming_paradigm_shift}
\end{figure}

In machine learning we use the answers and the data as input to the development process and the rules are the output we produce. Figure \ref{programming_paradigm_shift} contrasts this change in programming.

Consider any linear function of the form:
$$
    f(x) = w_1x + w_0,
$$
with $ x, w_1, w_0 \in \mathbb{R} $. We could use this function to make predictions about one dimensional input data. For example we might have a (training) dataset of living space of houses in $ m^2 $ $ X $ and their corresponding price $ Y $. Each row $X_i$ corresponds to the row $Y_i$. We could choose the bias $ w_0 $ and the \textit{weight} $ w_1 $ such that the squared error for all rows $i$ is minimal in a process we know as linear regression. When given new data our linear model can make predictions of the potential prices of the houses.


\begin{figure}
    \centering
    \includegraphics[height=4cm, keepaspectratio]{neural_networks.png}
    \caption{The fundamental idea behind neurons}
    \label{neural_network}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=3cm, keepaspectratio]{neural_network_layers.png}
    \caption{A small neural network arranged in layers. An input layer, a hidden layer and an output layer}
    \label{neural_network_layers}
\end{figure}

\paragraph{Activation function}
An important component is not only the linear function but also the activation function: Before we pass the value of the function f(x) on, we apply some function $ \phi $. An activation function applied to a linear function is referred to as \textit{neuron}. The activation function allows us to introduce a non-linearity, which in turn makes smaller networks of neurons able to perform much better on more complex tasks than just a simple linear activation. Common functions used are:
\begin{itemize}
    \item ReLu: $ \phi : max(0, f(x)) $
    \item Sigmoid: $ \phi : \frac{1}{1 + e^{f(x)}} $
    \item Binary step: $ \phi : \begin{cases}
                  0 & f(x) < 0    \\
                  1 & f(x) \geq 0 \\
              \end{cases}$
\end{itemize}

This resulting basic neuron is depicted in figure \ref{neural_network}. If we want to make more complex inferences than just the prices of houses, we can arrange multiple neurons into larger structures like chains or layers (\ref{neural_network_layers}). To build such networks we need to generalize the linear function of the neuron for higher dimensional input:

\begin{equation}
    y_k = \phi\left(\sum_{j=0}^{m}w_jx_j\right)
\end{equation}

But this poses a problem. We can find globally optimal solutions for linear neurons with linear regression. However, this does not work for other activation functions. Moreover, as the size of the network grows, the computational cost of these methods grows too large. Moreover A different approach is necessary.

\paragraph{Loss functions} In the context of linear regression the term \textit{mean squared error} was already mentioned. It is the average squared difference between the predictions and the desired output:

\begin{equation}
    MSE = \frac{1}{n} \sum_{i = 1}^{n}(X_i - Y_i)^2
\end{equation}

This is one type of \textit{loss function}, which in general is a measure to describe the error we seek to \textit{minimize} in an optimization process.

\begin{figure}
    \centering
    \includegraphics[height=5cm, keepaspectratio]{loss_curve.png}
    \caption{(a) Data points of price versus floor space of houses for sale in Berkeley, CA, in July 2009, along with the linear function hypothesis that minimizes squared-error loss: y = 0.232x + 246. (b) Plot of the loss function for various values of $ w_0 $, $ w_1 $. Note that the loss function is convex, with a single global minimum. \cite[p. 1251]{russell_artificial_2021}}
    \label{loss_curve}
\end{figure}

\paragraph{(Stochastic) gradient descent and backpropagation} By utilizing the loss function we can feed an input into the network and measure for any permutation of the weights $ w_i $ how big the error of the network is. By measuring the loss of the training set $ X $ we can find out how well the current configuration of the network fits the data.

Let's go back to the problem of house price prediction. If we plot the MSE for the weights $ w_0 $ and $ w_1 $ of our single neuron the result is a parabola in figure \ref{loss_curve}. Ideally, we want to walk down into the valley where the error is minimal. So for any given combination of the weights we have to find out in which direction the downward slope maximal. The direction of the greatest change of a scalar function is called gradient, which is formalized as follows:

$$
    \nabla f(p)=\left[\begin{array}{c}
            \dfrac{\partial f}{\partial x_1}(p) \\
            \dfrac{\partial f}{\partial x_2}(p) \\
            \vdots                              \\
            \dfrac{\partial f}{\partial x_n}(p)
        \end{array}\right]
$$,
where the point $p $ is a point in the \textit{parameter space}, e.g. as depicted in figure \ref{loss_curve}(b) for $w_0 $ and $ w_1 $. For a single neuron we can use the methods learned in calculus to find the respective \textit{partial derivatives} $  \dfrac{\partial f}{\partial x_i}(p) $. By repeatedly calculating the gradient and updating the weights to move further in the direction of the downward slope, we describe the algorithm of gradient descent:

\begin{algorithm}
    \caption{Gradient descent outline \cite[p. 1253]{russell_artificial_2021}}\label{alg:gradient_descent}
    \begin{algorithmic}
        \State $w \gets \text{any point in the parameter space}$
        \While{not converged}
        \ForEach{$w_i$ in $w$}
        \State $w_i \gets w_i - \alpha \dfrac{\partial f}{\partial w_i}(Loss(w))$
        \EndFor
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The variable $ \alpha $ for gradient descent describes the \textit{learning rate}, so by how much we update the weights. As we have to differentiate not only a single neuron but a network of neurons, some constraints have to be defined to make derivation possible:

\begin{itemize}
    \item All functions utilized by the neuron and the network (activation, linear combination of weights) have to be differentiable
    \item The network has to be a directed acyclic graph, thus, no loops etc.
\end{itemize}

This makes the neural networks one type of \textit{computational graph}, which have the convenient property letting us find the gradients of the weights in respect to the loss by \textit{backpropagation} by essentially applying the chain rule. Figure \ref{computational_graph} shows an example of a computational graph and its derivation.

\begin{figure}[!h]
    \centering
    \subfloat[Functions are nodes]{
        \includegraphics[width=6cm, keepaspectratio]{computational_graph_functions.png}
    }
    % \hfill
    \subfloat[Differentiate the graph by node $e$]{
        \includegraphics[width=6cm, keepaspectratio]{computational_graph_derivative.png}
    }
    \caption{An example of a computational graph}
    \label{computational_graph}
\end{figure}

In summary, by defining a neural network, we define a weight space. This space is essentially a room of all possible values of the weights. We can think of neural networks as functions or programs. Thus, the architecture of the neural network decides on a range of possible programs. Each of these programs is more or less suited for the task given. The feedback given by the loss function indicates the suitability. With gradient descent we search this space of programs for one that minimizes the error. Due to the nature of gradient descent, we are not guaranteed to find the globally optimal program.

\subsection{Convolutional Neural Network}
As already stated, the neural network or computational graph is not limited to one specific type of function as long as the function can be differentiated. As shown in \ref{neural_network_layers} in its basic form, neural networks have a n-dimensional vector as input. If we have an input like an image, we would have to break down the information about the adjacency to fit the form of a vector. As a result, permutations of the values of the image wouldn't change the result of the output.

Furthermore, the computational requirements for such an approach would grow rapidly. A fully connected first layer would need $n^2$ weights for an image of the size $n$. A small image of the size 256x256 would need 65536 weights for the first layer alone. Ideally we would also like the output of the network to be invariant to translation. This means that a pattern found in the corner should produce the same response as a pattern found in the center of the image.

To achieve this we can utilize convolution. In general terms a "convolution is an integral that expresses the amount of overlap of one function $g$ as it is shifted over another function $f$." \cite{weisstein_convolution_nodate}

$$
    f * g(x) = \int_{-\infty}^{\infty} f(t)g(x - t) dt
$$

This might seem very abstract but firstly, only the discrete case is relevant to us and secondly, only the finite case. This boils down the convolution to filtering $F$, which also comes up in image processing:

$$
    F(u, v) = \sum_{x} \sum_{y} I(u+x, v+y)H(x, y)
$$

For a given coordinate $u,v$ each value in the Kernel $H$ is multiplied by the  corresponding value of the image returned by function $I$. The figure \ref{convolution} illustrates how a convolution would look on a black and white image. In image processing for example, we might use a Laplacian filter to extract certain features like edges. By using convolutions in a neural network we move away from these hand-crafted features by letting the parameters of the kernels be trained to fit the desired outcome. By combining multiple layers of convolutions the network can extract higher level abstractions of the image. \cite{ilin_abstraction_2017}

\begin{figure}
    \centering
    \includegraphics[height=5cm, keepaspectratio]{convolution.png}
    \caption{Convolution of a 8x8 black and white image with a 3x3 kernel, no padding and a stride of 1. \cite{bruasdal_deep_2020}}
    \label{convolution}
\end{figure}


The application of this method for image related tasks has become very popular. The first large scale application in 2012 by Krizhevsky et al. \cite{krizhevsky_imagenet_2017} vastly outperformed any previous methods. This approach is very relevant to the board game setting in Abalone as each board position is essentially an image. Patterns of marbles are relevant in different positions of the board just as different shapes and objects in different places in an image.

\subsection{Residual networks}
By stacking many layers a network can fit more complex domains. Their size has earned them the name \textit{deep neural networks}. But a central issue with increasing the size of the network is the problem of \textit{vanishing gradients}. For example the sigmoid function is in it's limit $0$ for $-\infty$ and $1$ for $\infty$ and its derivative approaches $0$ for both cases as depicted in figure \ref{residual_network} (a). That means in backpropagation larger input values produce very small output values when passed through such a neuron. As the derivatives are multiplied during the backpropagation (chain rule) they become even smaller, thus vanishing as they further approach $0$ with each added layer.

\begin{figure}[!h]
    \centering
    \subfloat[Sigmoid function and its derivative \cite{noauthor_wolframalpha_nodate}]{
        \includegraphics[height=4cm, keepaspectratio]{sigmoid_vs_derivative.png}
    }
    % \hfill
    \subfloat[A residual block \cite{li_sewer_2019}]{
        \includegraphics[height=4cm, keepaspectratio]{residual_block.png}
    }
    \caption{Motivation and implementation of residual networks}
    \label{residual_network}
\end{figure}

To mitigate this issue one could either use different activation functions like the ReLu or use residual connections. The value of the input before passing through the neuron is added with some weight to the output as shown in figure \ref{residual_network}(b).

This section on neural networks has only touched the raw fundamentals of neural networks very swiftly. Nevertheless it shows that the core ideas are quite simple (in hindsight of course), especially compared to the very comprehensive methods of classical AI. The theoretic foundations to these methods are actually quite old. With the unlocking of more and more computational power and data researchers discovered the "unreasonable effectiveness of data \cite{halevy_unreasonable_2009} and the combination of neural networks and gradient descent. To dive deeper the book "Artificial Intelligence: A Modern Approach" \cite{russell_artificial_2021} offers a very good first impression.

\subsection{AlphaGo}
Now we have reached the culmination of our theoretical analysis. All the components we introduced so far constitute the knowledge necessary to understand AlphaGo and AlphaZero. Just like Abalone Go is a perfect information zero-sum board game. It already popped up in the complexity comparison \ref{complexity_table}, which clearly shows that Go is more complex in its state space and its search tree. At the time when there was a lot of success in other games with minimax many people saw Go as the most challenging board game \cite{muller_computer_2002}. The only successful method at the time was MCTS, for which we already saw two influential papers in the context of Go and MCTS by David Silver.

By combining the reinforcement learning framework with MCTS and neural networks David Silver et al. achieved the milestone of beating Lee Seedol. The program AlphaGo consists of three key components:

\begin{enumerate}
    \item A rollout policy network $p_{\pi}$ and a policy network $p_{\sigma}$ trained by supervised learning
    \item A policy network $p_{\rho}$ trained with reinforcement learning and a value network $v_{\theta}$ derived from $p_{\rho}$
    \item Lookahead search using (asynchronous) Monte Carlo Tree Search guided by $p_{\sigma}$ and $v_{\theta}$. The rollouts for the search are performed by the rollout network $p_{\pi}$.
\end{enumerate}

The first step is the supervised training based on the KGS Go Server dataset of 30 million positions. The rollout policy network $p_{\pi}$ is a relatively small network that is supposed to provide quick rollouts (simulations). As we already encountered for Abalone this is necessary because random rollouts for such complex games can produce very weak guidance for low numbers and also very long matches. This is supported by comparisons made between the prediction quality of 100 rollouts with a uniformly random policy versus with the network $p_{\pi}$ in figure \ref{alpha_go_prediction_quality}.

\begin{figure}
    \centering
    \includegraphics[height=4cm, keepaspectratio]{alpha_go_prediction_quality.png}
    \caption{Comparison prediction quality of the different components \cite{silver_mastering_2016}}
    \label{alpha_go_prediction_quality}
\end{figure}

The policy network $p_{\sigma}$ is trained in the same fashion except it is larger in size and thus is computationally more expensive. The policy network $p_{\sigma}$ is used for the next step. The goal is to adjust "the policy towards the correct goal of winning games, rather than maximizing predictive accuracy" \cite[p.484]{silver_mastering_2016} by using policy gradient RL and self-play. Initially, the network $p_{\rho}$ is initialized with structure and weights equivalent to $p_{\sigma}$. Then games with the current version of the policy network $p_{p}$ are played against random previous versions of the network. At timestep $t$, for a mini-batch of $n$ games, the results of the games are taken and the REINFORCE algorithm is used to fit the weights of the networks to the outcome of the game $z_t$. The newly trained policy network $p_{\rho}$ wins 80\% of the games against $p_{\sigma}$.

Lastly, $p_{\rho}$ is used to train a neural network with weights $\theta$ for the evaluation function $v_{\theta}(s)$. This evaluation function approaches the state-value function $v_{p_{\rho}}(s)$, so the expected outcome of the game given the policy $p_{\rho}$: $v_{\theta}(s) \approx v_{p_{\rho}}(s)$. Figure \ref{alpha_go_prediction_quality} shows how $v_{\theta}$ appraches the same prediction accuracy as doing 15,000 rollouts with policy $p_{\rho}$.

\begin{figure}
    \centering
    \includegraphics[width=16cm, keepaspectratio]{alpha_go_mcts.png}
    \caption{Monte Carlo Tree Search in AlphaGo. \cite{silver_mastering_2016} Note the algorithm structure mentioned in figure \ref{monte_carlo_tree_search} is maintained, but selection and simulation is more advanced}
    \label{alpha_go_mcts}
\end{figure}

During live play against an opponent AlphaGo uses MCTS to improve the performance versus just using the pure output of the policy network. The edges, or state $s$ action $a$ pairs, of the search tree store an action value $Q(s, a)$, a visit count $N(s, a)$ and a prior probability $P(s, a)$ (cf. figure \ref{alpha_go_mcts}).

\begin{itemize}
    \item During the \textbf{selection} phase the next child is selected by taking the action with maximum action value. To encourage exploration in the beginning an additional term $u(s,a)$ is added to the action value $Q$. Each step during selection can be described as $t$ yielding:
          \begin{equation}
              a_t = \operatorname*{argmax}_a (Q(s_t,a) + u(s_t, a))
          \end{equation}
          with the exploration term:
          \begin{equation}
              u(s, a) \propto \frac{P(s,a)}{1 + N(s,a)}
          \end{equation}
    \item When a leaf node is reached at timestep $T$, the state $s_T$ is \textbf{expanded} by the policy network $p_{\sigma}$ (Note that $p_{\sigma}$ is being used instead of $p_{\rho}$, empericial analysis be the team showed this performs better). The resulting probability distribution over the actions is stored as prior probabilities: $P(s_T, a) = p_{a|s_T}$.
    \item The \textbf{evaluation} of the leaf node $s_L$ is done by combining the evaluation function $v_{\theta}$ and the results of the rollouts $z_L$ played with the rollout network $p_{\pi}$. Both elements are weighted by a mixing parameter $\lambda$:
          \begin{equation}
              V(s_L) = (1 - \lambda)v_{\theta}(s_L) + \lambda z_L
          \end{equation}
    \item Lastly, after each simulation the results are \textbf{backpropagated} through all visited edges the root node. The count $N(s,a)$ is incremented and $Q(s,a)$ is updated by:
          \begin{equation}
              Q(s,a) = \frac{1}{N(s,a)}\sum_{i=1}^{n}1(s,a,i)V(s_L^i)
          \end{equation}
          where $n$ is the number of simulations " s L i is the leaf node from the $i$th simulation, and $1(s,a,i)$ indicates whether the edge was traversed during the $i$th simulation" \cite[p. 529]{silver_mastering_2016}.
\end{itemize}

The computational requirements for AlphaGo are significant. During training the team utilized 50 GPUs, the supervised learning stage ran for 3 weeks anyways. During live play they utilized a machine with 48 CPUs and 8 GPUs, a distributed variant with 1,202 CPUs and 176 GPUs was tested as well.

A more detailed entry to the topic is provided by the nature article "Mastering the game of Go with deep neural networks and tree search". \cite{silver_mastering_2016}

\subsection{AlphaZero}
A problem with AlphaGo's architecture is its complexity. There are many hyperparameters for the four different neural networks and many other moving parts. AlphaZero is a simplification that conveniently performs even better and is easier to generalize.

AlphaZero learns "tabula rasa", from a blank slate. Essentially, AlphaZero only uses a modified version of the self-play RL mechanism from AlphaGo, all other parts from the training pipeline are scrapped. The two main components of AlphaZero are:
\begin{enumerate}
    \item A single neural network $f_{\theta}$. Compared to AlphaGo, the architecture of the neural network has been simplified by combining the value network and the policy network into one network with two heads. The network is only trained through self-play.
    \item A simplified version of AlphaGo's MCTS, that doesn't use rollouts and only relies on the network $f_{\theta}$ to guide the search. The MCTS variant is not only used for live play but also for self-play.
\end{enumerate}

The network $f_{\theta}$ takes the current board state and the last seven boards as input. The position of the players marbles is split into seperate planes, $1$ for a placed stone and $0$ if there is none. Additionally, there is one feature plane to indicate the player in turn which is set to $1$ for black's turn and $0$ for white's turn. This makes a total of 17 planes forming an input stack of the size $19 \times 19 \times 17$.

\begin{figure}[!h]
    \centering
    \subfloat[The general architecture of the network]{
        \includegraphics[width=6cm, keepaspectratio]{alphazero_architecture_network.png}
    }
    \hfill
    \subfloat[The layers of the value head]{
        \includegraphics[width=3cm, keepaspectratio]{alphazero_architecture_policy_head.png}
    }
    \hfill
    \subfloat[The layers of the value head]{
        \includegraphics[width=3cm, keepaspectratio]{alphazero_architecture_value_head.png}
    }
    \caption{The archtecture of the neural network $f_{\theta}$ \cite{silver_mastering_2017}}
    \label{alpha_zero_neural_network}
\end{figure}

As shown by figure \ref{alpha_zero_neural_network} (a) the network is divided into four types of blocks:

\begin{itemize}
    \item The convolutional block and residual tower hold they main chunk of trainable parameters.
    \item The value head outputs a scalar representing the value function.
    \item The policy head returns a probability distribution over all possible moves.
\end{itemize}

The team showed empirically \cite[p. 9]{silver_mastering_2017} that this \textit{dueling-network} \cite{wang_dueling_2016} architecture works significantly better for Go than other architectures.

\begin{figure}[!h]
    \centering
    \subfloat[The self-play]{
        \includegraphics[width=8.2cm, keepaspectratio]{alpha_zero_self_play.png}
    }
    % \hfill
    \subfloat[The training step with experience $(s_i, \pi_i, z)$ from self-play]{
        \includegraphics[width=8.2cm, keepaspectratio]{alpha_zero_training.png}
    }
    \caption{The self-play training pipeline of AlphaZero \cite[p. 5]{silver_mastering_2017}}
    \label{alpha_zero_training}
\end{figure}

To train the network $f_{\theta}$, it is initialized with random weights. At each turn $i$ during self-play $f_{\theta}$ guides a MCTS, which produces a probability distribution $\pi_t$ over all (legal) moves. The move $a_i$ is selected according to the probabilities $pi$: $a_i ~ \pi_t$. So instead of using the network $f_{\theta}$ directly to decide on the next move, MCTS produces an improved policy through repeated application of $f_{\theta}$. The authors describe MCTS in this context as a \textit{policy improvement} operator. The self-play games are played until a terminal state $s_T$ is reached. The result of the game is $z_T \in \{-1, +1\}$ for either a loss or a win.  For each move during the game he result of the game $z_t$ is recorded together with the board state $s_t$ and the search probabilities $\pi_t$, where $z_t \mp z_T $ depending on the players perspective: $(s_t, \pi_t, z_t)$. The self-play is illustrated in figure \ref{alpha_zero_training} (a).

This tuple is stored in an experience buffer which is used for training the next iteration of the network. The parameters of the networks are fitted to mini-batches from the experience buffer match the probabilities $\pi_t$ for the policy head and the game result $z_t$ for the value head. In total, $700,000$ mini batches of size $2,048$ from $4.9$ million games were sampled.
