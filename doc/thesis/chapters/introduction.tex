\chapter{Introduction}

Board games are and have been a popular environment to test the capabilities of state of the art artificial intelligence against human opponents. Many board games are widely known making them a tangible measure of performance. The most prominent examples are the games of Chess and Go. For both, machines defeating the current best players has been representative of fundamental progress in computing.

IBM's "Deep Blue" defeated Gary Kasparov in 1996 \cite{higgins_brief_2017} by utilizing search to look ahead into the game tree and deliberate on the next move. This approach is a prime example for symbolic AI approaches, "good-old-fashioned-AI" ("GOFAI") \cite{haugeland_artificial_1985}, which rely on logic and search on symbolic representations.

However, these knowledge-based approaches are severely limited by our ability to properly model the problem correctly and exhaustively. For example, in the case of Deep Blue it requires us to encode our knowledge about chess in a heuristic function to evaluate the board. Only then we can search for actions that maximize this function. Problems with large complexity would require tremendous efforts, which just become unfeasable at a certain point. A different approach would be devising (general) methods to learn the necessary domain knowledge from scratch,  \emph{tablula rasa}. As Alan Turing put it:

\begin{quote}
    Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the childâ€™s? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child-brain is something like a note-book as one buys it from the stationers. Rather little mechanism, and lots of blank sheets. [...] Our hope is that there is so little mechanism in the child-brain that something like it can be easily programmed.
    \cite{turing_icomputing_1950}
\end{quote}

The recent success of "AlphaGo" in 2016 against the long-time world-champion Lee Sedol \cite{deepmind_match_nodate} in the game Go is a milestones that perfectly demonstrates this shift towards "bottom-up" or subsymbolic methods. \cite{nilsson_artificial_1998} The increasing availability in computational power (and data) has enabled two subsymbolic methods to find large success in unclaimed territory such as copmuter vision or natural language processing. Namely those are neural networks and (stochastic) gradient descent. Combined they provide a general function approximator, that can be trained in a process akin to the learning described by Turing.

In the case of Go, designing a powerful heuristic function was deemed not possible for humans. AlphaGo used (deep) neural networks and gradient descent to train a evaluation function based on a large database of expert moves. With the help of Monte Carlo Tree Search they used this function to play against itself and improve further. \cite{silver_mastering_2017}

Building on this success DeepMind, the company behind AlphaGo, further improved the architecture. "AlphaGo Zero" and the generalization "AlphaZero"  learn, without the help of the database of expert moves and surpassed the performance of AlphaGo significantly. Since then the architecture has been applied to Chess, Shogi and Atari games by removing the last piece of human knowledge in the system: The rules of the game. \cite{schrittwieser_mastering_2020}

At this point our endeauvor begins, as the purpose of this writing is to apply the methods of AlphaZero to the game of Abalone.

\section{Research goals}
First, let us establish the main research questions that will guide us throughout this thesis.

\paragraph{The goal} is to apply the general framework of self-play learning outlined in "Mastering the game of Go without human knowledge". \cite{silver_mastering_2017} The paper gives clear instructions on the theoretical groundwork for the system but doesn't give clear instructions for the implementation. There is no open source code provided.

\paragraph{Sub-goal 1} is to compare classical search based methods to this AlphaZero's deep reinforcement learning based on several criteria such as win/loss ratio, computational requirements, etc.