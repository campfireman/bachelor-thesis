\chapter{Conclusion}
\label{conclusion}

\section{Goal Evaluation}
The application of AlphaZero to Abalone has proven to be difficult. Without the availability of an training framework based on AlphaZero, the time-frame for this thesis would have been too small. The framework gave some guarantees about correctness and saved engineering efforts. The hexagonal board and the more complex move system introduced difficulties in applying the original architecture. A more significant problem is the state space and game tree complexity of Abalone, which require much more compute than Hex or Othello. The high demands for compute limited the amount of experiments, that could be run. Machine learning is highly empirical, therefore less iterations means less hypotheses and hyperparameter configurations could be tested. The originally stated goal was to apply the methods of AlphaZero to Abalone. As  we created a training pipeline and game-playing agent capable of running, this goal is at least partially achieved. However, no trends of convergence towards an optimal policy was observed. The most successful variant was the network, that was warmed-up with experience from the heuristic agent. It was immediately able to beat the random agent in 100\% of the games.

The achieved results are weak when compared with the performance of minimax based methods. The compute resources required overshadow the modest single threaded implementation by Verloop. As Abalone has a lower strategic complexity than e.g. chess, the heuristics designed by humans are quite powerful. A big advantage of the self-play learning is the ability to learn new strategies, previously not known to humans, as observed in AlphaGo. We could not achieve this augmentation of human knowledge for Abalone. The win-ratio against heuristic players was too low.

\section{Future Work}
The method proposed by AlphaZero is extremely powerful and remains promising for Abalone as well. Due to limitations in compute (or efficiency) for the experiments it has not been possible to replicate the ground breaking success achieved in Go for Abalone. This also points at the major downside of the method as it requires significantly more compute than any classical knowledge based methods.

Looking at deep RL in general, a lowering in the cost for compute would greatly benefit this method. Gradient descent and even simple feedfowards for neural networks are very expensive operations even with the proliferation of ever more powerful hardware accelerators. The hardware used by DeepMind for AlphaGo is only accessible to top researchers in the field due to the high cost. There are two main potential avenues through which we could reap the benefits of this method with lower capital requirements:

\begin{itemize}
    \item Further theoretic improvements bringing significant speedups. This could be something along the lines of the incremental improvement between AlphaGo and AlphaZero or full paradigm shifts in the methodology.
    \item The cost for training neural networks coming down by an order of magnitude from current levels. This could be due simply to the passage of time as in the past accelerators like GPUs still followed an exponential improvementrate as observed in Moore's Law for CPUs \cite{moore_cramming_2006} ("Huang's Law" \cite{noauthor_huangs_2021}). Another factor would be archtitectural changes that improve scalability and performance for deep learning specifically. Additionally, the rising economic significance of machine learning has provided an incentive for more specialized hardware like TPUs \cite{noauthor_tpu_nodate} or Jim Keller's Grayskull. \cite{noauthor_grayskull_nodate} The same reason reignited interest in optical computing accelerators to bring drastic changes in power requirements and performance for matrix multiplication. \cite{noauthor_lightmatter_nodate,noauthor_lightelligence_nodate}
\end{itemize}

In addition to gaining more compute resources, there are also potential theoretic improvements to make. Abalearn describes a "risk-sensitive" approach as "the problems we encountered was that self-play was not effective because the agent repeatedly kept playing the same kind of moves, never ending a game" \cite[p. 8]{campos_abalearn_2003}. The function described could be transferred to this work in order to encourage more aggressive game-play.

Adjusting the reward to nudge the behaviour of the agent into a certain direction remains promising as well, even though the attempt was not successful. It could be combined with a pretrained network and a longer training duration to achieve stronger results.

The team around David Silver at DeepMind continues to generalize and improve the method of self-play in combination with deep RL. Very recently they expanded the scope of the framework to include imperfect-information games like poker. They dubbed it the "Player of Games" \cite{schmid_player_2021}.