\chapter{Conclusion}
The method proposed by AlphaZero is extremely powerful and has proven to be promising for Abalone as well. Due to limitations in compute for the experiments it has not been possible to replicate the ground breaking success achieved in Go for Abalone. This also points at the major downside of the method as it requires significantly more compute than any classical knowledge based method. Gradient descent and even simple feedfowards for neural networks are very expensive operations even with the proliferation of ever more powerful hardware accelerators. The hardware used by DeepMind for AlphaGo is only accessible to top researchers in the field due to the high cost. There are two main potential avenues through which we could reap the benefits of this method with lower capital requirements:

\begin{itemize}
    \item Further theoretic improvements bringing significant speedups. This could be something along the lines of the incremental improvement between AlphaGo and AlphaZero or full paradigm shifts in the methodology.
    \item The cost for training neural networks coming down by an order of magnitude from current levels. This could be due simply to the passage of time as in the past accelerators like GPUs still follow an improvementrate as Moore's Law ("Huang's Law"). \cite{noauthor_huangs_2021}  Another factor would be archtitectural changes that improve scalability and performance for deep learning specifically. Additionally, the rising economic significance of machine learning has provided an incentive for more specialized hardware like TPUs \cite{noauthor_tpu_nodate} or Jim Keller's Grayskull. \cite{noauthor_grayskull_nodate} The same reason reignited interest in optical computing accelerators to bring drastic changes in power requirements and performance for matrix multiplication. \cite{noauthor_lightmatter_nodate,noauthor_lightelligence_nodate}
\end{itemize}