
@misc{abalone_sa_abalone_nodate,
  title = {Abalone Rulebook},
  shorttitle = {Abalone {{Rulebook}}},
  author = {Abalone S.A.},
  copyright = {Copyright Abalone S.A.},
  howpublished = {https://cdn.1j1ju.com/medias/c2/b0/3a-abalone-rulebook.pdf},
  file = {/home/ture/Zotero/storage/JHEB8Q64/3a-abalone-rulebook.pdf}
}

@article{aichholzer_algorithmic_2002,
  title = {Algorithmic Fun-Abalone},
  author = {Aichholzer, Oswin and Aurenhammer, Franz and Werner, Tino},
  year = {2002},
  journal = {Special Issue on Foundations of Information Processing of TELEMATIK},
  volume = {1},
  pages = {4--6},
  file = {/home/ture/Zotero/storage/8QRDUB38/Aichholzer et al. - 2002 - Algorithmic fun-abalone.pdf}
}

@misc{campfireman_campfiremanabalone-boai_2021,
  title = {Campfireman/{{Abalone}}-{{BoAI}}},
  author = {{campfireman}},
  year = {2021},
  month = jun,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT}
}

@article{campos_abalearn_2003,
  title = {Abalearn: {{Ecient Self}}-{{Play Learning}} of the Game {{Abalone}}},
  shorttitle = {Abalearn},
  author = {Campos, Pedro and Langlois, Thibault},
  year = {2003},
  abstract = {This paper presents Abalearn, a self-teaching Abalone pro- gram capable of automatically reaching an intermediate level of play without needing expert-labeled training examples or deep searches. Our approach is based on a reinforcement learning algorithm that is risk- seeking, since defensive players in Abalone tend to never end a game. We extend the risk-sensitive reinforcement learning framework in order to deal with large state spaces and we also propose a set of features that seem relevant for achieving a good level of play. We evaluate our approach using a fixed heuristic opponent as a bench- mark, pitting our agents against human players online and comparing samples of our agents at dierent times of training.},
  file = {/home/ture/Zotero/storage/BF4V5CJK/Campos and Langlois - 2009 - Abalearn Ecient Self-Play Learning of the game Ab.pdf}
}

@phdthesis{chorus_implementing_2009,
  type = {Master {{Thesis}}},
  title = {Implementing a Computer Player for Abalone Using Alpha-Beta and Monte-Carlo Search},
  author = {Chorus, Pascal},
  year = {2009},
  school = {Citeseer},
  file = {/home/ture/Zotero/storage/YY9WTM47/Chorus - 2009 - Implementing a computer player for abalone using a.pdf}
}

@misc{deepmind_match_nodate,
  title = {Match 1 - {{Google DeepMind Challenge Match}}: {{Lee Sedol}} vs {{AlphaGo}}},
  shorttitle = {Match 1 - {{Google DeepMind Challenge Match}}},
  author = {{DeepMind}},
  abstract = {Watch DeepMind's program AlphaGo take on the legendary Lee Sedol (9-dan pro), the top Go player of the past decade, in a \$1M 5-game challenge match in Seoul. This is the livestream for Match 1 to be played on: 9th March 13:00 KST (local), 04:00 GMT; note for US viewers this is the day before on: 8th March 20:00 PT, 23:00 ET.  In October 2015, AlphaGo became the first computer program ever to beat a professional Go player by winning 5-0 against the reigning 3-times European Champion Fan Hui (2-dan pro). That work was featured in a front cover article in the science journal Nature in January 2016. Match commentary by Michael Redmond (9-dan pro) and Chris Garlock.},
  howpublished = {https://www.youtube.com/watch?v=vFr3K2DORc8\&t=7020s}
}

@article{demichelis_simple_2004,
  title = {The Simple Geometry of Perfect Information Games},
  author = {Demichelis, Stefano and Ritzberger, Klaus and Swinkels, Jeroen M.},
  year = {2004},
  month = jun,
  journal = {International Journal of Game Theory},
  volume = {32},
  number = {3},
  pages = {315--338},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s001820400169},
  language = {en},
  file = {/home/ture/Zotero/storage/LE4XDBX5/Demichelis et al. - 2004 - The simple geometry of perfect information games.pdf}
}

@misc{foster_how_2019,
  title = {How to Build Your Own {{AlphaZero AI}} Using {{Python}} and {{Keras}}},
  author = {Foster, David},
  year = {2019},
  month = dec,
  journal = {Applied Data Science},
  abstract = {Teach a machine to learn Connect4 strategy through self-play and deep learning.},
  language = {en},
  file = {/home/ture/Zotero/storage/QIH4KCYY/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188.html}
}

@misc{higgins_brief_2017,
  title = {A {{Brief History}} of {{Deep Blue}}, {{IBM}}'s {{Chess Computer}} | {{Mental Floss}}},
  author = {Higgins, Chris},
  year = {2017},
  month = jul,
  howpublished = {https://web.archive.org/web/20170803130439/https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer},
  file = {/home/ture/Zotero/storage/ZE4C9CJX/brief-history-deep-blue-ibms-chess-computer.html}
}

@article{holland_studying_2006,
  title = {Studying {{Complex Adaptive Systems}}},
  author = {Holland, John H.},
  year = {2006},
  month = mar,
  journal = {Journal of Systems Science and Complexity},
  volume = {19},
  number = {1},
  pages = {1--8},
  issn = {1009-6124, 1559-7067},
  doi = {10.1007/s11424-006-0001-z},
  abstract = {Complex adaptive systems (cas) \textendash{} systems that involve many components that adapt or learn as they interact \textendash{} are at the heart of important contemporary problems. The study of cas poses unique challenges: Some of our most powerful mathematical tools, particularly methods involving fixed points, attractors, and the like, are of limited help in understanding the development of cas. This paper suggests ways to modify research methods and tools, with an emphasis on the role of computer-based models, to increase our understanding of cas.},
  language = {en},
  file = {/home/ture/Zotero/storage/PJZH78SX/Holland - 2006 - Studying Complex Adaptive Systems.pdf}
}

@misc{hui_alphago_2018,
  title = {{{AlphaGo}}: {{How}} It Works Technically?},
  shorttitle = {{{AlphaGo}}},
  author = {Hui, Jonathan},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it\ldots},
  language = {en},
  file = {/home/ture/Zotero/storage/AJ7N92UX/alphago-how-it-works-technically-26ddcc085319.html}
}

@incollection{hutchison_parallel_2010,
  title = {Parallel {{Minimax Tree Searching}} on {{GPU}}},
  booktitle = {Parallel {{Processing}} and {{Applied Mathematics}}},
  author = {Rocki, Kamil and Suda, Reiji},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Wasniewski, Jerzy},
  year = {2010},
  volume = {6067},
  pages = {449--456},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14390-8_47},
  abstract = {The paper describes results of minimax tree searching algorithm implemented within CUDA platform. The problem regards move choice strategy in the game of Reversi. The parallelization scheme and performance aspects are discussed, focusing mainly on warp divergence problem and data transfer size. Moreover, a method of minimizing warp divergence and performance degradation is described. The paper contains both the results of test performed on multiple CPUs and GPUs. Additionally, it discusses {$\alpha\beta$} parallel pruning implementation.},
  isbn = {978-3-642-14389-2 978-3-642-14390-8},
  language = {en},
  file = {/home/ture/Zotero/storage/5WUKFQ42/Rocki and Suda - 2010 - Parallel Minimax Tree Searching on GPU.pdf}
}

@techreport{lee_abalone_2005,
  title = {Abalone \textendash{{Final Project Report}}},
  author = {Lee, Benson and Noh, Hyun Joo},
  year = {2005},
  pages = {11},
  language = {en},
  file = {/home/ture/Zotero/storage/D27L8CN9/Lee - Abalone â€“Final Project Report.pdf}
}

@inproceedings{lemmens_constructing_2005,
  title = {Constructing an Abalone Game-Playing Agent},
  booktitle = {Bachelor {{Conference Knowledge Engineering}}, {{Universiteit Maastricht}}},
  author = {Lemmens, NPPM},
  year = {2005},
  publisher = {{Citeseer}},
  file = {/home/ture/Zotero/storage/9ME8UC3J/Lemmens - 2005 - Constructing an abalone game-playing agent.pdf}
}

@techreport{mizrachi_introduction_2017,
  title = {Introduction to Artificial Intelligence {{Final Project}}},
  author = {Mizrachi, Rubi and Golran, Guy and Jacobi, Omer and Zats, Rom},
  year = {2017},
  institution = {{The Hebrew University of Jerusalem}},
  file = {/home/ture/Zotero/storage/N7P92TVG/report.pdf}
}

@article{nijssen_writing_nodate,
  title = {Writing a {{Bachelor Thesis}} in {{Computer Science}}},
  author = {Nijssen, Siegfried},
  pages = {37},
  language = {en},
  file = {/home/ture/Zotero/storage/TCQ856MM/Nijssen - Writing a Bachelor Thesis in Computer Science.pdf}
}

@misc{noauthor_abalone_2020,
  title = {Abalone (Board Game)},
  year = {2020},
  month = dec,
  journal = {Wikipedia},
  abstract = {Abalone is a two-player abstract strategy board game designed by Michel Lalet and Laurent L\'evi in 1987. Players are represented by opposing black and white marbles on a hexagonal board with the objective of pushing six of the opponent's marbles off the edge of the board. Abalone was published in 1990 and has sold more than 4.5 million units. The year it was published it received one of the first Mensa Select awards. It is currently sold in more than thirty countries.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Abalone\_(board\_game)\&oldid=994557581},
  language = {en},
  annotation = {Page Version ID: 994557581},
  file = {/home/ture/Zotero/storage/5H8G9LDZ/index.html}
}

@misc{noauthor_peer_nodate,
  title = {Peer {{Sommerlund}} / Haliotis},
  abstract = {Abalone board game library},
  file = {/home/ture/Zotero/storage/S3KRSZD7/haliotis.html}
}

@misc{noauthor_red_nodate,
  title = {Red {{Blob Games}}: {{Hexagonal Grids}}},
  shorttitle = {Red {{Blob Games}}},
  abstract = {Guide to math, algorithms, and code for hexagonal grids in games},
  howpublished = {https://www.redblobgames.com/grids/hexagons/},
  language = {en},
  file = {/home/ture/Zotero/storage/6XJJDMJ9/hexagons.html}
}

@misc{noauthor_simple_nodate,
  title = {Simple {{Alpha Zero}}},
  howpublished = {https://web.stanford.edu/\textasciitilde surag/posts/alphazero.html},
  file = {/home/ture/Zotero/storage/S73RNETQ/alphazero.html}
}

@misc{noauthor_suragnairalpha-zero-general_nodate,
  title = {Suragnair/Alpha-Zero-General: {{A}} Clean Implementation Based on {{AlphaZero}} for Any Game in Any Framework + Tutorial + {{Othello}}/{{Gobang}}/{{TicTacToe}}/{{Connect4}} and More},
  howpublished = {https://github.com/suragnair/alpha-zero-general},
  file = {/home/ture/Zotero/storage/M78KG5GT/alpha-zero-general.html}
}

@misc{noauthor_ture_nodate,
  title = {Ture / Abalone},
  journal = {GitLab},
  abstract = {GitLab.com},
  howpublished = {https://gitlab.com/CampFireMan/abalone},
  language = {en},
  file = {/home/ture/Zotero/storage/5BYMCU37/abalone.html}
}

@misc{noauthor_unique_mentions_of_ml_frameworks_nodate,
  title = {Unique\_mentions\_of\_ml\_frameworks},
  howpublished = {https://pbs.twimg.com/media/DX5I8r\_VwAACbmo?format=jpg\&name=medium},
  file = {/home/ture/Zotero/storage/4SNRKRRT/DX5I8r_VwAACbmo.html}
}

@misc{noauthor_zobrist_nodate,
  title = {Zobrist {{Hashing}} - {{Chessprogramming}} Wiki},
  howpublished = {https://www.chessprogramming.org/Zobrist\_Hashing},
  file = {/home/ture/Zotero/storage/E9U4YWBE/Zobrist_Hashing.html}
}

@article{ozcan_simple_2004,
  title = {A {{Simple Intelligent Agent}} for {{Playing Abalone Game}}: {{ABLA}}},
  shorttitle = {A {{Simple Intelligent Agent}} for {{Playing Abalone Game}}},
  author = {{\"O}zcan, Ender and Hulagu, Berk},
  year = {2004},
  month = jan,
  abstract = {Forming winning strategies for board games requires good heuristics and fast search algorithms on game trees. High branching factors and the need for looking deeper in game trees are overwhelming, even for today's high performance PC's. Therefore, better game-plays are only available with better algorithms and heuristics for an ordinary player, not with faster machines. Abalone is a recent two-person strategy game. Initial evaluations point out that the branching factor is larger than the chess. In this paper, a new heuristic used by a simple intelligent agent for playing Abalone game, named as ABLA is introduced. ABLA's performance is promising as compared to existing computerized Abalone players.},
  file = {/home/ture/Zotero/storage/Y8U7NXW8/Ã–zcan and Hulagu - 2004 - A Simple Intelligent Agent for Playing Abalone Gam.pdf}
}

@article{papadopoulos_exploring_2012,
  title = {Exploring {{Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha}}-{{Beta Search}}},
  author = {Papadopoulos, Athanasios and Toumpas, Konstantinos and Chrysopoulos, Antonios and Mitkas, Pericles A},
  year = {2012},
  journal = {IEEE Conference on Computational Intelligence and Games},
  pages = {8},
  abstract = {This paper discusses the design and implementation of a highly efficient MiniMax algorithm for the game Abalone. For perfect information games with relatively low branching factor for their decision tree (such as Chess, Checkers etc.) and a highly accurate evaluation function, Alpha-Beta search proved to be far more efficient than Monte Carlo Tree Search. In recent years many new techniques have been developed to improve the efficiency of the Alpha-Beta tree, applied to a variety of scientific fields. This paper explores several techniques for increasing the efficiency of Alpha-Beta Search on the board game of Abalone while introducing some new innovative techniques that proved to be very effective. The main idea behind them is the incorporation of probabilistic features to the otherwise deterministic AlphaBeta search.},
  language = {en},
  file = {/home/ture/Zotero/storage/6NQG9BJM/Papadopoulos et al. - 2012 - Exploring Optimization Strategies in Board Game Ab.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2021},
  edition = {Fourth},
  publisher = {{Pearson Education, Inc}},
  abstract = {Artificial Intelligence: A Modern Approach offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.},
  isbn = {978-1-5376-0031-4},
  language = {en},
  file = {/home/ture/Zotero/storage/UKE25V5T/Stuart Russell, Peter Norvig - Artificial Intelligence_ A Modern Approach (4th Edition) (Pearson Series in Artifical Intelligence)-Language_ English (2020).pdf}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3\textemdash the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4\textemdash the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi\textemdash canonical environments for high-performance planning\textemdash the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game. A reinforcement-learning algorithm that combines a tree-based search with a learned model achieves superhuman performance in high-performance planning and visually complex domains, without any knowledge of their underlying dynamics.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science Subject\_term\_id: computational-science;computer-science},
  file = {/home/ture/Zotero/storage/GZDB2LDU/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf;/home/ture/Zotero/storage/DAI2XJ6I/s41586-020-03051-4.html}
}

@misc{scriptim_scriptimabalone-boai_2021,
  title = {Scriptim/{{Abalone}}-{{BoAI}}},
  author = {Scriptim},
  year = {2021},
  month = apr,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT License         ,                 MIT License},
  keywords = {abalone,ai,ai-battle-game,artificial-intelligence,game,machine-learning,python,python3}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  language = {en},
  file = {/home/ture/Zotero/storage/MD3YXY65/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  language = {en},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {/home/ture/Zotero/storage/5J8BTAU2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/ture/Zotero/storage/ZC9N7QNB/nature24270.html}
}

@misc{towzeur_towzeurgym-abalone_2021,
  title = {Towzeur/Gym-Abalone},
  author = {{towzeur}},
  year = {2021},
  month = jan,
  abstract = {An environment of the board game Abalone using OpenAI's Gym API},
  keywords = {abalone,gym,open-ai,reinforcement-learning}
}

@book{vasilev_python_2019,
  title = {Python Deep Learning: Exploring Deep Learning Techniques and Neural Network Architectures with {{PyTorch}}, {{Keras}}, and {{TensorFlow}}},
  shorttitle = {Python Deep Learning},
  author = {Vasilev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
  year = {2019},
  edition = {Second edition},
  publisher = {{Packt Publishing Limited}},
  address = {{Birmingham Mumbai}},
  isbn = {978-1-78934-846-0},
  language = {en},
  file = {/home/ture/Zotero/storage/TP5DPUZ2/Vasilev et al. - 2019 - Python deep learning exploring deep learning tech.pdf}
}


