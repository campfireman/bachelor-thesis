
@misc{abalone_sa_abalone_nodate,
  title = {Abalone Rulebook},
  shorttitle = {Abalone {{Rulebook}}},
  author = {Abalone S.A.},
  copyright = {Copyright Abalone S.A.},
  howpublished = {https://cdn.1j1ju.com/medias/c2/b0/3a-abalone-rulebook.pdf},
  file = {/home/ture/Zotero/storage/JHEB8Q64/3a-abalone-rulebook.pdf}
}

@article{aichholzer_algorithmic_2002,
  title = {Algorithmic Fun-Abalone},
  author = {Aichholzer, Oswin and Aurenhammer, Franz and Werner, Tino},
  year = {2002},
  journal = {Special Issue on Foundations of Information Processing of TELEMATIK},
  volume = {1},
  pages = {4--6},
  file = {/home/ture/Zotero/storage/8QRDUB38/Aichholzer et al. - 2002 - Algorithmic fun-abalone.pdf}
}

@article{bruasdal_deep_2020,
  title = {Deep Reinforcement {{Learning Using Monte}}-{{Carlo Tree Search}} for {{Hex}} and {{Othello}}},
  author = {Bru{\aa}sdal, Henrik},
  year = {2020},
  publisher = {{NTNU}},
  abstract = {N\aa r Deepminds AlphaGo-program slo den menneskelige profesjonelle Go-spilleren Fan Hui i 2015 var dette et stort gjennombrudd for kunstig intelligens til spilling. Go hadde vist seg \aa{} motst\aa{} de teknikkene som lenge hadde sl\aa tt mennesker i spill som sjakk. Gjennom en nyskapende kombinasjon av dype nevrale nettverk, forsterkende l\ae ring og Monte Carlo-tres\o k ble Go endelig mestret. Kort tid etterp\aa{} kom AlphaGo Zero, som oppn\aa dde det samme ved \aa{} l\ae re utelukkende ved \aa{} spille mot seg selv, og AlphaZero, som generaliserte teknikken til andre spill. Dette arbeidet inneholder en n\o ye gjennomgang av disse systemene og arbeidet i feltet som ledet opp mot dem. Det g\aa r gjennom min egen implementasjon av denne teknikken og dens bruk i spillene Hex og Othello. Ved hjelp av denne implementasjonen har jeg unders\o kt rollen utrulling spiller i algoritmen. Dette var en sentral del av tidligere arbeid i feltet og fremdeles brukt i AlphaGo, men ikke i AlphaGo Zero og AlphaZero. Flere eksperimenter har blitt gjennomf\o rt for \aa{} f\aa{} empiriske data for om utrulling fremdeles kan v\ae re en gunstig del av denne nyskapende kombinasjonen av teknikker, og hvordan disse i s\aa{} fall b\o r gjennomf\o res. Selv om det var noen indikasjoner i dataene p\aa{} at utrulling har liten eller ingen positiv effect er disse resultatene stort sett ikke entydige. Noen svakheter i oppsettet har blitt identifisert og noen nye sp\o rsm\aa l har blitt oppdaget. Men arbeidet har resultert i et funksjonelt system som kan brukes til videre unders\o kelser av problemomr\aa det og enten gi mer entydige data eller innsikt i nye sp\o rsm\aa l.},
  langid = {english},
  annotation = {Accepted: 2021-09-15T16:00:35Z},
  file = {/home/ture/Zotero/storage/RXYGMGDB/Bruåsdal - 2020 - Deep reinforcement Learning Using Monte-Carlo Tree.pdf;/home/ture/Zotero/storage/HSPTZDLR/2777474.html}
}

@misc{campfireman_campfiremanabalone-boai_2021,
  title = {Campfireman/{{Abalone}}-{{BoAI}}},
  author = {{campfireman}},
  year = {2021},
  month = jun,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT}
}

@article{campos_abalearn_2003,
  title = {Abalearn: Ecient {{Self}}-{{Play Learning}} of the Game {{Abalone}}},
  shorttitle = {Abalearn},
  author = {Campos, Pedro and Langlois, Thibault},
  year = {2003},
  abstract = {This paper presents Abalearn, a self-teaching Abalone pro- gram capable of automatically reaching an intermediate level of play without needing expert-labeled training examples or deep searches. Our approach is based on a reinforcement learning algorithm that is risk- seeking, since defensive players in Abalone tend to never end a game. We extend the risk-sensitive reinforcement learning framework in order to deal with large state spaces and we also propose a set of features that seem relevant for achieving a good level of play. We evaluate our approach using a fixed heuristic opponent as a bench- mark, pitting our agents against human players online and comparing samples of our agents at dierent times of training.},
  file = {/home/ture/Zotero/storage/BF4V5CJK/Campos and Langlois - 2009 - Abalearn Ecient Self-Play Learning of the game Ab.pdf}
}

@mastersthesis{chorus_implementing_2009,
  title = {Implementing a Computer Player for Abalone Using Alpha-Beta and Monte-Carlo Search},
  author = {Chorus, Pascal},
  year = {2009},
  school = {Citeseer},
  file = {/home/ture/Zotero/storage/YY9WTM47/Chorus - 2009 - Implementing a computer player for abalone using a.pdf}
}

@misc{deepmind_match_nodate,
  title = {Match 1 - {{Google DeepMind Challenge Match}}: Lee {{Sedol}} vs {{AlphaGo}}},
  shorttitle = {Match 1 - {{Google DeepMind Challenge Match}}},
  author = {{DeepMind}},
  abstract = {Watch DeepMind's program AlphaGo take on the legendary Lee Sedol (9-dan pro), the top Go player of the past decade, in a \$1M 5-game challenge match in Seoul. This is the livestream for Match 1 to be played on: 9th March 13:00 KST (local), 04:00 GMT; note for US viewers this is the day before on: 8th March 20:00 PT, 23:00 ET.  In October 2015, AlphaGo became the first computer program ever to beat a professional Go player by winning 5-0 against the reigning 3-times European Champion Fan Hui (2-dan pro). That work was featured in a front cover article in the science journal Nature in January 2016. Match commentary by Michael Redmond (9-dan pro) and Chris Garlock.},
  howpublished = {https://www.youtube.com/watch?v=vFr3K2DORc8\&t=7020s}
}

@article{demichelis_simple_2004,
  title = {The Simple Geometry of Perfect Information Games},
  author = {Demichelis, Stefano and Ritzberger, Klaus and Swinkels, Jeroen M.},
  year = {2004},
  month = jun,
  journal = {International Journal of Game Theory},
  volume = {32},
  number = {3},
  pages = {315--338},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s001820400169},
  langid = {english},
  file = {/home/ture/Zotero/storage/LE4XDBX5/Demichelis et al. - 2004 - The simple geometry of perfect information games.pdf}
}

@misc{foster_how_2019,
  title = {How to Build Your Own {{AlphaZero AI}} Using {{Python}} and {{Keras}}},
  author = {Foster, David},
  year = {2019},
  month = dec,
  journal = {Applied Data Science},
  abstract = {Teach a machine to learn Connect4 strategy through self-play and deep learning.},
  langid = {english},
  file = {/home/ture/Zotero/storage/QIH4KCYY/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188.html}
}

@misc{fridman_206_nodate,
  title = {\#206 - {{Ishan Misra}}: Self-{{Supervised Deep Learning}} in {{Computer Vision}} | {{Lex Fridman Podcast}}},
  shorttitle = {\#206 - {{Ishan Misra}}},
  author = {Fridman, Lex},
  number = {206},
  abstract = {Ishan Misra is a research scientist at FAIR working on self-supervised visual learning.},
  langid = {american},
  file = {/home/ture/Zotero/storage/2YLN3AXC/ishan-misra.html}
}

@article{gao_multithreaded_nodate,
  title = {Multithreaded {{Pruned Tree Search}} in {{Distributed Systems}}},
  author = {Gao, Yaoqing and Marsland, T A},
  pages = {11},
  abstract = {Although efficient support for data-parallel applications is relatively well established, it remains open how well to support irregular and dynamic problems where there are no regular data structures and communication patterns. Tree search is central to solving a variety of problems in artificial intelligence and an important subset of the irregular applications where tasks are frequently created and terminated. In this paper, we introduce the design of a multithreaded distributed runtime system. Efficiency and ease of parallel programming are the two primary goals. In our system, multithreading is used to specify the asynchronous behavior in parallel game tree search, and dynamic load balancing is employed for efficient performance.},
  langid = {english},
  file = {/home/ture/Zotero/storage/KYCV5AV9/Gao and Marsland - Multithreaded Pruned Tree Search in Distributed Sy.pdf}
}

@misc{higgins_brief_2017,
  title = {A {{Brief History}} of {{Deep Blue}}, {{IBM}}'s {{Chess Computer}} | {{Mental Floss}}},
  author = {Higgins, Chris},
  year = {2017},
  month = jul,
  howpublished = {https://web.archive.org/web/20170803130439/https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer},
  file = {/home/ture/Zotero/storage/ZE4C9CJX/brief-history-deep-blue-ibms-chess-computer.html}
}

@article{holland_studying_2006,
  title = {Studying {{Complex Adaptive Systems}}},
  author = {Holland, John H.},
  year = {2006},
  month = mar,
  journal = {Journal of Systems Science and Complexity},
  volume = {19},
  number = {1},
  pages = {1--8},
  issn = {1009-6124, 1559-7067},
  doi = {10.1007/s11424-006-0001-z},
  abstract = {Complex adaptive systems (cas) \textendash{} systems that involve many components that adapt or learn as they interact \textendash{} are at the heart of important contemporary problems. The study of cas poses unique challenges: Some of our most powerful mathematical tools, particularly methods involving fixed points, attractors, and the like, are of limited help in understanding the development of cas. This paper suggests ways to modify research methods and tools, with an emphasis on the role of computer-based models, to increase our understanding of cas.},
  langid = {english},
  file = {/home/ture/Zotero/storage/PJZH78SX/Holland - 2006 - Studying Complex Adaptive Systems.pdf}
}

@misc{hui_alphago_2018,
  title = {{{AlphaGo}}: How It Works Technically?},
  shorttitle = {{{AlphaGo}}},
  author = {Hui, Jonathan},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it\ldots},
  langid = {english},
  file = {/home/ture/Zotero/storage/AJ7N92UX/alphago-how-it-works-technically-26ddcc085319.html}
}

@incollection{hutchison_parallel_2010,
  title = {Parallel {{Minimax Tree Searching}} on {{GPU}}},
  booktitle = {Parallel {{Processing}} and {{Applied Mathematics}}},
  author = {Rocki, Kamil and Suda, Reiji},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Wasniewski, Jerzy},
  year = {2010},
  volume = {6067},
  pages = {449--456},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14390-8_47},
  abstract = {The paper describes results of minimax tree searching algorithm implemented within CUDA platform. The problem regards move choice strategy in the game of Reversi. The parallelization scheme and performance aspects are discussed, focusing mainly on warp divergence problem and data transfer size. Moreover, a method of minimizing warp divergence and performance degradation is described. The paper contains both the results of test performed on multiple CPUs and GPUs. Additionally, it discusses {$\alpha\beta$} parallel pruning implementation.},
  isbn = {978-3-642-14389-2 978-3-642-14390-8},
  langid = {english},
  file = {/home/ture/Zotero/storage/5WUKFQ42/Rocki and Suda - 2010 - Parallel Minimax Tree Searching on GPU.pdf}
}

@article{JMLR:v9:vandermaaten08a,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  file = {/home/ture/Zotero/storage/XIYLPMQH/vandermaaten08a.pdf}
}

@article{kochenderfer_algorithms_nodate,
  title = {Algorithms for {{Decision Making}}},
  author = {Kochenderfer, Mykel J and Wheeler, Tim A and Wray, Kyle H},
  pages = {690},
  langid = {english},
  file = {/home/ture/Zotero/storage/8EJIZEXH/Kochenderfer et al. - Algorithms for Decision Making.pdf}
}

@techreport{lee_abalone_2005,
  title = {Abalone \textendash{{Final Project Report}}},
  author = {Lee, Benson and Noh, Hyun Joo},
  year = {2005},
  pages = {11},
  langid = {english},
  file = {/home/ture/Zotero/storage/D27L8CN9/Lee - Abalone –Final Project Report.pdf}
}

@inproceedings{lemmens_constructing_2005,
  title = {Constructing an Abalone Game-Playing Agent},
  booktitle = {Bachelor {{Conference Knowledge Engineering}}, {{Universiteit Maastricht}}},
  author = {Lemmens, NPPM},
  year = {2005},
  publisher = {{Citeseer}},
  file = {/home/ture/Zotero/storage/9ME8UC3J/Lemmens - 2005 - Constructing an abalone game-playing agent.pdf}
}

@techreport{mizrachi_introduction_2017,
  title = {Introduction to Artificial Intelligence {{Final Project}}},
  author = {Mizrachi, Rubi and Golran, Guy and Jacobi, Omer and Zats, Rom},
  year = {2017},
  institution = {{The Hebrew University of Jerusalem}},
  file = {/home/ture/Zotero/storage/N7P92TVG/report.pdf}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  langid = {english},
  file = {/home/ture/Zotero/storage/3W78FHUN/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@book{moroney_ai_2020,
  title = {{{AI}} and {{Machine Learning}} for {{Coders}}: A {{Programmer}}'s {{Guide}} to {{Artificial Intelligence}}},
  shorttitle = {{{AI}} and {{Machine Learning}} for {{Coders}}},
  author = {Moroney, Laurence},
  year = {2020},
  publisher = {{O'Reilly}},
  abstract = {If you're looking to make a career move from programmer to AI specialist, this is the ideal place to start. Based on Laurence Moroney's extremely successful AI courses, this introductory book provides a hands-on, code-first approach to help you build confidence while you learn key topics. You'll understand how to implement the most common scenarios in machine learning, such as computer vision, natural language processing (NLP), and sequence modeling for web, mobile, cloud, and embedded runtimes. Most books on machine learning begin with a daunting amount of advanced math. This guide is built on practical lessons that let you work directly with the code. You'll learn:  How to build models with TensorFlow using skills that employers desire The basics of machine learning by working with code samples How to implement computer vision, including feature detection in images How to use NLP to tokenize and sequence words and sentences Methods for embedding models in Android and iOS How to serve models over the web and in the cloud with TensorFlow Serving},
  googlebooks = {462OzQEACAAJ},
  isbn = {978-1-4920-7819-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Information Theory,Computers / Machine Theory},
  file = {/home/ture/Zotero/storage/P6Q9E7U7/Laurence Moroney - AI and Machine Learning for Coders_ A Programmer's Guide to Artificial Intelligence-O'Reilly Media (2020).pdf}
}

@article{nijssen_writing_nodate,
  title = {Writing a {{Bachelor Thesis}} in {{Computer Science}}},
  author = {Nijssen, Siegfried},
  pages = {37},
  langid = {english},
  file = {/home/ture/Zotero/storage/TCQ856MM/Nijssen - Writing a Bachelor Thesis in Computer Science.pdf}
}

@misc{noauthor_abalone_2020,
  title = {Abalone (Board Game)},
  year = {2020},
  month = dec,
  journal = {Wikipedia},
  abstract = {Abalone is a two-player abstract strategy board game designed by Michel Lalet and Laurent L\'evi in 1987. Players are represented by opposing black and white marbles on a hexagonal board with the objective of pushing six of the opponent's marbles off the edge of the board. Abalone was published in 1990 and has sold more than 4.5 million units. The year it was published it received one of the first Mensa Select awards. It is currently sold in more than thirty countries.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Abalone\_(board\_game)\&oldid=994557581},
  langid = {english},
  annotation = {Page Version ID: 994557581},
  file = {/home/ture/Zotero/storage/5H8G9LDZ/index.html}
}

@misc{noauthor_free_nodate,
  title = {Free {{Trial}} and {{Free Tier}}},
  journal = {Google Cloud},
  abstract = {Start building on GCP with a Free Trial that includes \$300 in credits. Plus, enjoy access to 20+ select products, like Compute Engine, free of charge.},
  howpublished = {https://cloud.google.com/free},
  langid = {english},
  file = {/home/ture/Zotero/storage/CVDRRM54/free.html}
}

@misc{noauthor_peer_nodate,
  title = {Peer {{Sommerlund}} / Haliotis},
  abstract = {Abalone board game library},
  file = {/home/ture/Zotero/storage/S3KRSZD7/haliotis.html}
}

@misc{noauthor_red_nodate,
  title = {Red {{Blob Games}}: Hexagonal {{Grids}}},
  shorttitle = {Red {{Blob Games}}},
  abstract = {Guide to math, algorithms, and code for hexagonal grids in games},
  howpublished = {https://www.redblobgames.com/grids/hexagons/},
  langid = {english},
  file = {/home/ture/Zotero/storage/6XJJDMJ9/hexagons.html}
}

@misc{noauthor_simple_nodate,
  title = {Simple {{Alpha Zero}}},
  howpublished = {https://web.stanford.edu/\textasciitilde surag/posts/alphazero.html},
  file = {/home/ture/Zotero/storage/S73RNETQ/alphazero.html}
}

@misc{noauthor_tpu_nodate,
  title = {{{TPU Research Cloud}}},
  howpublished = {https://sites.research.google/trc/},
  file = {/home/ture/Zotero/storage/96VFE5C2/trc.html}
}

@misc{noauthor_ture_nodate,
  title = {Ture / Abalone},
  journal = {GitLab},
  abstract = {GitLab.com},
  howpublished = {https://gitlab.com/CampFireMan/abalone},
  langid = {english},
  file = {/home/ture/Zotero/storage/5BYMCU37/abalone.html}
}

@misc{noauthor_unique_mentions_of_ml_frameworks_nodate,
  title = {Unique\_mentions\_of\_ml\_frameworks},
  howpublished = {https://pbs.twimg.com/media/DX5I8r\_VwAACbmo?format=jpg\&name=medium},
  file = {/home/ture/Zotero/storage/4SNRKRRT/DX5I8r_VwAACbmo.html}
}

@misc{noauthor_zobrist_nodate,
  title = {Zobrist {{Hashing}} - {{Chessprogramming}} Wiki},
  howpublished = {https://www.chessprogramming.org/Zobrist\_Hashing},
  file = {/home/ture/Zotero/storage/E9U4YWBE/Zobrist_Hashing.html}
}

@article{papadopoulos_exploring_2012,
  title = {Exploring {{Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha}}-{{Beta Search}}},
  author = {Papadopoulos, Athanasios and Toumpas, Konstantinos and Chrysopoulos, Antonios and Mitkas, Pericles A},
  year = {2012},
  journal = {IEEE Conference on Computational Intelligence and Games},
  pages = {8},
  abstract = {This paper discusses the design and implementation of a highly efficient MiniMax algorithm for the game Abalone. For perfect information games with relatively low branching factor for their decision tree (such as Chess, Checkers etc.) and a highly accurate evaluation function, Alpha-Beta search proved to be far more efficient than Monte Carlo Tree Search. In recent years many new techniques have been developed to improve the efficiency of the Alpha-Beta tree, applied to a variety of scientific fields. This paper explores several techniques for increasing the efficiency of Alpha-Beta Search on the board game of Abalone while introducing some new innovative techniques that proved to be very effective. The main idea behind them is the incorporation of probabilistic features to the otherwise deterministic AlphaBeta search.},
  langid = {english},
  file = {/home/ture/Zotero/storage/6NQG9BJM/Papadopoulos et al. - 2012 - Exploring Optimization Strategies in Board Game Ab.pdf}
}

@article{rosin_multi-armed_2011,
  title = {Multi-Armed Bandits with Episode Context},
  author = {Rosin, Christopher D.},
  year = {2011},
  month = mar,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {61},
  number = {3},
  pages = {203--230},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-011-9258-6},
  abstract = {A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0, 1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multiarmed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm.},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKNKB6JG/Rosin - 2011 - Multi-armed bandits with episode context.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial {{Intelligence}}: A {{Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2021},
  edition = {Fourth},
  publisher = {{Pearson Education, Inc}},
  abstract = {Artificial Intelligence: A Modern Approach offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.},
  isbn = {978-1-5376-0031-4},
  langid = {english},
  file = {/home/ture/Zotero/storage/UKE25V5T/Stuart Russell, Peter Norvig - Artificial Intelligence_ A Modern Approach (4th Edition) (Pearson Series in Artifical Intelligence)-Language_ English (2020).pdf}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3\textemdash the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4\textemdash the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi\textemdash canonical environments for high-performance planning\textemdash the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game. A reinforcement-learning algorithm that combines a tree-based search with a learned model achieves superhuman performance in high-performance planning and visually complex domains, without any knowledge of their underlying dynamics.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science Subject\_term\_id: computational-science;computer-science},
  file = {/home/ture/Zotero/storage/GZDB2LDU/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf;/home/ture/Zotero/storage/DAI2XJ6I/s41586-020-03051-4.html}
}

@misc{scriptim_scriptimabalone-boai_2021,
  title = {Scriptim/{{Abalone}}-{{BoAI}}},
  author = {Scriptim},
  year = {2021},
  month = apr,
  abstract = {A Python implementation of the board game Abalone intended to be played by artificial intelligence},
  copyright = {MIT License         ,                 MIT License},
  keywords = {abalone,ai,ai-battle-game,artificial-intelligence,game,machine-learning,python,python3}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  langid = {english},
  file = {/home/ture/Zotero/storage/MD3YXY65/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {/home/ture/Zotero/storage/5J8BTAU2/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/ture/Zotero/storage/ZC9N7QNB/nature24270.html}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/ture/Zotero/storage/BBRQKJF4/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@techreport{thakoor_learning_nodate,
  type = {Final Project Report},
  title = {Learning to {{Play Othello Without Human Knowledge}}},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  institution = {{Stanford University}},
  abstract = {Game playing is a popular area within the field of artificial intelligence. Most agents in literature have hand-crafted features and are often trained on datasets obtained from expert human play. We implement a self- play based algorithm using neural networks for policy estimation and Monte Carlo Tree Search for policy im- provement, with no input human knowledge that learns to play Othello. We evaluate our learning algorithm for 6x6 and 8x8 versions of the game of Othello. Our work is compared with random and greedy baselines, as well as a minimax agent that uses a hand-crafted scoring function, and achieves impressive results. Further, our agent for the 6x6 version of Othello easily outperforms humans when tested against it.},
  file = {/home/ture/Zotero/storage/THCSVXJR/writeup.pdf}
}

@misc{thakoor_suragnairalpha-zero-general_nodate,
  title = {Suragnair/Alpha-Zero-General: A Clean Implementation Based on {{AlphaZero}} for Any Game in Any Framework + Tutorial + {{Othello}}/{{Gobang}}/{{TicTacToe}}/{{Connect4}} and More},
  author = {Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha},
  howpublished = {https://github.com/suragnair/alpha-zero-general},
  file = {/home/ture/Zotero/storage/M78KG5GT/alpha-zero-general.html}
}

@misc{towzeur_towzeurgym-abalone_2021,
  title = {Towzeur/Gym-Abalone},
  author = {{towzeur}},
  year = {2021},
  month = jan,
  abstract = {An environment of the board game Abalone using OpenAI's Gym API},
  keywords = {abalone,gym,open-ai,reinforcement-learning}
}

@article{turing_icomputing_1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  file = {/home/ture/Zotero/storage/RB33PS2W/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/home/ture/Zotero/storage/6RJMJ3NW/986238.html}
}

@book{vasilev_python_2019,
  title = {Python Deep Learning: Exploring Deep Learning Techniques and Neural Network Architectures with {{PyTorch}}, {{Keras}}, and {{TensorFlow}}},
  shorttitle = {Python Deep Learning},
  author = {Vasilev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
  year = {2019},
  edition = {Second edition},
  publisher = {{Packt Publishing Limited}},
  address = {{Birmingham Mumbai}},
  isbn = {978-1-78934-846-0},
  langid = {english},
  file = {/home/ture/Zotero/storage/TP5DPUZ2/Vasilev et al. - 2019 - Python deep learning exploring deep learning tech.pdf}
}

@article{verloop_critical_nodate,
  title = {A {{Critical Review}}: Exploring {{Optimization Strategies}} in {{Board Game Abalone}} for {{Alpha}}-{{Beta Search}}},
  author = {Verloop, Michiel},
  pages = {49},
  langid = {english},
  file = {/home/ture/Zotero/storage/EK73C5BP/Verloop - A Critical Review Exploring Optimization Strategi.pdf}
}

@article{vinyals_grandmaster_2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  langid = {english},
  file = {/home/ture/Zotero/storage/Y2K2TQCF/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf}
}

@article{wang_adaptive_2021,
  title = {Adaptive {{Warm}}-{{Start MCTS}} in {{AlphaZero}}-like {{Deep Reinforcement Learning}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2021},
  month = may,
  journal = {arXiv:2105.06136 [cs]},
  eprint = {2105.06136},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {AlphaZero has achieved impressive performance in deep reinforcement learning by utilizing an architecture that combines search and training of a neural network in self-play. Many researchers are looking for ways to reproduce and improve results for other games/tasks. However, the architecture is designed to learn from scratch, tabula rasa, accepting a cold-start problem in self-play. Recently, a warmstart enhancement method for Monte Carlo Tree Search was proposed to improve the self-play starting phase. It employs a fixed parameter I to control the warm-start length. Improved performance was reported in small board games. In this paper we present results with an adaptive switch method. Experiments show that our approach works better than the fixed I , especially for ''deep,'' tactical, games (Othello and Connect Four). We conjecture that the adaptive value for I is also influenced by the size of the game, and that on average I will increase with game size. We conclude that AlphaZero-like deep reinforcement learning benefits from adaptive rollout based warm-start, as Rapid Action Value Estimate did for rollout-based reinforcement learning 15 years ago.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/ture/Zotero/storage/T6AJBPW2/Wang et al. - 2021 - Adaptive Warm-Start MCTS in AlphaZero-like Deep Re.pdf}
}

@article{wang_warm-start_2020,
  title = {Warm-{{Start AlphaZero Self}}-{{Play Search Enhancements}}},
  author = {Wang, Hui and Preuss, Mike and Plaat, Aske},
  year = {2020},
  journal = {arXiv:2004.12357 [cs]},
  volume = {12270},
  eprint = {2004.12357},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {528--542},
  doi = {10.1007/978-3-030-58115-2_37},
  abstract = {Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE) and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different (small) board games, with especially RAVE based variants playing strongly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ture/Zotero/storage/IC9JLYFX/Wang et al. - 2020 - Warm-Start AlphaZero Self-Play Search Enhancements.pdf;/home/ture/Zotero/storage/Y68GT8W8/2004.html}
}


